{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "- DQN Ordenar Lista.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KbquQTFT4jD"
      },
      "source": [
        "#Demo de TF-Agents para ordenar elmentos de una lista usando DQN:\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dliJD0WRUMWV"
      },
      "source": [
        "0) Preparar el ambiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Qxbe02w0T0ip",
        "outputId": "12c705b1-b2c3-4f90-b13a-7277aa451d2e"
      },
      "source": [
        "#@title Instalar Paquete de TF-Agents\r\n",
        "!pip install -q tf-agents\r\n",
        "print(\"TF-Agentes instalado.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-Agentes instalado.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJl4YsniURev",
        "cellView": "form",
        "outputId": "846b4817-289c-48e1-9a44-4842af7cc3f5"
      },
      "source": [
        "#@title Cargar Librerías\r\n",
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "import abc\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from random import randint\r\n",
        "from sklearn import preprocessing\r\n",
        "import copy\r\n",
        "\r\n",
        "from tf_agents.environments import py_environment\r\n",
        "from tf_agents.environments import tf_py_environment\r\n",
        "\r\n",
        "from tf_agents.environments import utils\r\n",
        "from tf_agents.specs import array_spec\r\n",
        "\r\n",
        "from tf_agents.policies import random_tf_policy\r\n",
        "\r\n",
        "from tf_agents.trajectories import time_step as ts\r\n",
        "\r\n",
        "from tf_agents.agents.dqn import dqn_agent\r\n",
        "from tf_agents.networks import q_network\r\n",
        "from tf_agents.utils import common\r\n",
        "\r\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n",
        "from tf_agents.trajectories import trajectory\r\n",
        "\r\n",
        "import os\r\n",
        "from tf_agents.policies import policy_saver\r\n",
        "\r\n",
        "tf.compat.v1.enable_v2_behavior()\r\n",
        "\r\n",
        "print(\"Librerías cargadas.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Librerías cargadas.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ONe5w_nUYME"
      },
      "source": [
        "1) Establecer las clases del Problema y del Agente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TQx1eodsvKj",
        "outputId": "e93b5b43-003a-4dab-b1f9-1abc3bf3287b"
      },
      "source": [
        "#@title Definir las primitivas a usar para ordenar la lista\r\n",
        "\r\n",
        "# intercambia valores de pos1 con pos2 \r\n",
        "def intercambiar(lista, pos1, pos2):\r\n",
        "  # chequea que las posiciones no se encuentran fuera de la lista\r\n",
        "  if (pos1 < 0) or (pos1 >= len(lista)):\r\n",
        "    if (pos1 < 0):\r\n",
        "      pos1 = 0\r\n",
        "    else:\r\n",
        "      pos1 =  len(lista) - 1\r\n",
        "  if (pos2 < 0) or (pos2 >= len(lista)):\r\n",
        "    if (pos2 < 0):\r\n",
        "      pos2 = 0\r\n",
        "    else:\r\n",
        "      pos2 =  len(lista) - 1\r\n",
        "  # chequea que no sean las mismas posiciones\r\n",
        "  if pos1 == pos2:\r\n",
        "    return lista\r\n",
        "  else:    \r\n",
        "    # realiza el intercambio\r\n",
        "    lista[pos1], lista[pos2] = lista[pos2], lista[pos1]\r\n",
        "    return lista\r\n",
        "\r\n",
        "# mueve el valor de posAnt a posNueva\r\n",
        "def mover(lista, posAnt, posNueva):\r\n",
        "  # chequea que las posiciones no se encuentran fuera de la lista\r\n",
        "  if (posAnt < 0) or (posAnt >= len(lista)):\r\n",
        "    if (posAnt < 0):\r\n",
        "      posAnt = 0\r\n",
        "    else:\r\n",
        "      posAnt =  len(lista) - 1\r\n",
        "  if (posNueva < 0) or (posNueva >= len(lista)):\r\n",
        "    if (posNueva < 0):\r\n",
        "      posNueva = 0\r\n",
        "    else:\r\n",
        "      posNueva =  len(lista) - 1\r\n",
        "  # chequea que no sean las mismas posiciones\r\n",
        "  if posAnt == posNueva:\r\n",
        "    return lista\r\n",
        "  else:    \r\n",
        "    # realiza el intercambio\r\n",
        "    lista.insert(posNueva, lista.pop(posAnt))\r\n",
        "    return lista\r\n",
        "\r\n",
        "# pone una posición más adelante al valor de pos \r\n",
        "def adelantar(lista, pos, cant):\r\n",
        "  return mover(lista, pos, (pos-cant))\r\n",
        "\r\n",
        "# pone una posición más atrpas al valor de pos\r\n",
        "def atrasar(lista, pos, cant):\r\n",
        "  return mover(lista, pos, (pos+cant))\r\n",
        "\r\n",
        "# función auxiliar para contar la cantidad de desordenados\r\n",
        "# (debe ser de menor a mayor)\r\n",
        "def contarDesordenados(lista):  \r\n",
        "  cantError = 0\r\n",
        "  if len(lista) > 0:\r\n",
        "    i = 0 \r\n",
        "    while i < len(lista):\r\n",
        "      ant = lista[i]\r\n",
        "      j = i + 1\r\n",
        "      while j < len(lista):\r\n",
        "        actual = lista[j]\r\n",
        "        if actual < ant:\r\n",
        "          cantError = cantError + 1\r\n",
        "        j = j + 1      \r\n",
        "      i = i + 1\r\n",
        "  return cantError\r\n",
        "\r\n",
        "# devuelve la posición con el menor valor de toda la lista \r\n",
        "def devMayorMenor(lista):\r\n",
        "  posMenor = 0\r\n",
        "  valMenor = None\r\n",
        "  posMayor = 0\r\n",
        "  valMayor = None  \r\n",
        "  for pos in range(len(lista)):\r\n",
        "    if (valMenor == None) or (lista[pos] < valMenor):\r\n",
        "      valMenor = lista[pos]\r\n",
        "      posMenor = pos\r\n",
        "    if (valMayor == None) or (lista[pos] < valMayor):\r\n",
        "      valMayor = lista[pos]\r\n",
        "      posMayor = pos      \r\n",
        "  return posMenor, posMayor\r\n",
        "\r\n",
        "# variable auxiliar para determinar máximo de acciones a probar antes de abortar\r\n",
        "POSIBLES_ACCIONES_DESC = [ \"mover\", \"adelantar\", \"atrasar\", \"intercambiar\" ]\r\n",
        "POSIBLES_ACCIONES = [ mover, adelantar, atrasar, intercambiar ]\r\n",
        "\r\n",
        "print(\"Primitivas de acciones definidas: \", POSIBLES_ACCIONES_DESC)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Primitivas de acciones definidas:  ['mover', 'adelantar', 'atrasar', 'intercambiar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R9SyNuiUjyT",
        "cellView": "code",
        "outputId": "3f6b2784-8170-4e4a-ce1c-7b4bafee9c88"
      },
      "source": [
        "#@title Definir Entorno del Problema \n",
        "\n",
        "# parámetros generales\n",
        "MAX_ITERACIONES_REALIZAR = 100\n",
        "TAMANIO_MINIMO_LISTA = 3\n",
        "TAMANIO_MAXIMO_LISTA = 5\n",
        "MAXIMO_VALOR_ACTION = (TAMANIO_MAXIMO_LISTA * 10 + TAMANIO_MAXIMO_LISTA)\n",
        "\n",
        "def parsearAccion(action):\n",
        "  # como DQN sólo permite 1 action numérica\n",
        "  # esta función se ocupa de parsearla para determinar:\n",
        "  #    tipo de acción\n",
        "  #    param1\n",
        "  #    param2\n",
        "  aux = action\n",
        "  idAccion = aux // 100\n",
        "  aux = aux - idAccion * 100\n",
        "  param1 = aux // 10\n",
        "  aux = aux - param1 * 10\n",
        "  param2 = aux\n",
        "  #print(action, idAccion, param1, param2)\n",
        "  return idAccion, param1, param2\n",
        "\n",
        "# Un entorno que represente el juego podría verse así:\n",
        "class OrdenarListasEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, reGenerarReset=True):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=MAXIMO_VALOR_ACTION, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(TAMANIO_MAXIMO_LISTA,), dtype=np.float32, name='observation')      \n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "    self._reGenerarListaReset = reGenerarReset\n",
        "    if self._reGenerarListaReset:\n",
        "      # inicializa vacía porque se define en el reset\n",
        "      self._listaOriginal = []\n",
        "    else:\n",
        "      # la lista se define sólo al principio, luego se vuelve a desordenar\n",
        "      self._listaOriginal = self.crearLista()\n",
        "    self._lista = []\n",
        "\n",
        "  def action_spec(self):\n",
        "    # devuelve la forma de las acciones\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    # devuelve la forma de las observaciones   \n",
        "    return self._observation_spec\n",
        "\n",
        "  def render(self, mode = 'human'):\n",
        "    # devuelve la lista para mostsrar\n",
        "    return np.array(self._lista, dtype=np.int32)\n",
        "\n",
        "  def _reset(self):\n",
        "    # resetea el entorno\n",
        "    if self._reGenerarListaReset:\n",
        "      # cada vez que se reseta, se define la lista\n",
        "      self._listaOriginal = self.crearLista()\n",
        "    # siempre la lista de trabajo se copia de la original\n",
        "    self._lista = copy.deepcopy( self._listaOriginal ) \n",
        "    # actualiza el estado considerando cantidad de ordenados\n",
        "    self.actualizarEstado()\n",
        "    self._cantIteraciones = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(self.devolverObsActual())\n",
        "\n",
        "  def crearLista(self):\n",
        "    # genera los valores de las listas al azar\n",
        "    #cantElemRnd = randint(TAMANIO_MINIMO_LISTA, TAMANIO_MAXIMO_LISTA)\n",
        "    cantElemRnd = TAMANIO_MAXIMO_LISTA\n",
        "    lista = []\n",
        "    for j in range(cantElemRnd): \n",
        "      lista.append( randint(-99, 99) )\n",
        "    return lista\n",
        "\n",
        "  def actualizarEstado(self):\n",
        "    # actualiza el valor del estado del entorno\n",
        "    # teniendo en cuenta la cantidad de errores negativos\n",
        "    self._state = - contarDesordenados(self._lista)\n",
        "    return self._state\n",
        "\n",
        "  def devolverObsActual(self):\n",
        "    # devuelve valores para la observación actual\n",
        "    # los valores de la lista (rellenando con cero)\n",
        "    ##res = [ self._state ]\n",
        "    res = []\n",
        "    res.extend( self._lista )\n",
        "    while (len(res) < TAMANIO_MAXIMO_LISTA):\n",
        "      res.append( 100 )      \n",
        "    # nota: para DQN parece ser que conviene \n",
        "    # que sean valores decimales pequeños,\n",
        "    # así genera menor \"loss\"\n",
        "    #r = []\n",
        "    #for v in res:\n",
        "    #  r.append( v / 100.0 )\n",
        "    # normaliza los valores para que sean más homogeneos los valores\n",
        "    r = (res - np.min(res)) / (np.max(res) - np.min(res))\n",
        "    return  np.array([round(v,3) for v in r], dtype=np.float32)\n",
        "\n",
        "  def _step(self, action):\n",
        "    # aplica una acción sobre el entorno\n",
        "    \n",
        "    if self._episode_ended:\n",
        "      # si el entorno está finalizado, lo resetea\n",
        "      return self.reset()\n",
        "\n",
        "    # actualiza cantidad de interacciones \n",
        "    self._cantIteraciones = self._cantIteraciones - 1\n",
        "\n",
        "    # parsea la accion para determinar acción con sus parámetros\n",
        "    idAccion, param1, param2 = parsearAccion(action)\n",
        "\n",
        "    # si es un id de acción válida\n",
        "    if idAccion >= 0 and idAccion < len(POSIBLES_ACCIONES):\n",
        "      # aplica la acción correspondiente en cada lista\n",
        "      # y calculando la cantidad de desordenados como error\n",
        "      self._lista = POSIBLES_ACCIONES[idAccion](self._lista, param1, param2)\n",
        "      \n",
        "      # actualiza el estado con la cantidad de valores correctos\n",
        "      self.actualizarEstado()\n",
        "\n",
        "    # determina si debe finalizar o no\n",
        "    if (self._state == 0) or (abs(self._cantIteraciones) >= abs(MAX_ITERACIONES_REALIZAR)):\n",
        "      # si está todo ordenado \n",
        "      # o si la cantidad de iteraciones llega al límite\n",
        "      # fuerza que finaliza\n",
        "      self._episode_ended = True\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # si finaliza\n",
        "      # devuelve el reward (siempre se maximiza):\n",
        "      # si logra ordenaar\n",
        "      # se calcula penalizando la cantidad de iteraciones \n",
        "      if (self._state == 0):\n",
        "        reward = MAX_ITERACIONES_REALIZAR + self._cantIteraciones\n",
        "      else:\n",
        "        reward = self._state\n",
        "      return ts.termination(self.devolverObsActual(), reward)\n",
        "    else:\n",
        "      # si no finaliza      \n",
        "      return ts.transition(\n",
        "         self.devolverObsActual(), reward=self._state, discount=0.9)\n",
        "         #discount=1.0)\n",
        "\n",
        "print(\"\\nEntorno del Problema definido.\")\n",
        "\n",
        "# Definir entornos de entrenamiento y de evaluación\n",
        "# (ambos con lista que se cambia cada vez que se resetea)\n",
        "train_py_env = OrdenarListasEnv(True)\n",
        "eval_py_env = OrdenarListasEnv(True)\n",
        "\n",
        "# Definir wrapper para convertir en entornos TF\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "# define política al azar independiente del Agente\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "\n",
        "print(\"\\nEntornos de entrenamiento y prueba definidos. \")\n",
        "\n",
        "# definir simulador para probar el entorno\n",
        "def SimularEntorno(env, policy, titulo):\n",
        "    print(\"\\n** \", titulo, \"**\")                   \n",
        "    # muesta estado inicial\n",
        "    time_step = env.reset()      \n",
        "    #ob = time_step.observation.numpy()[0]\n",
        "    print(\" Ini: [\", time_step, \"]\")    \n",
        "    print(\" Lista Inicial = \", env.pyenv.render()[0] )\n",
        "    j = 1\n",
        "    while not time_step.is_last():\n",
        "      # la política determina la acción a realizar\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = env.step(action_step.action)\n",
        "      # recupera la observación y muestra el nuevo estado \n",
        "      ac = action_step.action.numpy()[0]\n",
        "      idAccion, param1, param2 = parsearAccion(ac)\n",
        "      r = time_step.reward.numpy()[0]\n",
        "      ##ob = time_step.observation.numpy()[0]\n",
        "      descAccion = \"acción \" +  POSIBLES_ACCIONES_DESC[ idAccion ] + \"(\" + str(param1) + \",\" + str(param2) + \")\"\n",
        "      print(\" #\", j, \":\", descAccion, \"-> Estado/Reward \", r, \"[\", time_step, \",\", action_step, \"]\")\n",
        "      ### print(\"    Lista = \", env.pyenv.render()[0] )\n",
        "      j = j + 1\n",
        "    # muestra estado final\n",
        "    print(\" Recompensa Final = \", r )\n",
        "    print(\" Lista Final = \", env.pyenv.render()[0] )\n",
        "    return r\n",
        "\n",
        "print(\"Simulador del entorno definido.\")\n",
        "\n",
        "# Probar el entorno definido con Política Aleatoria (opcional)\n",
        "Probar_Entorno = True #@param {type:\"boolean\"}\n",
        "if Probar_Entorno:\n",
        "  SimularEntorno(eval_env, random_policy, \"Probando el entorno del problema con política al azar\")\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Entorno del Problema definido.\n",
            "\n",
            "Entornos de entrenamiento y prueba definidos. \n",
            "Simulador del entorno definido.\n",
            "\n",
            "**  Probando el entorno del problema con política al azar **\n",
            " Ini: [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 1.   , 0.299]], dtype=float32)>) ]\n",
            " Lista Inicial =  [ 8 93 95 95 34]\n",
            " # 1 : acción mover(5,2) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 0.299, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([52], dtype=int32)>, state=(), info=()) ]\n",
            " # 2 : acción mover(3,8) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 0.299, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([38], dtype=int32)>, state=(), info=()) ]\n",
            " # 3 : acción mover(2,3) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([23], dtype=int32)>, state=(), info=()) ]\n",
            " # 4 : acción mover(4,6) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([46], dtype=int32)>, state=(), info=()) ]\n",
            " # 5 : acción mover(2,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n",
            " # 6 : acción mover(0,5) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 0.299, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=()) ]\n",
            " # 7 : acción mover(4,1) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 1.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([41], dtype=int32)>, state=(), info=()) ]\n",
            " # 8 : acción mover(5,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 1.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([54], dtype=int32)>, state=(), info=()) ]\n",
            " # 9 : acción mover(5,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 1.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([55], dtype=int32)>, state=(), info=()) ]\n",
            " # 10 : acción mover(3,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 1.   , 0.977, 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([34], dtype=int32)>, state=(), info=()) ]\n",
            " # 11 : acción mover(4,2) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 1.   , 0.299, 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([42], dtype=int32)>, state=(), info=()) ]\n",
            " # 12 : acción mover(3,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 1.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([36], dtype=int32)>, state=(), info=()) ]\n",
            " # 13 : acción mover(1,3) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.299, 1.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=()) ]\n",
            " # 14 : acción mover(3,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([30], dtype=int32)>, state=(), info=()) ]\n",
            " # 15 : acción mover(3,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([39], dtype=int32)>, state=(), info=()) ]\n",
            " # 16 : acción mover(4,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([45], dtype=int32)>, state=(), info=()) ]\n",
            " # 17 : acción mover(3,1) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.977, 0.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([31], dtype=int32)>, state=(), info=()) ]\n",
            " # 18 : acción mover(4,2) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.977, 1.   , 0.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([42], dtype=int32)>, state=(), info=()) ]\n",
            " # 19 : acción mover(3,4) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.977, 1.   , 0.299, 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([34], dtype=int32)>, state=(), info=()) ]\n",
            " # 20 : acción mover(1,9) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 1.   , 0.299, 0.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([19], dtype=int32)>, state=(), info=()) ]\n",
            " # 21 : acción mover(0,2) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.299, 1.   , 0.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=()) ]\n",
            " # 22 : acción mover(0,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=()) ]\n",
            " # 23 : acción mover(1,9) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([19], dtype=int32)>, state=(), info=()) ]\n",
            " # 24 : acción mover(5,2) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 1.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([52], dtype=int32)>, state=(), info=()) ]\n",
            " # 25 : acción mover(4,4) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 1.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([44], dtype=int32)>, state=(), info=()) ]\n",
            " # 26 : acción mover(3,8) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 1.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([38], dtype=int32)>, state=(), info=()) ]\n",
            " # 27 : acción mover(4,1) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.977, 0.   , 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([41], dtype=int32)>, state=(), info=()) ]\n",
            " # 28 : acción mover(1,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 1.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([19], dtype=int32)>, state=(), info=()) ]\n",
            " # 29 : acción mover(3,9) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 1.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([39], dtype=int32)>, state=(), info=()) ]\n",
            " # 30 : acción mover(0,0) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 1.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n",
            " # 31 : acción mover(0,1) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.299, 1.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=()) ]\n",
            " # 32 : acción mover(4,5) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.299, 1.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([45], dtype=int32)>, state=(), info=()) ]\n",
            " # 33 : acción mover(0,5) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.977, 1.   , 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=()) ]\n",
            " # 34 : acción mover(3,8) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.977, 0.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([38], dtype=int32)>, state=(), info=()) ]\n",
            " # 35 : acción mover(2,8) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([28], dtype=int32)>, state=(), info=()) ]\n",
            " # 36 : acción mover(5,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.299, 1.   , 0.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([50], dtype=int32)>, state=(), info=()) ]\n",
            " # 37 : acción mover(1,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 1.   , 0.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([19], dtype=int32)>, state=(), info=()) ]\n",
            " # 38 : acción mover(2,2) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 1.   , 0.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([22], dtype=int32)>, state=(), info=()) ]\n",
            " # 39 : acción mover(3,1) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 1.   , 1.   , 0.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([31], dtype=int32)>, state=(), info=()) ]\n",
            " # 40 : acción mover(0,6) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 1.   , 0.   , 0.299, 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=()) ]\n",
            " # 41 : acción mover(1,8) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=()) ]\n",
            " # 42 : acción mover(4,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 1.   , 0.   , 0.299, 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([40], dtype=int32)>, state=(), info=()) ]\n",
            " # 43 : acción mover(3,0) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 1.   , 0.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([30], dtype=int32)>, state=(), info=()) ]\n",
            " # 44 : acción mover(1,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=()) ]\n",
            " # 45 : acción mover(1,3) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=()) ]\n",
            " # 46 : acción mover(4,5) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([45], dtype=int32)>, state=(), info=()) ]\n",
            " # 47 : acción mover(5,5) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([55], dtype=int32)>, state=(), info=()) ]\n",
            " # 48 : acción mover(5,3) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([53], dtype=int32)>, state=(), info=()) ]\n",
            " # 49 : acción mover(2,2) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([22], dtype=int32)>, state=(), info=()) ]\n",
            " # 50 : acción mover(0,0) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n",
            " # 51 : acción mover(3,4) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([34], dtype=int32)>, state=(), info=()) ]\n",
            " # 52 : acción mover(5,1) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([51], dtype=int32)>, state=(), info=()) ]\n",
            " # 53 : acción mover(3,7) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([37], dtype=int32)>, state=(), info=()) ]\n",
            " # 54 : acción mover(0,9) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 1.   , 0.977, 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=()) ]\n",
            " # 55 : acción mover(2,0) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 1.   , 0.   , 0.977, 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n",
            " # 56 : acción mover(0,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=()) ]\n",
            " # 57 : acción mover(2,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([24], dtype=int32)>, state=(), info=()) ]\n",
            " # 58 : acción mover(2,9) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 1.   , 0.977, 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, state=(), info=()) ]\n",
            " # 59 : acción mover(4,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 1.   , 0.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([40], dtype=int32)>, state=(), info=()) ]\n",
            " # 60 : acción mover(1,0) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.299, 0.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=()) ]\n",
            " # 61 : acción mover(1,1) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.299, 0.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=()) ]\n",
            " # 62 : acción mover(0,9) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 1.   , 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=()) ]\n",
            " # 63 : acción mover(2,3) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.299, 0.   , 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([23], dtype=int32)>, state=(), info=()) ]\n",
            " # 64 : acción mover(0,7) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=()) ]\n",
            " # 65 : acción mover(3,7) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([37], dtype=int32)>, state=(), info=()) ]\n",
            " # 66 : acción mover(4,5) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([45], dtype=int32)>, state=(), info=()) ]\n",
            " # 67 : acción mover(2,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n",
            " # 68 : acción mover(4,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([44], dtype=int32)>, state=(), info=()) ]\n",
            " # 69 : acción mover(2,9) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, state=(), info=()) ]\n",
            " # 70 : acción mover(0,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n",
            " # 71 : acción mover(5,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([54], dtype=int32)>, state=(), info=()) ]\n",
            " # 72 : acción mover(1,8) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.299, 1.   , 0.977, 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=()) ]\n",
            " # 73 : acción mover(3,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 1.   , 0.299, 1.   , 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([30], dtype=int32)>, state=(), info=()) ]\n",
            " # 74 : acción mover(5,1) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([51], dtype=int32)>, state=(), info=()) ]\n",
            " # 75 : acción mover(3,7) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 1.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([37], dtype=int32)>, state=(), info=()) ]\n",
            " # 76 : acción mover(3,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([36], dtype=int32)>, state=(), info=()) ]\n",
            " # 77 : acción mover(3,7) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 1.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([37], dtype=int32)>, state=(), info=()) ]\n",
            " # 78 : acción mover(4,5) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 1.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([45], dtype=int32)>, state=(), info=()) ]\n",
            " # 79 : acción mover(3,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([36], dtype=int32)>, state=(), info=()) ]\n",
            " # 80 : acción mover(0,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 1.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=()) ]\n",
            " # 81 : acción mover(5,0) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 1.   , 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([50], dtype=int32)>, state=(), info=()) ]\n",
            " # 82 : acción mover(3,2) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.977, 0.   , 0.299, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([32], dtype=int32)>, state=(), info=()) ]\n",
            " # 83 : acción mover(0,5) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.299, 1.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=()) ]\n",
            " # 84 : acción mover(0,0) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.299, 1.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n",
            " # 85 : acción mover(5,5) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.299, 1.   , 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([55], dtype=int32)>, state=(), info=()) ]\n",
            " # 86 : acción mover(3,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 1.   , 0.977]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([30], dtype=int32)>, state=(), info=()) ]\n",
            " # 87 : acción mover(3,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.299, 0.977, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([35], dtype=int32)>, state=(), info=()) ]\n",
            " # 88 : acción mover(2,6) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.977, 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([26], dtype=int32)>, state=(), info=()) ]\n",
            " # 89 : acción mover(3,5) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.977, 0.299, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([35], dtype=int32)>, state=(), info=()) ]\n",
            " # 90 : acción mover(0,9) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 0.299, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=()) ]\n",
            " # 91 : acción mover(2,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, state=(), info=()) ]\n",
            " # 92 : acción mover(5,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.977, 1.   , 1.   , 0.299]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([55], dtype=int32)>, state=(), info=()) ]\n",
            " # 93 : acción mover(4,1) -> Estado/Reward  7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.299, 0.977, 1.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([41], dtype=int32)>, state=(), info=()) ]\n",
            " Recompensa Final =  7.0\n",
            " Lista Final =  [ 8 34 93 95 95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diEOEg3JaMHa",
        "cellView": "form",
        "outputId": "87ae63ce-4c02-43ba-8590-1f7a76811cdc"
      },
      "source": [
        "#@title Definir el Agente tipo DQN\r\n",
        "\r\n",
        "##learning_rate = 1e-3  # @param {type:\"number\"}\r\n",
        "##cant_neuronas_ocultas = 100 # @param {type:\"integer\"}\r\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\r\n",
        "cant_neuronas_ocultas = \"100, 50, 25\" # @param {type:\"string\"}\r\n",
        "\r\n",
        "# Define cantidad de neuronas ocultas para RNA-Q\r\n",
        "hidden_layers = []\r\n",
        "for val in cant_neuronas_ocultas.split(','):\r\n",
        "  if  int(val) < 1:\r\n",
        "    hidden_layers.append( 10 )\r\n",
        "  else:\r\n",
        "    hidden_layers.append( int(val) )\r\n",
        "fc_layer_params = tuple(hidden_layers, )\r\n",
        "\r\n",
        "# Define RNA-Q\r\n",
        "q_net = q_network.QNetwork(\r\n",
        "    train_env.observation_spec(),\r\n",
        "    train_env.action_spec(),\r\n",
        "    fc_layer_params=fc_layer_params)\r\n",
        "\r\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\r\n",
        "\r\n",
        "train_step_counter = tf.Variable(0)\r\n",
        "\r\n",
        "# Define el agente de tipo Q\r\n",
        "ag = dqn_agent.DqnAgent(\r\n",
        "    train_env.time_step_spec(),\r\n",
        "    train_env.action_spec(),\r\n",
        "    q_network=q_net,\r\n",
        "    optimizer=optimizer,\r\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\r\n",
        "    train_step_counter=train_step_counter)\r\n",
        "\r\n",
        "ag.initialize()\r\n",
        "\r\n",
        "# define política para evaluación para el Agente\r\n",
        "eval_policy = ag.policy\r\n",
        "\r\n",
        "# define política para recolección de datos para el Agente\r\n",
        "collect_policy = ag.collect_policy\r\n",
        "\r\n",
        "print(\"Agente DQN inicializado. \")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agente DQN inicializado. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-G18iz7flcn",
        "cellView": "form",
        "outputId": "92d7cf49-db05-4d6c-bffc-513b9eefeed2"
      },
      "source": [
        "#@title Definir Métricas para evaluación\r\n",
        "\r\n",
        "# Se usa el promedio de la recompensa (la más común)\r\n",
        "# See also the metrics module for standard implementations of different metrics.\r\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\r\n",
        "\r\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\r\n",
        "\r\n",
        "  total_return = 0.0\r\n",
        "  for _ in range(num_episodes):\r\n",
        "\r\n",
        "    time_step = environment.reset()\r\n",
        "    episode_return = 0.0\r\n",
        "    while not time_step.is_last():\r\n",
        "      action_step = policy.action(time_step)\r\n",
        "      time_step = environment.step(action_step.action)\r\n",
        "      episode_return += time_step.reward\r\n",
        "    total_return += episode_return\r\n",
        "\r\n",
        "  avg_return = total_return / num_episodes\r\n",
        "  return avg_return.numpy()[0]\r\n",
        "\r\n",
        "print(\"Métricas definidas.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Métricas definidas.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLCBLMD4Zsia"
      },
      "source": [
        "3) Llevar a cabo el Entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EBRZGSkZ5N6",
        "cellView": "form",
        "outputId": "0579e314-8ee2-4510-ae2c-e3bf4efe4e14"
      },
      "source": [
        "#@title Preparar datos para Entrenamiento\r\n",
        "\r\n",
        "initial_collect_steps =   100# @param {type:\"integer\"} \r\n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\r\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\r\n",
        "batch_size = 64  # @param {type:\"integer\"}\r\n",
        "\r\n",
        "\r\n",
        "# Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\r\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n",
        "    data_spec = ag.collect_data_spec,\r\n",
        "    batch_size = train_env.batch_size,\r\n",
        "    max_length = replay_buffer_max_length)\r\n",
        "\r\n",
        "# Recolecta datos generados al azar\r\n",
        "# This loop is so common in RL, that we provide standard implementations. \r\n",
        "# For more details see the drivers module.\r\n",
        "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\r\n",
        "\r\n",
        "def collect_step(environment, policy, buffer):\r\n",
        "  time_step = environment.current_time_step()\r\n",
        "  action_step = policy.action(time_step)\r\n",
        "  next_time_step = environment.step(action_step.action)\r\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n",
        "\r\n",
        "  # Add trajectory to the replay buffer\r\n",
        "  buffer.add_batch(traj)\r\n",
        "\r\n",
        "def collect_data(env, policy, buffer, steps):\r\n",
        "  for _ in range(steps):\r\n",
        "    collect_step(env, policy, buffer)\r\n",
        "\r\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\r\n",
        "\r\n",
        "print(\"\\nDatos recolectados.\")\r\n",
        "\r\n",
        "# Muestra ejemplo de los datos recolectados\r\n",
        "##iter(replay_buffer.as_dataset()).next()\r\n",
        "\r\n",
        "# Preparar los datos recolectados con trajectories de shape [Bx2x...]\r\n",
        "dataset = replay_buffer.as_dataset(\r\n",
        "    num_parallel_calls=3, \r\n",
        "    sample_batch_size=batch_size, \r\n",
        "    num_steps=2).prefetch(3)\r\n",
        "iterator = iter(dataset)\r\n",
        "# Muestra ejemplo \r\n",
        "##iterator.next()\r\n",
        "print(\"\\nDataset creado.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Datos recolectados.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
            "\n",
            "Dataset creado.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k3S5IqGhK-a",
        "cellView": "form",
        "outputId": "ef8b1a84-e243-41dd-a632-b14d1c594618"
      },
      "source": [
        "#@title Entrenar al Agente\r\n",
        "\r\n",
        "cant_ciclos_entrenamiento = 5000 # @param {type:\"integer\"}\r\n",
        "log_cada_ciclos = 200  # @param {type:\"integer\"}\r\n",
        "evaluar_cada_ciclos = 1000  # @param {type:\"integer\"}\r\n",
        "cant_episodios_evaluacion =  25# @param {type:\"integer\"}\r\n",
        "\r\n",
        "#  Optimize by wrapping some of the code in a graph using TF function (Optional)\r\n",
        "ag.train = common.function(ag.train)\r\n",
        "\r\n",
        "# Reset the train step\r\n",
        "ag.train_step_counter.assign(0)\r\n",
        "\r\n",
        "# Evaluate the agent's policy once before training.\r\n",
        "avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n",
        "ar_ciclo = []\r\n",
        "ar_returns = []\r\n",
        "ar_loss = []\r\n",
        "\r\n",
        "print(\"\\n** Comienza el Entrenamiento **\\n\")\r\n",
        "for _ in range(cant_ciclos_entrenamiento):\r\n",
        "\r\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\r\n",
        "  collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n",
        "\r\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\r\n",
        "  experience, unused_info = next(iterator)\r\n",
        "  train_loss = ag.train(experience).loss\r\n",
        "\r\n",
        "  step = ag.train_step_counter.numpy()\r\n",
        "\r\n",
        "  if (step == 1) or (step % log_cada_ciclos == 0):\r\n",
        "    print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \r\n",
        "    ar_ciclo.append( step )\r\n",
        "    ar_loss.append( train_loss )\r\n",
        "    avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n",
        "    ar_returns.append( avg_return )\r\n",
        "\r\n",
        "    if (step == 1) or (step % evaluar_cada_ciclos == 0):\r\n",
        "      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\r\n",
        "\r\n",
        "print(\"\\n** Entrenamiento Finalizado **\\n\")\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "** Comienza el Entrenamiento **\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "step = 1: loss = 24.569\n",
            "step = 1: Promedio Recompensa = -463.8\n",
            "step = 200: loss = 157.425\n",
            "step = 400: loss = 13.804\n",
            "step = 600: loss = 5.927\n",
            "step = 800: loss = 24.470\n",
            "step = 1000: loss = 13.496\n",
            "step = 1000: Promedio Recompensa = -196.1\n",
            "step = 1200: loss = 19.274\n",
            "step = 1400: loss = 8.975\n",
            "step = 1600: loss = 14.806\n",
            "step = 1800: loss = 87.931\n",
            "step = 2000: loss = 69.400\n",
            "step = 2000: Promedio Recompensa = -22.2\n",
            "step = 2200: loss = 34.905\n",
            "step = 2400: loss = 43.096\n",
            "step = 2600: loss = 72.598\n",
            "step = 2800: loss = 113.148\n",
            "step = 3000: loss = 50.837\n",
            "step = 3000: Promedio Recompensa = 92.5\n",
            "step = 3200: loss = 24.805\n",
            "step = 3400: loss = 66.287\n",
            "step = 3600: loss = 38.468\n",
            "step = 3800: loss = 82.248\n",
            "step = 4000: loss = 75.141\n",
            "step = 4000: Promedio Recompensa = 51.4\n",
            "step = 4200: loss = 20.990\n",
            "step = 4400: loss = 37.434\n",
            "step = 4600: loss = 53.773\n",
            "step = 4800: loss = 32.200\n",
            "step = 5000: loss = 48.928\n",
            "step = 5000: Promedio Recompensa = 22.6\n",
            "\n",
            "** Entrenamiento Finalizado **\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "9EBBl7mRkQYa",
        "cellView": "form",
        "outputId": "6701db2c-ed57-47ff-ca13-5354dfe822a1"
      },
      "source": [
        "#@title Mostrar Gráficos del Entrenamiento\r\n",
        "\r\n",
        "\r\n",
        "plt.figure(figsize=(12,5)) \r\n",
        "plt.plot( ar_ciclo, ar_returns)\r\n",
        "plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\r\n",
        "#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n",
        "plt.ylabel('Valor')\r\n",
        "plt.xlabel('Ciclo')\r\n",
        "plt.xlim(right=max(ar_ciclo))   \r\n",
        "plt.grid(True)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.figure(figsize=(12,5)) \r\n",
        "#plt.plot( ar_ciclo, ar_returns)\r\n",
        "plt.plot( ar_ciclo, ar_loss, color=\"red\" )\r\n",
        "plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\r\n",
        "#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n",
        "plt.ylabel('Valor')\r\n",
        "plt.xlabel('Ciclo')\r\n",
        "plt.xlim(right=max(ar_ciclo))   \r\n",
        "plt.grid(True)\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j79yPUetlUbs"
      },
      "source": [
        "4) Probar entrenamiento comparando resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLkdkcBjl3Xs",
        "cellView": "form",
        "outputId": "40a2caa0-b3af-4644-f4ee-ce1a9e7e0d3e"
      },
      "source": [
        "#@title Probar el Agente Entrenado contra el Azar\r\n",
        "cantidad_probar = 1 # @param {type:\"integer\"}\r\n",
        "promAzar = 0\r\n",
        "promAgente = 0\r\n",
        "\r\n",
        "for i in range(cantidad_probar):\r\n",
        "\r\n",
        "  print(\"\\n> Prueba \", i+1, \":\")\r\n",
        "\r\n",
        "  # crea nuevo entorno que mantiene la misma lista\r\n",
        "  prueba_env =  tf_py_environment.TFPyEnvironment( OrdenarListasEnv(False) )\r\n",
        "\r\n",
        "  # Probar Aleatorio\r\n",
        "  valorAzar = SimularEntorno(prueba_env, random_policy, \"Resultados Aleatorio\") \r\n",
        "  promAzar = promAzar + valorAzar\r\n",
        "\r\n",
        "  # Probar Agente Entrenado\r\n",
        "  valorAgente = SimularEntorno(prueba_env, ag.policy, \"Resultados de Agente Entrenado\") \r\n",
        "  promAgente = promAgente + valorAgente\r\n",
        "\r\n",
        "  # Decide Ganador\r\n",
        "  if valorAzar < valorAgente:\r\n",
        "    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera MEJOR resultado que el azar (\", valorAzar,\")\")\r\n",
        "  else:\r\n",
        "    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera PEOR resultado que el azar (\", valorAzar,\")\")\r\n",
        "\r\n",
        "# Decide Ganador General\r\n",
        "if cantidad_probar > 0:\r\n",
        "  promAgente = promAgente / cantidad_probar\r\n",
        "  promAzar = promAzar / cantidad_probar\r\n",
        "  print(\"\\n================================================================================================\\n\")\r\n",
        "  if promAzar < promAgente:\r\n",
        "    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene MEJORES resultado que  el azar (\", promAzar,\")\")\r\n",
        "  else:\r\n",
        "    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene PEORES resultados que el azar (\", promAzar,\")\")\r\n",
        "  print(\"\\n================================================================================================\\n\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> Prueba  1 :\n",
            "\n",
            "**  Resultados Aleatorio **\n",
            " Ini: [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.661, 0.   , 1.   ]], dtype=float32)>) ]\n",
            " Lista Inicial =  [ -9 -59  10 -99  66]\n",
            " # 1 : acción mover(2,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.   , 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([23], dtype=int32)>, state=(), info=()) ]\n",
            " # 2 : acción mover(0,0) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.   , 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n",
            " # 3 : acción mover(4,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.   , 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([45], dtype=int32)>, state=(), info=()) ]\n",
            " # 4 : acción mover(3,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.   , 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([33], dtype=int32)>, state=(), info=()) ]\n",
            " # 5 : acción mover(2,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.661, 1.   , 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([28], dtype=int32)>, state=(), info=()) ]\n",
            " # 6 : acción mover(4,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.661, 1.   , 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([48], dtype=int32)>, state=(), info=()) ]\n",
            " # 7 : acción mover(5,5) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.661, 1.   , 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([55], dtype=int32)>, state=(), info=()) ]\n",
            " # 8 : acción mover(2,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 1.   , 0.   , 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([28], dtype=int32)>, state=(), info=()) ]\n",
            " # 9 : acción mover(3,7) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 1.   , 0.661, 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([37], dtype=int32)>, state=(), info=()) ]\n",
            " # 10 : acción mover(3,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 1.   , 0.   , 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([38], dtype=int32)>, state=(), info=()) ]\n",
            " # 11 : acción mover(5,3) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 1.   , 0.661, 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([53], dtype=int32)>, state=(), info=()) ]\n",
            " # 12 : acción mover(4,1) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.   , 0.242, 1.   , 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([41], dtype=int32)>, state=(), info=()) ]\n",
            " # 13 : acción mover(3,8) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.   , 0.242, 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([38], dtype=int32)>, state=(), info=()) ]\n",
            " # 14 : acción mover(5,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.   , 0.242, 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([50], dtype=int32)>, state=(), info=()) ]\n",
            " # 15 : acción mover(0,6) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.   , 0.242, 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=()) ]\n",
            " # 16 : acción mover(2,1) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.   , 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([21], dtype=int32)>, state=(), info=()) ]\n",
            " # 17 : acción mover(5,1) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 1.   , 0.242, 0.   , 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([51], dtype=int32)>, state=(), info=()) ]\n",
            " # 18 : acción mover(4,7) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 1.   , 0.242, 0.   , 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([47], dtype=int32)>, state=(), info=()) ]\n",
            " # 19 : acción mover(1,0) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.242, 0.   , 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=()) ]\n",
            " # 20 : acción mover(4,8) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.242, 0.   , 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([48], dtype=int32)>, state=(), info=()) ]\n",
            " # 21 : acción mover(3,5) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.242, 0.661, 0.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([35], dtype=int32)>, state=(), info=()) ]\n",
            " # 22 : acción mover(0,8) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.661, 0.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n",
            " # 23 : acción mover(1,7) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.661, 0.   , 1.   , 0.242]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=()) ]\n",
            " # 24 : acción mover(5,0) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.242, 0.545, 0.661, 0.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([50], dtype=int32)>, state=(), info=()) ]\n",
            " # 25 : acción mover(2,2) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.242, 0.545, 0.661, 0.   , 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([22], dtype=int32)>, state=(), info=()) ]\n",
            " # 26 : acción mover(0,6) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.661, 0.   , 1.   , 0.242]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>, state=(), info=()) ]\n",
            " # 27 : acción mover(1,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.   , 1.   , 0.242, 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=()) ]\n",
            " # 28 : acción mover(4,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.661, 0.545, 0.   , 1.   , 0.242]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([40], dtype=int32)>, state=(), info=()) ]\n",
            " # 29 : acción mover(1,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.661, 0.   , 1.   , 0.242, 0.545]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([19], dtype=int32)>, state=(), info=()) ]\n",
            " # 30 : acción mover(2,0) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.661, 0.   , 0.242, 0.545]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n",
            " # 31 : acción mover(4,0) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 1.   , 0.661, 0.   , 0.242]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([40], dtype=int32)>, state=(), info=()) ]\n",
            " # 32 : acción mover(1,0) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.661, 0.   , 0.242]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=()) ]\n",
            " # 33 : acción mover(4,6) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.661, 0.   , 0.242]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([46], dtype=int32)>, state=(), info=()) ]\n",
            " # 34 : acción mover(0,8) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.661, 0.   , 0.242, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n",
            " # 35 : acción mover(4,0) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.661, 0.   , 0.242]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([40], dtype=int32)>, state=(), info=()) ]\n",
            " # 36 : acción mover(2,6) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.545, 0.   , 0.242, 0.661]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([26], dtype=int32)>, state=(), info=()) ]\n",
            " # 37 : acción mover(1,7) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1.   , 0.   , 0.242, 0.661, 0.545]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([17], dtype=int32)>, state=(), info=()) ]\n",
            " # 38 : acción mover(0,9) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.242, 0.661, 0.545, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([9], dtype=int32)>, state=(), info=()) ]\n",
            " # 39 : acción mover(5,4) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.242, 0.661, 0.545, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([54], dtype=int32)>, state=(), info=()) ]\n",
            " # 40 : acción mover(2,3) -> Estado/Reward  60.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([60.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.242, 0.545, 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([23], dtype=int32)>, state=(), info=()) ]\n",
            " Recompensa Final =  60.0\n",
            " Lista Final =  [-99 -59  -9  10  66]\n",
            "\n",
            "**  Resultados de Agente Entrenado **\n",
            " Ini: [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.545, 0.242, 0.661, 0.   , 1.   ]], dtype=float32)>) ]\n",
            " Lista Inicial =  [ -9 -59  10 -99  66]\n",
            " # 1 : acción mover(3,0) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.545, 0.242, 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([30], dtype=int32)>, state=(), info=()) ]\n",
            " # 2 : acción mover(2,1) -> Estado/Reward  98.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([98.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[0.   , 0.242, 0.545, 0.661, 1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([21], dtype=int32)>, state=(), info=()) ]\n",
            " Recompensa Final =  98.0\n",
            " Lista Final =  [-99 -59  -9  10  66]\n",
            "\n",
            "--> El Agente Entrenado ( 98.0 ) genera MEJOR resultado que el azar ( 60.0 )\n",
            "\n",
            "================================================================================================\n",
            "\n",
            "= En promedio, el Agente Entrenado ( 98.0 ) tiene MEJORES resultado que  el azar ( 60.0 )\n",
            "\n",
            "================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjj7bRe7AB-l"
      },
      "source": [
        "5) Graba el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAllujCaAGUM",
        "outputId": "37ef4a32-1ee5-4195-bedc-3e9278edae71"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "modelDir = '/gdrive/MyDrive/IA/demo Agentes/Modelos'"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V2EiqdwAy_R",
        "outputId": "44976e7c-76ed-4cad-b932-4ab586e41057"
      },
      "source": [
        "# guarda la politica del agente entrenado\r\n",
        "policy_dir = os.path.join(modelDir, 'policy-OrdenarLista')\r\n",
        "tf_policy_saver = policy_saver.PolicySaver(ag.policy)\r\n",
        "tf_policy_saver.save(policy_dir)\r\n",
        "\r\n",
        "# para cargarla :  saved_policy = tf.compat.v2.saved_model.load(policy_dir)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_and_return_conditional_losses, QNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 35). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as QNetwork_layer_call_and_return_conditional_losses, QNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 35). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista/assets\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}