{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN ANT-Turtle Random Pos Problem.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMsN3YeLQ22HnizoIF9+EFT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para resolver el problema de encontrar un Máximo Óptimo definido al azar en un plano haciendo mover un Agente de tipo Hormiga-Tortuga (ANT-Turtle):\r\n"]},{"cell_type":"markdown","metadata":{"id":"dliJD0WRUMWV"},"source":["0) Preparar el ambiente:"]},{"cell_type":"code","metadata":{"id":"Qxbe02w0T0ip","cellView":"form"},"source":["#@title Instalar Paquete de TF-Agents\r\n","!pip install -q tf-agents\r\n","print(\"TF-Agentes instalado.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJl4YsniURev","cellView":"form"},"source":["#@title Cargar Librerías\r\n","from __future__ import absolute_import\r\n","from __future__ import division\r\n","from __future__ import print_function\r\n","\r\n","import abc\r\n","import tensorflow as tf\r\n","import numpy as np\r\n","import matplotlib\r\n","import matplotlib.pyplot as plt\r\n","\r\n","import random\r\n","from random import randint, sample\r\n","import math\r\n","import operator\r\n","import copy\r\n","import pickle\r\n","import codecs\r\n","import IPython\r\n","\r\n","\r\n","from tf_agents.environments import py_environment\r\n","from tf_agents.environments import tf_py_environment\r\n","\r\n","from tf_agents.environments import utils\r\n","from tf_agents.specs import array_spec\r\n","\r\n","from tf_agents.policies import random_tf_policy\r\n","\r\n","from tf_agents.trajectories import time_step as ts\r\n","\r\n","from tf_agents.agents.dqn import dqn_agent\r\n","from tf_agents.networks import q_network\r\n","from tf_agents.agents.categorical_dqn import categorical_dqn_agent\r\n","from tf_agents.networks import categorical_q_network\r\n","\r\n","from tf_agents.utils import common\r\n","\r\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n","from tf_agents.trajectories import trajectory\r\n","\r\n","import os\r\n","from tf_agents.policies import policy_saver\r\n","\r\n","tf.compat.v1.enable_v2_behavior()\r\n","\r\n","print(\"Librerías cargadas.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ONe5w_nUYME"},"source":["1) Establecer las clases sobre el Problema a resolver:"]},{"cell_type":"code","metadata":{"id":"7dwMFYK2Un8Y","cellView":"form"},"source":["#@title Parámetros Generales:\r\n","# Espacio mínimo y máximo del espacio de búsqueda\r\n","CANT_MAXIMOS_LOCALES = 1 #@param {type:\"slider\", min:0, max:10, step:1}\r\n","MIN_ESPACIO_BUSQ = 0 #@param {type:\"slider\", min:-100, max:0, step:5}\r\n","MAX_ESPACIO_BUSQ = 30  #@param {type:\"slider\", min:0, max:100, step:5}\r\n","CANT_OBSTACULOS = 100  #@param {type:\"slider\", min:0, max:1000, step:10}\r\n","\r\n","# otros parámetros fijos\r\n","EVAPORACION_FEROMONAS = 1\r\n","ATRACCION_FEROMONAS = 0\r\n","ATRACCION_HEURISTICA = 0\r\n","\r\n","print(\"Parámetros definidos.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJQjSolbUViV","cellView":"form"},"source":["#@title Definir clase MAPA DE BÚSQUEDA (notar que es la misma usada en ANT de SWARM)\r\n","# Nota: las funciones de uso de feromonas se mantienen por compatibilidad pero no se usan\r\n","\r\n","class MapaBusqueda(object):\r\n","\r\n","    def __init__(self, limMin, limMax, cantMaximosLocales=0, cantObstaculos=0, valorIniFeromonas=1):\r\n","          self.limMin = limMin\r\n","          self.limMax = limMax\r\n","          self.cantMax = cantMaximosLocales\r\n","          self.cantObstaculos = cantObstaculos\r\n","          self.generarPosicionesRnd()\r\n","          self.inicializarFeromonas()\r\n","\r\n","    def _reset(self):\r\n","          #self.generarPosicionesRnd()\r\n","          self.inicializarFeromonas()\r\n","\r\n","    def generarPosicionesRnd(self):\r\n","      \r\n","        # Define Posiciíon inicial de las Hormigas\r\n","        self.Hormiguero = [ random.randint(self.limMin, self.limMax),  random.randint(self.limMin, self.limMax) ]      \r\n","\r\n","        # Define Posiciones Random para Máximos Locales y Óptimos\r\n","        # con Heurística también al Azar (entre 1 y 100)\r\n","        # controla que el máximo local/global no se ubique en la misma posición que el Hormiguero\r\n","        self.MaximosPos = []\r\n","        self.MaximosVal = []\r\n","        self.MaxOptimoID = 0\r\n","        auxMejorVal = -99\r\n","        i = 0\r\n","\r\n","        # Agrega el maximo global (siempre vale 100)\r\n","        auxPos = [ random.randint(self.limMin, self.limMax),  random.randint(self.limMin, self.limMax) ]\r\n","        self.MaximosPos.append( auxPos )\r\n","        self.MaximosVal.append( 100 )  \r\n","\r\n","        # Agrega máximos locales\r\n","        while len(self.MaximosPos)<(self.cantMax):\r\n","\r\n","            auxPos = [ random.randint(self.limMin, self.limMax),  random.randint(self.limMin, self.limMax) ]      \r\n","\r\n","            if (self.Hormiguero != auxPos):\r\n","                auxVal = random.randint(1, 90)\r\n","                self.MaximosPos.append( auxPos )\r\n","                self.MaximosVal.append( auxVal )\r\n","                \r\n","                if self.MaximosVal[self.MaxOptimoID] < auxVal:\r\n","                    self.MaxOptimoID = i\r\n","                i= i + 1\r\n","\r\n","        # Agrega los obstáculos controlando que no se ubique en una posición igual al hormiguero o a un máximo\r\n","        self.ObstaculosPos = []\r\n","        while len(self.ObstaculosPos)<self.cantObstaculos:\r\n","              auxPos = [ random.randint(self.limMin, self.limMax),  random.randint(self.limMin, self.limMax) ]\r\n","                  \r\n","              i = 0\r\n","              agregaObstaculo = True\r\n","              while agregaObstaculo and i<len(self.MaximosPos):\r\n","                if (self.MaximosPos[i] == auxPos) or (self.Hormiguero == auxPos):\r\n","                      agregaObstaculo = False                      \r\n","                i= i + 1\r\n","              \r\n","              if agregaObstaculo:\r\n","                    self.ObstaculosPos.append( auxPos )      \r\n","\r\n","        return True\r\n","   \r\n","    def printInformacion(self):\r\n","        # Muestra la información sobre el Mapa de Búsqueda\r\n","        print(\"\\n++ Ubicación del Hormiguero: \", self.Hormiguero)\r\n","\r\n","        print(\"\\n## Obstáculos Generados: \", len(self.ObstaculosPos))\r\n","        for auxPos in self.ObstaculosPos:\r\n","            print(\"  -->\", auxPos)\r\n","\r\n","        print(\"\\n** Máximos Generados: \", len(self.MaximosPos))\r\n","        for auxPos, auxVal in zip(self.MaximosPos, self.MaximosVal):\r\n","            print(\"  -->\", auxPos, \"{ \", auxVal, \" } \")\r\n","        print(\"\\n** Posición Máximo Óptimo: \", self.MaximosPos[self.MaxOptimoID], \" { \",  self.MaximosVal[self.MaxOptimoID],\" } **\")\r\n","        print(\"\\n\")\r\n","\r\n","    @property\r\n","    def posHormiguero(self):\r\n","        return self.Hormiguero         \r\n","\r\n","    @property\r\n","    def posMaximos(self):\r\n","        return self.MaximosPos    \r\n","\r\n","    @property\r\n","    def valMaximos(self):\r\n","        return self.MaximosVal   \r\n","        \r\n","    @property\r\n","    def posMaximoGlobal(self):\r\n","        return self.MaximosPos[self.MaxOptimoID]         \r\n","    \r\n","    @property\r\n","    def valMaximoGlobal(self):\r\n","        return self.MaximosVal[self.MaxOptimoID]  \r\n","\r\n","    @property\r\n","    def posObstaculos(self):\r\n","        return self.ObstaculosPos\r\n","\r\n","    def estaEnMaximo(self, posicion):\r\n","      # Indica si la posición corresponde o no a un Máximo \r\n","      # si es verdadero devuelve el valor correspondiente, sino 0\r\n","      for i in range(len(self.MaximosPos)):\r\n","        if posicion == self.MaximosPos[i]:\r\n","          return self.MaximosVal[i]\r\n","      return 0\r\n","\r\n","    def heuristica(self, posicion):\r\n","        # Define la Función Heurística para evaluar una posicion del mapa\r\n","        devuelve = 0\r\n","        disPosMenor = self.limMax\r\n","        for posMax, valMax in zip(self.MaximosPos, self.MaximosVal):\r\n","\r\n","              # calcula la distancia al punto de la Particula\r\n","              sqerrors = ((x - y)**2 for x, y in zip(posicion, posMax))\r\n","              distPos =  math.fsum(sqerrors)**0.5 / 10\r\n","              \r\n","              # Si es la menor distancia calcula la Heurística\r\n","              if disPosMenor > distPos:\r\n","                  disPosMenor = distPos \r\n","                  devuelve = ( valMax - distPos) \r\n","\r\n","          # Por las dudas controla que devuelva un valor positivo\r\n","        return max(devuelve,0)\r\n","\r\n","    def inicializarFeromonas(self):\r\n","        # Inicializa el mapa de feromonas como un diccionario del tamaño prefijado con valor constante \r\n","        self.mapaFeromonas = dict()\r\n","        self.feroDft = 1\r\n","        ## Nota: no se inicializa el mapa de feromonas con valores para no llenar la memoria\r\n","        ## lo que se hace es que si una posición es solicitada y no existe, \r\n","        ## se asume que el valor de 'self.feroDft'\r\n","        #for x in range(self.limMin,self.limMax):\r\n","        #    for y in range(self.limMin,self.limMax):              \r\n","        #      self.mapaFeromonas[x,y] = 1\r\n","        return True\r\n","\r\n","    def evaporarFeromonas(self, factorEvaporacion=EVAPORACION_FEROMONAS):\r\n","        # actualiza el valor por defecto\r\n","        self.feroDft = (1-factorEvaporacion)*self.feroDft\r\n","\r\n","        # actualiza el valor de las posiciones definidas\r\n","        for pos in self.mapaFeromonas:\r\n","            self.mapaFeromonas[pos] = (1-factorEvaporacion)*self.mapaFeromonas[pos] \r\n","        return True\r\n","\r\n","    def actualizaFeromonasRecorrido(self, recorrido, valorRecorrido):\r\n","        # si el valor del recorrido es cero o menos, no se actualiza la cantidad de feromonas\r\n","        if valorRecorrido<=0:\r\n","          return False\r\n","        # si es mayor a cero se actualizan las feromonas \r\n","        # de acuerdo al recorrido que tuvieron la hormigas y su valor\r\n","        valAplicar = valorRecorrido/100\r\n","        for posRec in recorrido:\r\n","              tpos = tuple(posRec)\r\n","              if tpos in self.mapaFeromonas:\r\n","                    self.mapaFeromonas[tpos] = self.mapaFeromonas[tpos] + valAplicar\r\n","              else:\r\n","                    self.mapaFeromonas[tpos] = self.feroDft + valAplicar\r\n","        return True\r\n","\r\n","    def cantFeromonas(self, posicion):\r\n","        # Nota: como no se inicializa el mapa de feromonas con valores para no llenar la memoria\r\n","        # lo que se hace es que si una posición es solicitada y no existe, \r\n","        ## se asume que el valor de 'self.feroDft' \r\n","        if (self.mapaFeromonas == None) or (self.mapaFeromonas == dict()):\r\n","            return self.feroDft\r\n","        tpos = tuple(posicion)\r\n","        if tpos in self.mapaFeromonas:\r\n","            return self.mapaFeromonas[tpos]\r\n","        else:\r\n","            return self.feroDft\r\n","\r\n","    def mostrarMapaFeromonas(self):\r\n","        # Genera el gráfico con el Mapa de Fermonas:\r\n","        plt.figure(figsize=(15,8)) \r\n","\r\n","        ## -- en Amarillo: para feromonas menores a 1 \r\n","        ## -- en Naranja: para feromonas iguales  a 1 \r\n","        ## -- en Marron: para feromonas mayores a 1 \r\n","        if self.feroDft < 1:   \r\n","            plt.rcParams['axes.facecolor'] = 'yellow'\r\n","        elif self.feroDft == 1:   \r\n","            plt.rcParams['axes.facecolor'] = 'orangered'\r\n","        else:\r\n","            plt.rcParams['axes.facecolor'] = 'saddlebrown'  \r\n","        for pos in self.mapaFeromonas:\r\n","            if self.mapaFeromonas[pos] < 1:\r\n","                plt.scatter(pos[0], pos[1], color='yellow')\r\n","            elif self.mapaFeromonas[pos] == 1:\r\n","                plt.scatter(pos[0], pos[1], color='orangered')\r\n","            else:\r\n","                plt.scatter(pos[0], pos[1], color='saddlebrown')\r\n","        \r\n","        ## -- en Verde: Posición del Máximo Global \r\n","        plt.scatter(mapa.posMaximoGlobal[0], mapa.posMaximoGlobal[1], color='green', s=150)\r\n","\r\n","        ## --en Violeta: Posición del Homiguero\r\n","        plt.scatter(mapa.posHormiguero[0], mapa.posHormiguero[1], color='violet', s=100)\r\n","\r\n","        plt.title('Mapa de Feromonas')        \r\n","        plt.xlim(self.limMin, self.limMax)\r\n","        plt.ylim(self.limMin, self.limMax)\r\n","        plt.grid(False)\r\n","        plt.show()\r\n","      \r\n","    def probabAsignada(self, posicion, atraccFeromonas=ATRACCION_FEROMONAS, atraccHeuristica=ATRACCION_HEURISTICA):           \r\n","        # si es una ubicación de un obstáculo, devuelve 0 para que no sea elegida\r\n","        for o in self.ObstaculosPos:\r\n","            if o == posicion:\r\n","              return 0\r\n","        \r\n","        # si la posición está fuera del espacio de búsqueda, devuelve 0 para que no sea elegida\r\n","        for pos in posicion:                \r\n","            if pos<self.limMin:\r\n","                  return 0\r\n","            if pos>self.limMax:\r\n","                  return 0\r\n","\r\n","        # determina el valor de probabilidad correspondiente        \r\n","        f = self.cantFeromonas(posicion)\r\n","        h = self.heuristica(posicion)\r\n","        probab = f**atraccFeromonas * h**atraccHeuristica\r\n","\r\n","        return probab   \r\n","\r\n","print(\"\\nClase Mapa de Búqueda definida\") \r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxJPPx0OVGR3","cellView":"form"},"source":["#@title Definir clase Ant-Turtle (basada en la usada para SWARM pero con cambios)\r\n","# Nota: en esta versión no se usan las feromonas y se agregan métodos para desplazar más acotado\r\n","\r\n","class HormigaTortuga(object):\r\n","\r\n","    def __init__(self, posInicial):        \r\n","        self.posActual = posInicial\r\n","        self.despRecorrido = [ [posInicial] ]\r\n","        self.direccionGrados = 0\r\n","        self.valRecorrido = 0\r\n","\r\n","    @property\r\n","    def recorridoEncuentraSolucion(self):\r\n","      return (self.valRecorrido > 0)\r\n","\r\n","    @property\r\n","    def recorridoUltimo(self):\r\n","        return self.despRecorrido   \r\n","\r\n","    @property\r\n","    def valRecorridoUltimo(self):\r\n","        return self.valRecorrido\r\n","\r\n","    @property\r\n","    def direccionActual(self):\r\n","        return self.direccionGrados\r\n","\r\n","    @property\r\n","    def posicionActual(self):\r\n","        return self.posActual\r\n","\r\n","    def determinaMovDireccion(self, direccionGrados):\r\n","        # determina hacia donde se mueve usando los grados de la dirección\r\n","        if direccionGrados >= 0 and direccionGrados < 45:\r\n","          mov = [0, -1]\r\n","        elif direccionGrados >= 45 and direccionGrados < 90:\r\n","          mov = [1, -1]\r\n","        elif direccionGrados >= 90 and direccionGrados < 135:\r\n","          mov = [1, 0]\r\n","        elif direccionGrados >= 135 and direccionGrados < 180:\r\n","          mov = [1, 1]\r\n","        elif direccionGrados >= 180 and direccionGrados < 225:\r\n","          mov = [0, 1]\r\n","        elif direccionGrados >= 225 and direccionGrados < 270:\r\n","          mov = [-1, 1]\r\n","        elif direccionGrados >= 270 and direccionGrados < 315:\r\n","          mov = [-1, 0]\r\n","        elif direccionGrados >= 315 and direccionGrados < 360:\r\n","          mov = [-1, -1]\r\n","        else:\r\n","          mov = [0, 0]\r\n","        return mov\r\n","\r\n","    def evaluarPosicion(self, mapa, pos):\r\n","        # Se fija que no supere ningún límite\r\n","        if pos[0] < mapa.limMin or pos[0] > mapa.limMax:\r\n","          return -1\r\n","        elif pos[1] < mapa.limMin or pos[1] > mapa.limMax:\r\n","          return -1\r\n","        # Se fija que no sea un obstáculo      \r\n","        elif pos in mapa.posObstaculos:\r\n","          return -1\r\n","        else:\r\n","          # Evalúa si se encontro algún Máximo (local o global)\r\n","          return mapa.estaEnMaximo(pos)\r\n","\r\n","    def ve(self, mapa, cantPasosVe=3):\r\n","        # devuelve matriz de objetos que ve\r\n","        # en la dirección que está y sus costados\r\n","        resVeCompleto = []       \r\n","        for iGrados in range(8):\r\n","          # inicializa lista auxiliar \r\n","          # de lo que ve en esa dirección\r\n","          resVe = np.zeros(cantPasosVe)\r\n","          # inicializa dirección en que ve\r\n","          nDir = (self.direccionGrados + (iGrados*45)) % 360           \r\n","          if nDir in [270, 315, 0, 45, 90]:\r\n","            # si corresponde a adelante o los costados\r\n","            movVe = self.determinaMovDireccion(nDir)\r\n","            # inicializ posición inicial a ver\r\n","            posVe = copy.deepcopy( self.posActual )\r\n","            for i in range( cantPasosVe ):\r\n","              # determina nueva posición\r\n","              nPosVe = []\r\n","              for p, m in zip(posVe, movVe):\r\n","                nPosVe.append( p + m )            \r\n","              # se fija que hay en esa posición y lo registra\r\n","              resVe[i]  = self.evaluarPosicion(mapa, nPosVe)\r\n","              # actualiza posición\r\n","              posVe = nPosVe\r\n","          # registra en visión completa\r\n","          resVeCompleto.append( resVe )\r\n","        # devuelve 8 listas:\r\n","        #       5 listas (adelante y costados) con valores:\r\n","        #         -1 para obstáculos o limites\r\n","        #          0 para posiciones normales\r\n","        #         >0 para máximos\r\n","        #       3 listas (atrás) con valores en 0\r\n","        return resVeCompleto\r\n","\r\n","    def recuerda(self, mapa, cantPasosRecuerda=3):\r\n","        # devuelve matriz de objetos que recuerda\r\n","        # en todas las direcciones\r\n","        todos_pos_recorrido = [pos for subRecorrido in self.despRecorrido for pos in subRecorrido]\r\n","        resRecuerdaCompleto = []    \r\n","        for iGrados in range(8):\r\n","          # inicializa dirección en que recuerda\r\n","          nDir = (self.direccionGrados + (iGrados*45)) % 360\r\n","          movRec = self.determinaMovDireccion(nDir)\r\n","          # inicializa posición inicial a recordar\r\n","          posRec = copy.deepcopy( self.posActual )\r\n","          # inicializa lista auxiliar \r\n","          # de lo que recuerda en esa dirección\r\n","          resRec = np.ones(cantPasosRecuerda)\r\n","          for i in range( cantPasosRecuerda ):\r\n","            # determina nueva posición\r\n","            nPosRec = []\r\n","            for p, m in zip(posRec, movRec):\r\n","              nPosRec.append( p + m )            \r\n","            # se fija si paso por esa posición\r\n","            if nPosRec in todos_pos_recorrido:\r\n","              resRec[i] = 0\r\n","            # actualiza posición\r\n","            posRec = nPosRec\r\n","          # registra en visión completa\r\n","          resRecuerdaCompleto.append( resRec )\r\n","        # devuelve 8 listas con valores:\r\n","        #           1 para posiciones que no paso    \r\n","        #           0 para posiciones por las que paso          \r\n","        return resRecuerdaCompleto\r\n","\r\n","\r\n","    def desplazarse(self, mapa, gradosGirar, cantPasos=1):\r\n","        # contra que la cantidad de pasos sea válida\r\n","        if cantPasos <= 0:\r\n","          self.despRecorrido.append( [self.posActual] )\r\n","          return 0\r\n","\r\n","        # calcula nuevos grados\r\n","        self.direccionGrados = (self.direccionGrados + gradosGirar) % 360      \r\n","        mov = self.determinaMovDireccion( self.direccionGrados )\r\n","        # si no es una dirección válida, no avanza\r\n","        if mov == [0, 0]:          \r\n","          self.despRecorrido.append( [self.posActual] )\r\n","          return 0         \r\n","\r\n","        # realiza el desplazamiento\r\n","        paso = 1\r\n","        auxRecorrido = []\r\n","        while paso <= cantPasos:\r\n","          # simula nueva posición\r\n","          nPos = []\r\n","          for p, m in zip(self.posActual, mov):\r\n","            nPos.append( p + m )\r\n","          valNPos = self.evaluarPosicion(mapa, nPos)\r\n","          if valNPos == -1:\r\n","            # es un obstáculo por lo que no se puede seguir avanzando \r\n","            # en esa dirección\r\n","            break\r\n","          else:\r\n","            # actualiza la posición actual\r\n","            self.posActual = nPos\r\n","            auxRecorrido.append( nPos )\r\n","            if valNPos > 0:\r\n","              # se encontro algún máximo, finaliza\r\n","              self.maxEncontrado = True\r\n","              self.valRecorrido = valNPos\r\n","              break\r\n","            else:\r\n","              # continua la búsqueda\r\n","              paso = paso + 1\r\n","        # devuelve la cantidad de pasos realizados\r\n","        self.despRecorrido.append( auxRecorrido )\r\n","        return (paso/cantPasos)\r\n","\r\n","print(\"\\nClase Ant-Turtle definida\")\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AF3C4LdwkZXa","cellView":"form"},"source":["#@title Definir funciones para generar el Gráfico con las posiciones de las partículas\r\n","\r\n","# Librerías especiales para usar\r\n","import matplotlib as mpl\r\n","from matplotlib import animation, rc\r\n","from IPython.display import HTML\r\n","\r\n","# método que se usa para generar gradiente de colores\r\n","def colorFader(c1, c2, mix=0.0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\r\n","    c1 = np.array(mpl.colors.to_rgb(c1))\r\n","    c2 = np.array(mpl.colors.to_rgb(c2))\r\n","    return mpl.colors.to_hex((1-mix)*c1 + mix*c2)\r\n","\r\n","# Método que se ejecuta por cada frame para mostrar \r\n","def updatePlot(i, ciclosPos, scat, axi):\r\n","    if axi is None or ciclosPos is None:\r\n","      return scat,\r\n","\r\n","    axi.set_xlabel('Ciclo: ' + str(i), fontsize=12)\r\n","  \r\n","      # Si no es el último ciclo, muestra también el global\r\n","    if i<len(ciclosPos):     \r\n","\r\n","      if len(ciclosPos[i])>0:      \r\n","        # marca posiciones anteriores en tamaño más pequeño\r\n","        # Separa las coordenadas x, y de las posiciones en el ciclo i              \r\n","        X, Y = zip(*ciclosPos[i])\r\n","        axi.scatter(X, Y, color='red', s=10)\r\n","        \r\n","        # Hace mover a las Partículas (Rojo)\r\n","        scat.set_offsets(ciclosPos[i])   \r\n","\r\n","    # Devuelve una lista de \"artistas\" para dibujar, \r\n","    # en este caso es sólo uno por lo que se pone una coma final\r\n","    return scat,    \r\n","\r\n","# Función para preparar el gráfico          \r\n","def PrepararGrafico(mapa, MIN_ESPACIO_BUSQ, MAX_ESPACIO_BUSQ, ciclosPos): \r\n","\r\n","    #fig = plt.figure(figsize=(14,7))\r\n","    fig = plt.figure(figsize=(13,6))\r\n","    ax = fig.add_subplot(111)\r\n","    plt.close()\r\n","\r\n","    textoTitulo = \"Gráfico del Movimiento de la Hormiga-Tortuga\"\r\n","    textoDesc = '\\n** Posición Máximo Óptimo a buscar : ' + str(mapa.posMaximoGlobal) + ' { ' +  str(mapa.valMaximoGlobal) +' } **'\r\n","    textoDesc = textoDesc + '\\n-- Colores:  Máximos Locales (AZUL) - Máximo Óptimo (VERDE) - '  \r\n","    textoDesc = textoDesc + '\\n- Obstáculos (NEGRO) - Hormiguero (VIOLETA) -- ' \r\n","    textoDesc = textoDesc + '\\n- Recorrido (ROJO) --- ' \r\n","\r\n","    ## --en Negro: posiciones de los obstáculos a esquivar\r\n","    for posi in mapa.posObstaculos:\r\n","        ax.scatter(posi[0], posi[1], color='black')\r\n","\r\n","    ## -- en Azul: Máximos Locales (positivos)\r\n","    ## -- en Cyan: Mínimos Locales (negativos)\r\n","    ## -- en Verde: Máximo Óptimo\r\n","    for posi, vali in zip(mapa.posMaximos, mapa.valMaximos):    \r\n","      valRel = abs(vali / mapa.valMaximoGlobal) \r\n","      if vali < 0:        \r\n","          c = colorFader('cyan', 'green', valRel) \r\n","          ax.scatter(posi[0], posi[1], color=c)\r\n","      else:\r\n","          c = colorFader('blue', 'green', valRel) \r\n","          ax.scatter(posi[0], posi[1], color=c)     \r\n","    ax.scatter(mapa.posMaximoGlobal[0], mapa.posMaximoGlobal[1], color='green', s=150)    \r\n","\r\n","    ## --en Violeta: Posición del Homiguero\r\n","    ax.scatter(mapa.posHormiguero[0], mapa.posHormiguero[1], color='violet', s=100)\r\n","\r\n","    # Define el tamaño de la figura\r\n","    ax.axis([MIN_ESPACIO_BUSQ, MAX_ESPACIO_BUSQ, MIN_ESPACIO_BUSQ, MAX_ESPACIO_BUSQ])\r\n","\r\n","    # Separa las coordenadas x, y de las posiciones en el ciclo inicial\r\n","    X, Y = zip(*ciclosPos[0])\r\n","    ## --en Rojo: posiciones de las posiciones de cada ciclo\r\n","    scat = ax.scatter(X, Y, color='red', s=50)\r\n","\r\n","    # Muestra título y texto debajo\r\n","    ax.set_title(textoTitulo)\r\n","    ax.set_ylabel(textoDesc, fontsize=11)\r\n","\r\n","    # Luego setea la animación usando los dos métodos anteriores \r\n","    cant = len(ciclosPos)\r\n","    ani = animation.FuncAnimation(fig, updatePlot, \r\n","                                  frames=cant, interval=cant,\r\n","                                  fargs=(ciclosPos, scat, ax), \r\n","                                  blit=True, repeat=False)\r\n","    \r\n","    return ani\r\n","\r\n","\r\n","# Define la configuración para el gráfico\r\n","MOSTRAR_HISTORICO_MOVIMIENTO = False\r\n","\r\n","### Nota: esto se agega para que funcione en Google Colab\r\n","##rc('animation', html='jshtml')\r\n","##ani\r\n","\r\n","print(\"Funciones para generar gráfico definidas.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_R9SyNuiUjyT","cellView":"form"},"source":["#@title Definir clase del Entorno del Problema \n","\n","# parámetros generales para el entorno\n","# cantidad de pixeles en cada dirección que ve o recuerda \n","OBS_MAX_PIXELES_VE_o_RECUERDA = 10 #@param {type:\"integer\"} \n","if OBS_MAX_PIXELES_VE_o_RECUERDA <= 1:\n","  OBS_MAX_PIXELES_VE_o_RECUERDA = 3\n","OBS_MAX_DIRECCIONES_VE_o_RECUERDA = 8 # corresponde a las 8 direcciones que ve / recuerda\n","POSIBLES_ACCIONES_DESC = [ \"mover\" ]\n","CANT_MAX_PASOS_REALIZAR = OBS_MAX_PIXELES_VE_o_RECUERDA\n","MAXIMO_VALOR_ACTION = CANT_MAX_PASOS_REALIZAR*10\n","\n","def parsearAccion(action):\n","  # id de tipo de acción siempre el mismo\n","  idAccion = 0\n","  # determina parámetros \n","  # cantidad de pasos y grados de dirección\n","  ppasos = action//10\n","  aux  = action - ppasos*10\n","  pgrados = (aux * 45) % 360\n","  return idAccion, pgrados, ppasos\n","\n","# Un entorno que represente el juego podría verse así:\n","class AntTurtleBuscarMaxMapaEntorno(py_environment.PyEnvironment):\n","\n","  def __init__(self, cantMaxIteraciones=50, resetCambiaTodo=True, Hash_configuracion_Mapa_Busqueda=None):\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=MAXIMO_VALOR_ACTION, name='action')\n","    self._observation_spec = array_spec.BoundedArraySpec(\n","        shape=(2, OBS_MAX_DIRECCIONES_VE_o_RECUERDA, OBS_MAX_PIXELES_VE_o_RECUERDA, ), dtype=np.float32, name='observation')      \n","    # inicializa parámetros generales\n","    self._cantMaxIteraciones = cantMaxIteraciones\n","    # nota: tener en cuenta que si la cantidad de iteraciones es muy grande, \n","    # se vuelve búsqueda exhaustiva y siempre queda muy cerca\n","    self._hashMapaBusq = Hash_configuracion_Mapa_Busqueda\n","    self._resetCambiaTodo = resetCambiaTodo  \n","    # inicializa variables \n","    self._episode_ended = False\n","    self._state = 0\n","    # define configuración del entorno\n","    if self._resetCambiaTodo:\n","      # cada vez que se resetea se define\n","      self._mapaBusq = None      \n","    else:\n","      # sólo se generan al iniciar y se mantiene la misma\n","      self._mapaBusq = self.crearEspacioBusqueda(self._hashMapaBusq)    \n","    self._antTurtle = None\n","    self._cantIteraciones = 0\n","\n","  def action_spec(self):\n","    # devuelve la forma de las acciones\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    # devuelve la forma de las observaciones   \n","    return self._observation_spec\n","\n","  def _reset(self):\n","    # resetea el entorno\n","    # crea partículas\n","    if self._resetCambiaTodo:\n","      # cada vez que se reseta, se define la lista de partículas\n","      self._mapaBusq = self.crearEspacioBusqueda(self._hashMapaBusq)    \n","    # resetea la hormiga-tortuga\n","    self._antTurtle = HormigaTortuga(self._mapaBusq.posHormiguero)\n","    # actualiza el estado considerando cantidad de ordenados\n","    self._state = 0\n","    self._cantIteraciones = 0\n","    self._episode_ended = False\n","    return ts.restart(self.devolverObsActual())\n","\n","  def crearEspacioBusqueda(self, Hash_configuracion_Mapa_Busqueda=None):\n","    if Hash_configuracion_Mapa_Busqueda is None or Hash_configuracion_Mapa_Busqueda==\"\":\n","      # crea el espacio de búsqueda nuevo\n","      mapa = MapaBusqueda(limMin=MIN_ESPACIO_BUSQ, limMax=MAX_ESPACIO_BUSQ, \\\n","                          cantMaximosLocales=CANT_MAXIMOS_LOCALES, cantObstaculos=CANT_OBSTACULOS)\n","    else:\n","      # usa configuración de mapa de búsqueda definida en hash\n","      mapa = pickle.loads(codecs.decode(Hash_configuracion_Mapa_Busqueda.encode(), \"base64\"))\n","    # devuelve el mapa y el valor mínimo para finalizar la búsqueda\n","    return mapa\n","\n","  def devolverObsActual(self):\n","    # devuelve valores para la observación actual\n","    obs = []\n","    # lo que VE actualmente\n","    # devuelve 8 listas:\n","    #       5 listas (adelante y costados) con valores:\n","    #         -1 para obstáculos o limites\n","    #          0 para posiciones normales\n","    #         >0 para máximos\n","    #       3 listas (atrás) con valores en 0\n","    obs.append(  self._antTurtle.ve(self._mapaBusq, OBS_MAX_PIXELES_VE_o_RECUERDA)  )\n","    # lo que RECUERDA de ciclos anteriores\n","    # devuelve 8 listas con valores:\n","    #                        1 para posiciones que no paso    \n","    #                        0 para posiciones por las que paso\n","    obs.append(  self._antTurtle.recuerda(self._mapaBusq, OBS_MAX_PIXELES_VE_o_RECUERDA)  )\n","    ##print(np.array(obs).shape)\n","    # formatea la salida como array float\n","    # nota: para DQN parece ser que conviene \n","    # normalizar los valores para que sean más homogeneos \n","    # y no demasiado dispares entre sí \n","    # (sino genera un 'loss' demasiado grande)        \n","    res = []\n","    for listaTipo in obs:\n","      auxT = []\n","      for listaVal in listaTipo:\n","        auxV = []\n","        for val in listaVal:\n","          auxV.append( round(val,4) )\n","        auxT.append( np.array(auxV, dtype=np.float32) )\n","      res.append( np.array(auxT, dtype=np.float32) )\n","    ##print(len(res), \"->\", res)\n","    res = np.array(res, dtype=np.float32) \n","    ##print(res.shape)    \n","    return res\n","\n","  def _step(self, action):\n","    # aplica una acción sobre el entorno\n","    \n","    if self._episode_ended:\n","      # si el entorno está finalizado, lo resetea\n","      return self.reset()\n","\n","    # actualiza cantidad de interacciones \n","    self._cantIteraciones = self._cantIteraciones - 1\n","\n","    # parsea la accion para determinar acción\n","    idAccion, param_grados, param_pasos = parsearAccion(action)\n","\n","    # aplica la acción correspondiente \n","    self._state = self._antTurtle.desplazarse(self._mapaBusq, param_grados, param_pasos)\n","    self._state = self._state / 10\n","\n","    # determina si debe finalizar o no\n","    terAlcanzaMaximo = self._antTurtle.recorridoEncuentraSolucion\n","    terLlegoMaxIteraciones = (abs(self._cantIteraciones) >= abs(self._cantMaxIteraciones))\n","    if terAlcanzaMaximo or terLlegoMaxIteraciones:\n","      # si lllegó a algún máximp\n","      # o si la cantidad de iteraciones llega al límite\n","      # fuerza que finaliza\n","      self._episode_ended = True\n","\n","    if self._episode_ended:\n","      # si finaliza\n","      # devuelve el reward final (siempre se maximiza)\n","      # usando la heuristica de la solución hallada\n","      if terAlcanzaMaximo:\n","        r = self._antTurtle.valRecorridoUltimo\n","      else:\n","        r = 0\n","      return ts.termination(self.devolverObsActual(), reward=round(r, 4))\n","    else:\n","      # si no finaliza\n","      return ts.transition(\n","         self.devolverObsActual(), reward=round(self._state, 4), discount=0.9)\n","         # notar que no se usa discount=1.0 porque sino genera problema de 'loss' muy grande\n","\n","  def render(self, mode = 'human'):\n","    # muestra información sobre el entorno\n","    if  self._cantIteraciones==0:\n","      # Muestra la información sobre el Mapa de Búsqueda\n","      print(\"\\n> Mapa de Búsqueda:\")\n","      # devuele el hash del mapa de búsqueda\n","      print(\"    * Hash del mapa de búsqueda definido: \")\n","      print(codecs.encode(pickle.dumps(self._mapaBusq), \"base64\").decode() )\n","      print(\"    ++ Ubicación del Hormiguero: \", self._mapaBusq.posHormiguero)\n","      strMax = \"\"\n","      for auxPos, auxVal in zip(self._mapaBusq.posMaximos, self._mapaBusq.valMaximos):\n","          if strMax != \"\":\n","            strMax = strMax  + \", \"\n","          strMax = strMax + str(auxPos) + \" { \" + str(auxVal) + \" } \"\n","      print(\"    ** Máximos Generados: \", len(self._mapaBusq.posMaximos), \" --> \", strMax)\n","      print(\"    ** Posición Máximo Óptimo: \", self._mapaBusq.posMaximoGlobal, \" { \",  self._mapaBusq.valMaximoGlobal,\" } **\")\n","      print(\"\\n\")  \n","    # muestra el valor del recorrido\n","    print(\"> Valor del Recorrido : \", self._antTurtle.valRecorridoUltimo)\n","    print(\"    \", self._antTurtle.recorridoUltimo)\n","    print(\"\\n\")    \n","    res = self._antTurtle.valRecorridoUltimo\n","    if self._episode_ended:\n","      # si termino genera el gráfico con el video de la animación\n","      ani = PrepararGrafico(self._mapaBusq, MIN_ESPACIO_BUSQ, MAX_ESPACIO_BUSQ, self._antTurtle.recorridoUltimo)    \n","      # Nota: esto se agega para que funcione en Google Colab\n","      rc('animation', html='jshtml')\n","      display(ani)\n","    return np.array(res, dtype=np.float32)\n","\n","print(\"\\nEntorno del Problema definido.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcg61YUM93yb","cellView":"form"},"source":["#@title Definir Simulador del Entorno y Entornos para Entrenamiento \n","\n","# si se indica este  hash del mapa de búsqueda \n","# siempre usa esta configuración para entrenamiento del agente\n","Max_Iteraciones_Entorno_para_Entrenamiento =  250#@param {type:\"integer\"}\n","if Max_Iteraciones_Entorno_para_Entrenamiento < 1:\n","  Max_Iteraciones_Entorno_para_Entrenamiento = 10\n","Usar_hash_configuracion_fija_para_Entrenamiento = False #@param {type:\"boolean\"}\n","Hash_configuracion_Mapa_Busqueda_Entrenamiento = \"gANjX19tYWluX18KTWFwYUJ1c3F1ZWRhCnEAKYFxAX1xAihYBgAAAGxpbU1pbnEDSwBYBgAAAGxp bU1heHEESx5YBwAAAGNhbnRNYXhxBUsBWA4AAABjYW50T2JzdGFjdWxvc3EGS2RYCgAAAEhvcm1p Z3Vlcm9xB11xCChLE0sRZVgKAAAATWF4aW1vc1Bvc3EJXXEKXXELKEsSSxZlYVgKAAAATWF4aW1v c1ZhbHEMXXENS2RhWAsAAABNYXhPcHRpbW9JRHEOSwBYDQAAAE9ic3RhY3Vsb3NQb3NxD11xEChd cREoSxVLEmVdcRIoSw1LFmVdcRMoSxpLDmVdcRQoSwpLFmVdcRUoSwlLHmVdcRYoSwdLA2VdcRco SwZLB2VdcRgoSwRLFWVdcRkoSxRLAmVdcRooSxBLC2VdcRsoSwRLB2VdcRwoSxNLGWVdcR0oSxVL EWVdcR4oSwFLEmVdcR8oSw1LFGVdcSAoSwpLGGVdcSEoSxdLFmVdcSIoSw1LG2VdcSMoSw9LB2Vd cSQoSxBLBWVdcSUoSw5LFWVdcSYoSwFLDGVdcScoSw9LD2VdcSgoSxFLG2VdcSkoSxlLE2VdcSoo Sx5LF2VdcSsoSwVLCGVdcSwoSxJLD2VdcS0oSwdLBGVdcS4oSwRLHGVdcS8oSwRLDWVdcTAoSwxL F2VdcTEoSx1LHmVdcTIoSxhLF2VdcTMoSwNLE2VdcTQoSwdLGmVdcTUoSxhLCmVdcTYoSwVLAWVd cTcoSxJLAmVdcTgoSwlLAmVdcTkoSwhLEWVdcTooSx5LAGVdcTsoSwdLAWVdcTwoSwlLBmVdcT0o SwdLEWVdcT4oSwdLCWVdcT8oSw5LGmVdcUAoSxZLEmVdcUEoSxFLBWVdcUIoSxxLEmVdcUMoSwBL EGVdcUQoSx1LC2VdcUUoSxFLDGVdcUYoSwJLEmVdcUcoSxlLF2VdcUgoSwFLDGVdcUkoSx1LD2Vd cUooSxlLD2VdcUsoSw1LHGVdcUwoSwNLAWVdcU0oSxxLDGVdcU4oSw1LB2VdcU8oSxZLDmVdcVAo SwxLFGVdcVEoSwlLFmVdcVIoSwFLEGVdcVMoSx5LAGVdcVQoSwZLEmVdcVUoSwRLGWVdcVYoSx5L HmVdcVcoSxxLCWVdcVgoSxFLCGVdcVkoSxZLAmVdcVooSxJLFGVdcVsoSxBLFmVdcVwoSwtLEGVd cV0oSwJLHmVdcV4oSxxLHGVdcV8oSwVLEmVdcWAoSwNLBmVdcWEoSxVLGWVdcWIoSwZLEGVdcWMo SxdLDWVdcWQoSxNLC2VdcWUoSwNLGWVdcWYoSw9LHGVdcWcoSwxLAWVdcWgoSxRLCWVdcWkoSw9L G2VdcWooSxhLF2VdcWsoSw5LD2VdcWwoSwNLD2VdcW0oSwdLD2VdcW4oSxpLHmVdcW8oSwxLF2Vd cXAoSxpLCmVdcXEoSwZLDmVdcXIoSwtLAGVdcXMoSxlLDWVdcXQoSwpLC2VlWA0AAABtYXBhRmVy b21vbmFzcXV9cXZYBwAAAGZlcm9EZnRxd0sBdWIu\" #@param {type:\"string\"}\n","if Usar_hash_configuracion_fija_para_Entrenamiento:\n","  hash = Hash_configuracion_Mapa_Busqueda_Entrenamiento\n","else:\n","  hash = None\n","\n","# Definir entornos de entrenamiento y de evaluación\n","# (ambos con lista que se cambia cada vez que se resetea)\n","# ya definidos dentro del wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment( AntTurtleBuscarMaxMapaEntorno(Max_Iteraciones_Entorno_para_Entrenamiento, True, hash) )\n","eval_env = tf_py_environment.TFPyEnvironment( AntTurtleBuscarMaxMapaEntorno(Max_Iteraciones_Entorno_para_Entrenamiento, True, hash) )\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","print(\"\\nEntornos de entrenamiento y prueba definidos. \")\n","\n","# definir simulador para probar el entorno\n","def SimularEntorno(env, policy, titulo, mostrarDetalle=True):\n","    print(\"\\n** \", titulo, \"**\")                   \n","    # muesta estado inicial\n","    time_step = env.reset()      \n","    # muestra la información del entorno\n","    env.pyenv.render()\n","    if mostrarDetalle:\n","      print(\" Ini: [\", time_step, \"]\")    \n","    j = 1\n","    strAccRew = \" \"\n","    while not time_step.is_last():\n","      # la política determina la acción a realizar\n","      action_step = policy.action(time_step)\n","      time_step = env.step(action_step.action)\n","      # recupera la observación y muestra el nuevo estado \n","      ac = action_step.action.numpy()[0]\n","      idAccion, grados, pasos = parsearAccion(ac)\n","      r = time_step.reward.numpy()[0]\n","      #ob = time_step.observation.numpy()[0]\n","      descAccion = POSIBLES_ACCIONES_DESC[ idAccion ] + \"(\" + str(grados) + \"°, \" + str(pasos) + \"p)\" \n","      if mostrarDetalle:\n","        print(\"  #\", j, \": acción \", descAccion, \"-> Estado/Reward \", r, \"[\", time_step, \",\", action_step, \"]\")\n","      else:\n","        if j > 1:\n","          strAccRew = strAccRew  + \" + \"\n","        strAccRew = strAccRew + descAccion + \" {\" + \"{:.4f}\".format(r) + \"}\" \n","        if (j % 3) == 0:\n","          strAccRew = strAccRew  + \"\\n \"\n","      j = j + 1\n","    # muestra estado final\n","    print( \"> \" + str(j-1)  + \" Acciones: \\n\", strAccRew )\n","    # devuelve el video que genera\n","    resRender = env.pyenv.render()\n","    return resRender[0]\n","\n","print(\"\\nSimulador del entorno definido.\")\n","\n","# Probar el entorno definido con Política Aleatoria (opcional)\n","Probar_Entorno = \"SI sin Detalle\" #@param [\"SI con Detalle\", \"SI sin Detalle\", \"NO\"]\n","Probar_Entorno_Bool = (Probar_Entorno != \"NO\")\n","Mostrar_Detalle_Probar_Entorno_Bool = (Probar_Entorno == \"SI con Detalle\")\n","if Probar_Entorno_Bool:\n","   SimularEntorno(eval_env, random_policy, \"Probando el entorno del problema con política al azar\", Mostrar_Detalle_Probar_Entorno_Bool)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZXN3XTRWQV-"},"source":["2) Establecer clase para el Agente:"]},{"cell_type":"code","metadata":{"id":"diEOEg3JaMHa","cellView":"form"},"source":["#@title Definir el Agente (tipo DQN o DQN Categórico)\n","\n","tipo_agente = \"DQN\" #@param [\"DQN\", \"DQN Categorico (C51)\"]\n","learning_rate = 1e-3  # @param {type:\"number\"}\n","cant_neuronas_ocultas = \"64, 16, 8\" # @param {type:\"string\"}\n","DQN_usa_capas_convNet = True # @param {type:\"boolean\"}\n","DQNCat_num_atoms = 51  # @param {type:\"integer\"}\n","\n","# controla cantidad de atoms para DQN Cat\n","if DQNCat_num_atoms <= 1:\n","  DQNCat_num_atoms = 51\n","\n","# Define cantidad de neuronas ocultas para RNA-Q\n","hidden_layers = []\n","for val in cant_neuronas_ocultas.split(','):\n","  if  int(val) < 1:\n","    hidden_layers.append( 10 )\n","  else:\n","    hidden_layers.append( int(val) )\n","fc_layer_params = tuple(hidden_layers, )\n","\n","if tipo_agente==\"DQN\":\n","\n","  #define las capas convolutional\n","  if DQN_usa_capas_convNet:\n","    # nota: ojo que si se pone más de una capa convolutional, tira errore el entorno\n","    CNN_preprocessing_layers = tf.keras.models.Sequential(\n","                                        [tf.keras.layers.Conv2D(2, 2, activation='relu', padding=\"same\"),\n","                                         tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","                                         tf.keras.layers.Flatten()])\n","  else:\n","    CNN_preprocessing_layers = None\n","\n","  # Define RNA-Q\n","  q_net = q_network.QNetwork(\n","      train_env.observation_spec(),\n","      train_env.action_spec(),\n","      preprocessing_layers=CNN_preprocessing_layers,\n","      fc_layer_params=fc_layer_params)\n","\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","  train_step_counter = tf.Variable(0)\n","\n","  # Define el agente de tipo Q\n","  ag = dqn_agent.DqnAgent(\n","      train_env.time_step_spec(),\n","      train_env.action_spec(),\n","      q_network=q_net,\n","      optimizer=optimizer,\n","      td_errors_loss_fn=common.element_wise_squared_loss,\n","      train_step_counter=train_step_counter)\n","\n","  ag.initialize()\n","\n","  print(\"Agente DQN inicializado. \")\n","\n","elif tipo_agente == \"DQN Categorico (C51)\":\n","  \n","  # Define RNA-Q Categórico\n","  categorical_q_net = categorical_q_network.CategoricalQNetwork(\n","      train_env.observation_spec(),\n","      train_env.action_spec(),\n","      num_atoms=DQNCat_num_atoms,\n","      fc_layer_params=fc_layer_params)\n","\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","  train_step_counter = tf.compat.v2.Variable(0)\n","  \n","  # parámetros especificos (por defecto)\n","  n_step_update = 2\n","  gamma = 0.99\n","\n","  # Define el agente de tipo Q Categórico\n","  ag = categorical_dqn_agent.CategoricalDqnAgent(\n","      train_env.time_step_spec(),\n","      train_env.action_spec(),\n","      categorical_q_network=categorical_q_net,\n","      optimizer=optimizer,\n","      min_q_value=0,\n","      max_q_value=MAXIMO_VALOR_ACTION,\n","      n_step_update=n_step_update,\n","      td_errors_loss_fn=common.element_wise_squared_loss,\n","      gamma=gamma,\n","      train_step_counter=train_step_counter)\n","  \n","  ag.initialize()\n","  \n","  print(\"Agente DQN Categorico (C51) inicializado. \")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLCBLMD4Zsia"},"source":["3) Llevar a cabo el Entrenamiento:"]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","cellView":"form"},"source":["#@title Definir Métricas para evaluación\r\n","\r\n","# Se usa el promedio de la recompensa (la más común)\r\n","# See also the metrics module for standard implementations of different metrics.\r\n","# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\r\n","\r\n","def compute_avg_return(environment, policy, num_episodes=10):\r\n","\r\n","  if num_episodes <= 0:\r\n","    return 0.0\r\n","    \r\n","  total_return = 0.0\r\n","  for _ in range(num_episodes):\r\n","\r\n","    time_step = environment.reset()\r\n","    episode_return = 0.0\r\n","    while not time_step.is_last():\r\n","      action_step = policy.action(time_step)\r\n","      time_step = environment.step(action_step.action)\r\n","      episode_return += time_step.reward\r\n","    total_return += episode_return\r\n","\r\n","  avg_return = total_return / num_episodes\r\n","  return avg_return.numpy()[0]\r\n","\r\n","print(\"Métricas definidas.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EBRZGSkZ5N6","cellView":"form"},"source":["#@title Preparar datos para Entrenamiento\r\n","\r\n","initial_collect_steps =   10000# @param {type:\"integer\"} \r\n","collect_steps_per_iteration = 100  # @param {type:\"integer\"}\r\n","replay_buffer_max_length = 100000  # @param {type:\"integer\"}\r\n","batch_size = 64  # @param {type:\"integer\"}\r\n","\r\n","# Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\r\n","replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n","    data_spec = ag.collect_data_spec,\r\n","    batch_size = train_env.batch_size,\r\n","    max_length = replay_buffer_max_length)\r\n","\r\n","# Recolecta datos generados al azar\r\n","# This loop is so common in RL, that we provide standard implementations. \r\n","# For more details see the drivers module.\r\n","# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\r\n","\r\n","def collect_step(environment, policy, buffer):\r\n","  time_step = environment.current_time_step()\r\n","  action_step = policy.action(time_step)\r\n","  next_time_step = environment.step(action_step.action)\r\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n","\r\n","  # Add trajectory to the replay buffer\r\n","  buffer.add_batch(traj)\r\n","\r\n","def collect_data(env, policy, buffer, steps=1):\r\n","  for _ in range(steps):\r\n","    collect_step(env, policy, buffer)\r\n","\r\n","collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\r\n","\r\n","print(\"\\nDatos recolectados.\")\r\n","\r\n","# Muestra ejemplo de los datos recolectados\r\n","##iter(replay_buffer.as_dataset()).next()\r\n","\r\n","if tipo_agente==\"DQN\":\r\n","  # Preparar los datos recolectados con trajectories de shape [Bx2x...]\r\n","  dataset = replay_buffer.as_dataset(\r\n","      num_parallel_calls=3, \r\n","      sample_batch_size=batch_size, \r\n","      num_steps=2).prefetch(3)\r\n","elif tipo_agente == \"DQN Categorico (C51)\":\r\n","  # Dataset generates trajectories with shape [BxTx...] where\r\n","  # T = n_step_update + 1.\r\n","  dataset = replay_buffer.as_dataset(\r\n","      num_parallel_calls=3, sample_batch_size=batch_size,\r\n","      num_steps=n_step_update + 1).prefetch(3)\r\n","\r\n","iterator = iter(dataset)\r\n","\r\n","# Muestra ejemplo \r\n","##iterator.next()\r\n","print(\"\\nDataset creado.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2k3S5IqGhK-a","cellView":"form"},"source":["#@title Entrenar al Agente\r\n","\r\n","cant_ciclos_entrenamiento_finalizar =  10000# @param {type:\"integer\"}\r\n","minima_recompensa_promedio_finalizar = 100 # @param {type:\"integer\"}\r\n","log_cada_ciclos = 200  # @param {type:\"integer\"}\r\n","mostar_recompensa_cada = 1000  # @param {type:\"integer\"}\r\n","cant_episodios_evaluacion =  10# @param {type:\"integer\"}\r\n","\r\n","#  Optimize by wrapping some of the code in a graph using TF function (Optional)\r\n","ag.train = common.function(ag.train)\r\n","\r\n","# Reset the train step\r\n","ag.train_step_counter.assign(0)\r\n","\r\n","# Evaluate the agent's policy once before training.\r\n","avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","ar_ciclo = []\r\n","ar_returns = []\r\n","ar_loss = []\r\n","\r\n","print(\"\\n** Comienza el Entrenamiento **\\n\")\r\n","for _ in range(cant_ciclos_entrenamiento_finalizar):\r\n","\r\n","  # Collect a few steps using collect_policy and save to the replay buffer.\r\n","  collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n","\r\n","  # Sample a batch of data from the buffer and update the agent's network.\r\n","  experience, unused_info = next(iterator)\r\n","  train_loss = ag.train(experience).loss\r\n","\r\n","  step = ag.train_step_counter.numpy()\r\n","\r\n","  if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % log_cada_ciclos == 0):\r\n","    print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \r\n","    ar_ciclo.append( step )\r\n","    ar_loss.append( train_loss )\r\n","    avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","    ar_returns.append( avg_return )\r\n","\r\n","    if (step == 1) or (step == cant_ciclos_entrenamiento_finalizar) or (step % mostar_recompensa_cada == 0):\r\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\r\n","\r\n","    if (avg_return >= minima_recompensa_promedio_finalizar):\r\n","      print('** Finaliza en step {0} por buen valor de recompensa promedio: {1:.1f}'.format(step, avg_return)) \r\n","      break\r\n","\r\n","print(\"\\n** Entrenamiento Finalizado **\\n\")\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EBBl7mRkQYa","cellView":"form"},"source":["#@title Mostrar Gráficos del Entrenamiento\r\n","\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","plt.plot( ar_ciclo, ar_returns)\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","#plt.plot( ar_ciclo, ar_returns)\r\n","plt.plot( ar_ciclo, ar_loss, color=\"red\" )\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rjj7bRe7AB-l"},"source":["4) Cargar / Graba el modelo de las políticas entrenadas:"]},{"cell_type":"code","metadata":{"id":"6V2EiqdwAy_R","cellView":"form"},"source":["#@title Cargar o Guardar el Modelo\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoAgentes/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-Ant-Trutle-RandomPosProblem\" #@param {type:\"string\"}\n","accion_realizar = \"-\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","# determina lugar donde se guarda el modelo\n","policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","\n","if accion_realizar != \"-\":\n","  # Montar Drive\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  # guarda la politica del agente entrenado\n","  tf_policy_saver = policy_saver.PolicySaver(ag.policy)\n","  tf_policy_saver.save(policy_dir)\n","  print(\"\\nPolítica del modelo guardada en \", policy_dir)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  saved_policy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica del modelo recuperada de \", policy_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j79yPUetlUbs"},"source":["5) Probar entrenamiento comparando resultados:"]},{"cell_type":"code","metadata":{"id":"lLkdkcBjl3Xs","cellView":"form"},"source":["#@title Realizar una prueba del Agente Entrenado contra el Azar\r\n","Max_Iteraciones_Entorno_para_probar =  250#@param {type:\"integer\"}\r\n","if Max_Iteraciones_Entorno_para_probar < 1:\r\n","  Max_Iteraciones_Entorno_para_probar = 1\r\n","Usar_hash_para_probar = False #@param {type:\"boolean\"}\r\n","Hash_configuracion_Mapa_Busqueda_probar = \"gANjX19tYWluX18KTWFwYUJ1c3F1ZWRhCnEAKYFxAX1xAihYBgAAAGxpbU1pbnEDSwBYBgAAAGxp bU1heHEESx5YBwAAAGNhbnRNYXhxBUsBWA4AAABjYW50T2JzdGFjdWxvc3EGS2RYCgAAAEhvcm1p Z3Vlcm9xB11xCChLE0sRZVgKAAAATWF4aW1vc1Bvc3EJXXEKXXELKEsSSxZlYVgKAAAATWF4aW1v c1ZhbHEMXXENS2RhWAsAAABNYXhPcHRpbW9JRHEOSwBYDQAAAE9ic3RhY3Vsb3NQb3NxD11xEChd cREoSxVLEmVdcRIoSw1LFmVdcRMoSxpLDmVdcRQoSwpLFmVdcRUoSwlLHmVdcRYoSwdLA2VdcRco SwZLB2VdcRgoSwRLFWVdcRkoSxRLAmVdcRooSxBLC2VdcRsoSwRLB2VdcRwoSxNLGWVdcR0oSxVL EWVdcR4oSwFLEmVdcR8oSw1LFGVdcSAoSwpLGGVdcSEoSxdLFmVdcSIoSw1LG2VdcSMoSw9LB2Vd cSQoSxBLBWVdcSUoSw5LFWVdcSYoSwFLDGVdcScoSw9LD2VdcSgoSxFLG2VdcSkoSxlLE2VdcSoo Sx5LF2VdcSsoSwVLCGVdcSwoSxJLD2VdcS0oSwdLBGVdcS4oSwRLHGVdcS8oSwRLDWVdcTAoSwxL F2VdcTEoSx1LHmVdcTIoSxhLF2VdcTMoSwNLE2VdcTQoSwdLGmVdcTUoSxhLCmVdcTYoSwVLAWVd cTcoSxJLAmVdcTgoSwlLAmVdcTkoSwhLEWVdcTooSx5LAGVdcTsoSwdLAWVdcTwoSwlLBmVdcT0o SwdLEWVdcT4oSwdLCWVdcT8oSw5LGmVdcUAoSxZLEmVdcUEoSxFLBWVdcUIoSxxLEmVdcUMoSwBL EGVdcUQoSx1LC2VdcUUoSxFLDGVdcUYoSwJLEmVdcUcoSxlLF2VdcUgoSwFLDGVdcUkoSx1LD2Vd cUooSxlLD2VdcUsoSw1LHGVdcUwoSwNLAWVdcU0oSxxLDGVdcU4oSw1LB2VdcU8oSxZLDmVdcVAo SwxLFGVdcVEoSwlLFmVdcVIoSwFLEGVdcVMoSx5LAGVdcVQoSwZLEmVdcVUoSwRLGWVdcVYoSx5L HmVdcVcoSxxLCWVdcVgoSxFLCGVdcVkoSxZLAmVdcVooSxJLFGVdcVsoSxBLFmVdcVwoSwtLEGVd cV0oSwJLHmVdcV4oSxxLHGVdcV8oSwVLEmVdcWAoSwNLBmVdcWEoSxVLGWVdcWIoSwZLEGVdcWMo SxdLDWVdcWQoSxNLC2VdcWUoSwNLGWVdcWYoSw9LHGVdcWcoSwxLAWVdcWgoSxRLCWVdcWkoSw9L G2VdcWooSxhLF2VdcWsoSw5LD2VdcWwoSwNLD2VdcW0oSwdLD2VdcW4oSxpLHmVdcW8oSwxLF2Vd cXAoSxpLCmVdcXEoSwZLDmVdcXIoSwtLAGVdcXMoSxlLDWVdcXQoSwpLC2VlWA0AAABtYXBhRmVy b21vbmFzcXV9cXZYBwAAAGZlcm9EZnRxd0sBdWIu\" #@param {type:\"string\"}\r\n","if Usar_hash_para_probar:\r\n","  hash = Hash_configuracion_Mapa_Busqueda_probar\r\n","else:\r\n","  hash = None\r\n","\r\n","# variable auxiliares\r\n","cantidad_probar =  1\r\n","promAzar = 0\r\n","promAgente = 0\r\n","\r\n","# determina política a usar\r\n","policy_agente_entrenado = None\r\n","if not('ag' in vars() or 'ag' in globals()) or ag is None:\r\n","  if not('saved_policy' in vars() or 'saved_policy' in globals()) or saved_policy is None:\r\n","    ValueError(\"No hay política entrenada definida.\")\r\n","  else:\r\n","    policy_agente_entrenado = saved_policy\r\n","    print(\"- Se usa la política recuperada del drive.\")\r\n","else:\r\n","  policy_agente_entrenado = ag.policy\r\n","  print(\"- Se usa la política del modelo entrenado.\")\r\n","\r\n","for i in range(cantidad_probar):\r\n","\r\n","  if cantidad_probar > 1:\r\n","    print(\"\\n> Prueba \", i+1, \":\")\r\n","\r\n","  # crea nuevo entorno que mantiene el espacio de búsqueda\r\n","  prueba_env =  tf_py_environment.TFPyEnvironment( AntTurtleBuscarMaxMapaEntorno(Max_Iteraciones_Entorno_para_probar, False, hash) )\r\n","\r\n","  # Probar Aleatorio\r\n","  valorAzar = SimularEntorno(prueba_env, random_policy, \"Resultados Aleatorio\", False) \r\n","  promAzar = promAzar + valorAzar\r\n","  \r\n","  print(\"\\n ************************************************************************************************************\")\r\n","  \r\n","  # Probar Agente Entrenado\r\n","  valorAgente = SimularEntorno(prueba_env, policy_agente_entrenado, \"Resultados de Agente Entrenado\", False) \r\n","  promAgente = promAgente + valorAgente\r\n","\r\n","  # Decide Ganador\r\n","  if valorAzar < valorAgente:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera MEJOR resultado que el azar (\", valorAzar,\")\")\r\n","  else:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera PEOR resultado que el azar (\", valorAzar,\")\")\r\n","\r\n","# Decide Ganador General\r\n","if cantidad_probar > 1:\r\n","  promAgente = promAgente / cantidad_probar\r\n","  promAzar = promAzar / cantidad_probar\r\n","  print(\"\\n================================================================================================\\n\")\r\n","  if promAzar < promAgente:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene MEJORES resultado que  el azar (\", promAzar,\")\")\r\n","  else:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene PEORES resultados que el azar (\", promAzar,\")\")\r\n","  print(\"\\n================================================================================================\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDoTrKC8OU29","cellView":"form"},"source":["#@title Realizar varias pruebas del Agente Entrenado contra el Azar \r\n","Max_Iteraciones_Entorno_para_probar = 250 #@param {type:\"integer\"}\r\n","if Max_Iteraciones_Entorno_para_probar < 1:\r\n","  Max_Iteraciones_Entorno_para_probar = 1\r\n","Usar_hash_para_probar = False #@param {type:\"boolean\"}\r\n","Hash_configuracion_Mapa_Busqueda_probar = \"gANjX19tYWluX18KTWFwYUJ1c3F1ZWRhCnEAKYFxAX1xAihYBgAAAGxpbU1pbnEDSwBYBgAAAGxp bU1heHEESx5YBwAAAGNhbnRNYXhxBUsBWA4AAABjYW50T2JzdGFjdWxvc3EGS2RYCgAAAEhvcm1p Z3Vlcm9xB11xCChLE0sRZVgKAAAATWF4aW1vc1Bvc3EJXXEKXXELKEsSSxZlYVgKAAAATWF4aW1v c1ZhbHEMXXENS2RhWAsAAABNYXhPcHRpbW9JRHEOSwBYDQAAAE9ic3RhY3Vsb3NQb3NxD11xEChd cREoSxVLEmVdcRIoSw1LFmVdcRMoSxpLDmVdcRQoSwpLFmVdcRUoSwlLHmVdcRYoSwdLA2VdcRco SwZLB2VdcRgoSwRLFWVdcRkoSxRLAmVdcRooSxBLC2VdcRsoSwRLB2VdcRwoSxNLGWVdcR0oSxVL EWVdcR4oSwFLEmVdcR8oSw1LFGVdcSAoSwpLGGVdcSEoSxdLFmVdcSIoSw1LG2VdcSMoSw9LB2Vd cSQoSxBLBWVdcSUoSw5LFWVdcSYoSwFLDGVdcScoSw9LD2VdcSgoSxFLG2VdcSkoSxlLE2VdcSoo Sx5LF2VdcSsoSwVLCGVdcSwoSxJLD2VdcS0oSwdLBGVdcS4oSwRLHGVdcS8oSwRLDWVdcTAoSwxL F2VdcTEoSx1LHmVdcTIoSxhLF2VdcTMoSwNLE2VdcTQoSwdLGmVdcTUoSxhLCmVdcTYoSwVLAWVd cTcoSxJLAmVdcTgoSwlLAmVdcTkoSwhLEWVdcTooSx5LAGVdcTsoSwdLAWVdcTwoSwlLBmVdcT0o SwdLEWVdcT4oSwdLCWVdcT8oSw5LGmVdcUAoSxZLEmVdcUEoSxFLBWVdcUIoSxxLEmVdcUMoSwBL EGVdcUQoSx1LC2VdcUUoSxFLDGVdcUYoSwJLEmVdcUcoSxlLF2VdcUgoSwFLDGVdcUkoSx1LD2Vd cUooSxlLD2VdcUsoSw1LHGVdcUwoSwNLAWVdcU0oSxxLDGVdcU4oSw1LB2VdcU8oSxZLDmVdcVAo SwxLFGVdcVEoSwlLFmVdcVIoSwFLEGVdcVMoSx5LAGVdcVQoSwZLEmVdcVUoSwRLGWVdcVYoSx5L HmVdcVcoSxxLCWVdcVgoSxFLCGVdcVkoSxZLAmVdcVooSxJLFGVdcVsoSxBLFmVdcVwoSwtLEGVd cV0oSwJLHmVdcV4oSxxLHGVdcV8oSwVLEmVdcWAoSwNLBmVdcWEoSxVLGWVdcWIoSwZLEGVdcWMo SxdLDWVdcWQoSxNLC2VdcWUoSwNLGWVdcWYoSw9LHGVdcWcoSwxLAWVdcWgoSxRLCWVdcWkoSw9L G2VdcWooSxhLF2VdcWsoSw5LD2VdcWwoSwNLD2VdcW0oSwdLD2VdcW4oSxpLHmVdcW8oSwxLF2Vd cXAoSxpLCmVdcXEoSwZLDmVdcXIoSwtLAGVdcXMoSxlLDWVdcXQoSwpLC2VlWA0AAABtYXBhRmVy b21vbmFzcXV9cXZYBwAAAGZlcm9EZnRxd0sBdWIu\" #@param {type:\"string\"}\r\n","if Usar_hash_para_probar:\r\n","  hash = Hash_configuracion_Mapa_Busqueda_probar\r\n","else:\r\n","  hash = None\r\n","\r\n","# variable auxiliares\r\n","cantidad_probar =  100 #@param {type:\"integer\"}\r\n","\r\n","# determina política a usar\r\n","policy_agente_entrenado = None\r\n","if not('ag' in vars() or 'ag' in globals()) or ag is None:\r\n","  if not('saved_policy' in vars() or 'saved_policy' in globals()) or saved_policy is None:\r\n","    ValueError(\"No hay política entrenada definida.\")\r\n","  else:\r\n","    policy_agente_entrenado = saved_policy\r\n","    print(\"- Se usa la política recuperada del drive.\")\r\n","else:\r\n","  policy_agente_entrenado = ag.policy\r\n","  print(\"- Se usa la política del modelo entrenado.\")\r\n","\r\n","\r\n","# crea nuevo entorno que mantiene el espacio de búsqueda\r\n","prueba_env =  tf_py_environment.TFPyEnvironment( AntTurtleBuscarMaxMapaEntorno(Max_Iteraciones_Entorno_para_probar, False, hash) )\r\n","\r\n","# calcula promedio de política al azar\r\n","promAzar = compute_avg_return(prueba_env, random_policy, num_episodes=cantidad_probar)\r\n","\r\n","# calcula promedio de política del agente entrenado\r\n","promAgente = compute_avg_return(prueba_env, policy_agente_entrenado, num_episodes=cantidad_probar)\r\n","\r\n","# Decide Ganador General\r\n","#promAgente = round(promAgente / cantidad_probar, 3)\r\n","#promAzar = round(promAzar / cantidad_probar, 3)\r\n","print(\"\\n================================================================================================\\n\")\r\n","if promAzar < promAgente:\r\n","  print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene MEJORES resultado que  el azar (\", promAzar,\")\")\r\n","else:\r\n","  print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene PEORES resultados que el azar (\", promAzar,\")\")\r\n","print(\"\\n================================================================================================\\n\")"],"execution_count":null,"outputs":[]}]}