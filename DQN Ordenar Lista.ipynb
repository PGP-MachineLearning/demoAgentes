{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN Ordenar Lista.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNVQ+cVCuQQ7Wgs4r4yYixI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para ordenar elmentos de una lista usando DQN:\r\n"]},{"cell_type":"markdown","metadata":{"id":"dliJD0WRUMWV"},"source":["0) Preparar el ambiente:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"Qxbe02w0T0ip","executionInfo":{"status":"ok","timestamp":1612649065851,"user_tz":180,"elapsed":2949,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"45e34b8a-37e1-4114-ee9d-c75c07b36a10"},"source":["#@title Instalar Paquete de TF-Agents\r\n","!pip install -q tf-agents\r\n","print(\"TF-Agentes instalado.\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TF-Agentes instalado.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJl4YsniURev","cellView":"form","executionInfo":{"status":"ok","timestamp":1612649068218,"user_tz":180,"elapsed":5300,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"31867e16-abcd-442b-d581-4448d83f182f"},"source":["#@title Cargar Librerías\r\n","from __future__ import absolute_import\r\n","from __future__ import division\r\n","from __future__ import print_function\r\n","\r\n","import abc\r\n","import tensorflow as tf\r\n","import numpy as np\r\n","import matplotlib\r\n","import matplotlib.pyplot as plt\r\n","from random import randint\r\n","from sklearn import preprocessing\r\n","import copy\r\n","\r\n","from tf_agents.environments import py_environment\r\n","from tf_agents.environments import tf_py_environment\r\n","\r\n","from tf_agents.environments import utils\r\n","from tf_agents.specs import array_spec\r\n","\r\n","from tf_agents.policies import random_tf_policy\r\n","\r\n","from tf_agents.trajectories import time_step as ts\r\n","\r\n","from tf_agents.agents.dqn import dqn_agent\r\n","from tf_agents.networks import q_network\r\n","from tf_agents.utils import common\r\n","\r\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n","from tf_agents.trajectories import trajectory\r\n","\r\n","import os\r\n","from tf_agents.policies import policy_saver\r\n","\r\n","tf.compat.v1.enable_v2_behavior()\r\n","\r\n","print(\"Librerías cargadas.\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Librerías cargadas.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3ONe5w_nUYME"},"source":["1) Establecer las clases del Problema y del Agente:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TQx1eodsvKj","cellView":"form","executionInfo":{"status":"ok","timestamp":1612649068221,"user_tz":180,"elapsed":5291,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"6237662f-12ee-48fd-911d-5d547b1e38aa"},"source":["#@title Definir las primitivas a usar para ordenar la lista\r\n","\r\n","# intercambia valores de pos1 con pos2 \r\n","def intercambiar(lista, pos1, pos2):\r\n","  # chequea que las posiciones no se encuentran fuera de la lista\r\n","  if (pos1 < 0) or (pos1 >= len(lista)):\r\n","    if (pos1 < 0):\r\n","      pos1 = 0\r\n","    else:\r\n","      pos1 =  len(lista) - 1\r\n","  if (pos2 < 0) or (pos2 >= len(lista)):\r\n","    if (pos2 < 0):\r\n","      pos2 = 0\r\n","    else:\r\n","      pos2 =  len(lista) - 1\r\n","  # chequea que no sean las mismas posiciones\r\n","  if pos1 == pos2:\r\n","    return lista\r\n","  else:    \r\n","    # realiza el intercambio\r\n","    lista[pos1], lista[pos2] = lista[pos2], lista[pos1]\r\n","    return lista\r\n","\r\n","# mueve el valor de posAnt a posNueva\r\n","def mover(lista, posAnt, posNueva):\r\n","  # chequea que las posiciones no se encuentran fuera de la lista\r\n","  if (posAnt < 0) or (posAnt >= len(lista)):\r\n","    if (posAnt < 0):\r\n","      posAnt = 0\r\n","    else:\r\n","      posAnt =  len(lista) - 1\r\n","  if (posNueva < 0) or (posNueva >= len(lista)):\r\n","    if (posNueva < 0):\r\n","      posNueva = 0\r\n","    else:\r\n","      posNueva =  len(lista) - 1\r\n","  # chequea que no sean las mismas posiciones\r\n","  if posAnt == posNueva:\r\n","    return lista\r\n","  else:    \r\n","    # realiza el intercambio\r\n","    lista.insert(posNueva, lista.pop(posAnt))\r\n","    return lista\r\n","\r\n","# función auxiliar para contar la cantidad de desordenados\r\n","# (debe ser de menor a mayor)\r\n","def contarDesordenados(lista):  \r\n","  cantError = 0\r\n","  if len(lista) > 0:\r\n","    i = 0 \r\n","    while i < len(lista):\r\n","      ant = lista[i]\r\n","      j = i + 1\r\n","      while j < len(lista):\r\n","        actual = lista[j]\r\n","        if actual < ant:\r\n","          cantError = cantError + 1\r\n","        j = j + 1      \r\n","      i = i + 1\r\n","  return cantError\r\n","\r\n","# variable auxiliar para determinar máximo de acciones a probar antes de abortar\r\n","POSIBLES_ACCIONES_DESC = [ \"mover\", \"intercambiar\" ]\r\n","POSIBLES_ACCIONES = [ mover,  intercambiar ]\r\n","\r\n","print(\"Primitivas de acciones definidas: \", POSIBLES_ACCIONES_DESC)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Primitivas de acciones definidas:  ['mover', 'intercambiar']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_R9SyNuiUjyT","cellView":"form","executionInfo":{"status":"ok","timestamp":1612649069672,"user_tz":180,"elapsed":6731,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"1934c507-bea6-49ee-9281-e2ff306e22b6"},"source":["#@title Definir Entorno del Problema \n","\n","# parámetros generales\n","MAX_ITERACIONES_REALIZAR = 100\n","TAMANIO_MINIMO_LISTA = 3\n","TAMANIO_MAXIMO_LISTA = 10\n","MAXIMO_VALOR_ACTION = (100 + (TAMANIO_MAXIMO_LISTA-1) * 10 + (TAMANIO_MAXIMO_LISTA-1))\n","\n","def parsearAccion(action):\n","  # como DQN sólo permite 1 action numérica\n","  # esta función se ocupa de parsearla para determinar:\n","  #    tipo de acción\n","  #    param1\n","  #    param2\n","  aux = action\n","  idAccion = aux // 100\n","  aux = aux - idAccion * 100\n","  param1 = aux // 10\n","  aux = aux - param1 * 10\n","  param2 = aux\n","  #print(action, idAccion, param1, param2)\n","  return idAccion, param1, param2\n","\n","# Un entorno que represente el juego podría verse así:\n","class OrdenarListasEnv(py_environment.PyEnvironment):\n","\n","  def __init__(self, reGenerarReset=True):\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=MAXIMO_VALOR_ACTION, name='action')\n","    self._observation_spec = array_spec.BoundedArraySpec(\n","        shape=(TAMANIO_MAXIMO_LISTA,), dtype=np.float32, name='observation')      \n","    self._state = 0\n","    self._episode_ended = False\n","    self._reGenerarListaReset = reGenerarReset\n","    if self._reGenerarListaReset:\n","      # inicializa vacía porque se define en el reset\n","      self._listaOriginal = []\n","    else:\n","      # la lista se define sólo al principio, luego se vuelve a desordenar\n","      self._listaOriginal = self.crearLista()\n","    self._lista = []\n","\n","  def action_spec(self):\n","    # devuelve la forma de las acciones\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    # devuelve la forma de las observaciones   \n","    return self._observation_spec\n","\n","  def render(self, mode = 'human'):\n","    # devuelve la lista para mostsrar\n","    return np.array(self._lista, dtype=np.int32)\n","\n","  def _reset(self):\n","    # resetea el entorno\n","    if self._reGenerarListaReset:\n","      # cada vez que se reseta, se define la lista\n","      self._listaOriginal = self.crearLista()\n","    # siempre la lista de trabajo se copia de la original\n","    self._lista = copy.deepcopy( self._listaOriginal ) \n","    # actualiza el estado considerando cantidad de ordenados\n","    self.actualizarEstado()\n","    self._cantIteraciones = 0\n","    self._episode_ended = False\n","    return ts.restart(self.devolverObsActual())\n","\n","  def crearLista(self):\n","    # genera los valores de las listas al azar\n","    cantElemRnd = randint(TAMANIO_MINIMO_LISTA, TAMANIO_MAXIMO_LISTA)\n","    #cantElemRnd = TAMANIO_MAXIMO_LISTA\n","    lista = []\n","    for j in range(cantElemRnd): \n","      lista.append( randint(-99, 99) )\n","    return lista\n","\n","  def actualizarEstado(self):\n","    # actualiza el valor del estado del entorno\n","    # teniendo en cuenta la cantidad de errores negativos\n","    self._state = - contarDesordenados(self._lista)\n","    return self._state\n","\n","  def devolverObsActual(self):\n","    # devuelve valores para la observación actual\n","    # los valores de la lista (rellenando con cero)\n","    # para que el agente sepa el estado real del entorno\n","    res = []\n","    res.extend( self._lista )\n","    val = 100\n","    while (len(res) < TAMANIO_MAXIMO_LISTA):\n","      res.append( val )\n","      val = val + 1\n","    # nota: para DQN parece ser que conviene \n","    # normalizar los valores para que sean más homogeneos \n","    # y no demasiado dispares entre sí \n","    # (sino genera un 'loss' demasiado grande)\n","    r = (res - np.min(res)) / (np.max(res) - np.min(res))\n","    return  np.array([round(v,3) for v in r], dtype=np.float32)\n","\n","  def _step(self, action):\n","    # aplica una acción sobre el entorno\n","    \n","    if self._episode_ended:\n","      # si el entorno está finalizado, lo resetea\n","      return self.reset()\n","\n","    # actualiza cantidad de interacciones \n","    self._cantIteraciones = self._cantIteraciones - 1\n","\n","    # parsea la accion para determinar acción con sus parámetros\n","    idAccion, param1, param2 = parsearAccion(action)\n","\n","    # si es un id de acción válida\n","    if idAccion >= 0 and idAccion < len(POSIBLES_ACCIONES):\n","      # aplica la acción correspondiente en cada lista\n","      # y calculando la cantidad de desordenados como error\n","      self._lista = POSIBLES_ACCIONES[idAccion](self._lista, param1, param2)\n","      \n","      # actualiza el estado con la cantidad de valores correctos\n","      self.actualizarEstado()\n","\n","    # determina si debe finalizar o no\n","    if (self._state == 0) or (abs(self._cantIteraciones) >= abs(MAX_ITERACIONES_REALIZAR)):\n","      # si está todo ordenado \n","      # o si la cantidad de iteraciones llega al límite\n","      # fuerza que finaliza\n","      self._episode_ended = True\n","\n","    if self._episode_ended:\n","      # si finaliza\n","      # devuelve el reward (siempre se maximiza):\n","      # si logra ordenaar\n","      # se calcula penalizando la cantidad de iteraciones \n","      if (self._state == 0):\n","        reward = MAX_ITERACIONES_REALIZAR + self._cantIteraciones\n","      else:\n","        reward = self._state\n","      return ts.termination(self.devolverObsActual(), reward)\n","    else:\n","      # si no finaliza      \n","      return ts.transition(\n","         self.devolverObsActual(), reward=self._state, discount=0.9)\n","         # notar que no se usa discount=1.0 porque sino genera problema de 'loss' muy grande\n","\n","print(\"\\nEntorno del Problema definido.\")\n","\n","# Definir entornos de entrenamiento y de evaluación\n","# (ambos con lista que se cambia cada vez que se resetea)\n","train_py_env = OrdenarListasEnv(True)\n","eval_py_env = OrdenarListasEnv(True)\n","\n","# Definir wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","print(\"\\nEntornos de entrenamiento y prueba definidos. \")\n","\n","# definir simulador para probar el entorno\n","def SimularEntorno(env, policy, titulo):\n","    print(\"\\n** \", titulo, \"**\")                   \n","    # muesta estado inicial\n","    time_step = env.reset()      \n","    #ob = time_step.observation.numpy()[0]\n","    print(\" Ini: [\", time_step, \"]\")    \n","    print(\" Lista Inicial = \", env.pyenv.render()[0] )\n","    j = 1\n","    while not time_step.is_last():\n","      # la política determina la acción a realizar\n","      action_step = policy.action(time_step)\n","      time_step = env.step(action_step.action)\n","      # recupera la observación y muestra el nuevo estado \n","      ac = action_step.action.numpy()[0]\n","      idAccion, param1, param2 = parsearAccion(ac)\n","      r = time_step.reward.numpy()[0]\n","      ##ob = time_step.observation.numpy()[0]\n","      descAccion = \"acción \" +  POSIBLES_ACCIONES_DESC[ idAccion ] + \"(\" + str(param1) + \",\" + str(param2) + \")\"\n","      print(\"  #\", j, \":\", descAccion, \"-> Estado/Reward \", r, \"[\", time_step, \",\", action_step, \"]\")\n","      ### print(\"    Lista = \", env.pyenv.render()[0] )\n","      j = j + 1\n","    # muestra estado final\n","    print(\" Recompensa Final = \", r )\n","    print(\" Lista Final = \", env.pyenv.render()[0] )\n","    return r\n","\n","print(\"Simulador del entorno definido.\")\n","\n","# Probar el entorno definido con Política Aleatoria (opcional)\n","Probar_Entorno = True #@param {type:\"boolean\"}\n","if Probar_Entorno:\n","  SimularEntorno(eval_env, random_policy, \"Probando el entorno del problema con política al azar\")\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","Entorno del Problema definido.\n","\n","Entornos de entrenamiento y prueba definidos. \n","Simulador del entorno definido.\n","\n","**  Probando el entorno del problema con política al azar **\n"," Ini: [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.023, 0.   , 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) ]\n"," Lista Inicial =  [-26 -66 -70 -49  55]\n"," # 1 : acción mover(8,6) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.023, 0.   , 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([86], dtype=int32)>, state=(), info=()) ]\n"," # 2 : acción mover(7,7) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.023, 0.   , 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([77], dtype=int32)>, state=(), info=()) ]\n"," # 3 : acción intercambiar(4,2) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.023, 0.718, 0.121, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([142], dtype=int32)>, state=(), info=()) ]\n"," # 4 : acción intercambiar(4,1) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.718, 0.121, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([141], dtype=int32)>, state=(), info=()) ]\n"," # 5 : acción mover(3,4) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.718, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([34], dtype=int32)>, state=(), info=()) ]\n"," # 6 : acción mover(7,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.718, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([79], dtype=int32)>, state=(), info=()) ]\n"," # 7 : acción intercambiar(6,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.718, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([169], dtype=int32)>, state=(), info=()) ]\n"," # 8 : acción mover(3,3) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.718, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([33], dtype=int32)>, state=(), info=()) ]\n"," # 9 : acción intercambiar(7,2) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.121, 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([172], dtype=int32)>, state=(), info=()) ]\n"," # 10 : acción mover(1,0) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.253, 0.121, 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=()) ]\n"," # 11 : acción mover(4,8) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.253, 0.121, 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([48], dtype=int32)>, state=(), info=()) ]\n"," # 12 : acción mover(1,3) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.121, 0.023, 0.253, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=()) ]\n"," # 13 : acción intercambiar(6,7) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.121, 0.023, 0.253, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([167], dtype=int32)>, state=(), info=()) ]\n"," # 14 : acción intercambiar(8,1) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.023, 0.253, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([181], dtype=int32)>, state=(), info=()) ]\n"," # 15 : acción mover(4,9) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.023, 0.253, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([49], dtype=int32)>, state=(), info=()) ]\n"," # 16 : acción intercambiar(2,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([129], dtype=int32)>, state=(), info=()) ]\n"," # 17 : acción intercambiar(2,2) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([122], dtype=int32)>, state=(), info=()) ]\n"," # 18 : acción mover(8,6) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([86], dtype=int32)>, state=(), info=()) ]\n"," # 19 : acción mover(1,4) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.121, 0.253, 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=()) ]\n"," # 20 : acción mover(5,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([50], dtype=int32)>, state=(), info=()) ]\n"," # 21 : acción intercambiar(5,3) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([153], dtype=int32)>, state=(), info=()) ]\n"," # 22 : acción mover(7,1) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.253, 0.   , 0.121, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([71], dtype=int32)>, state=(), info=()) ]\n"," # 23 : acción mover(0,8) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.121, 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n"," # 24 : acción intercambiar(6,2) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.718, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([162], dtype=int32)>, state=(), info=()) ]\n"," # 25 : acción mover(2,7) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.023, 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([27], dtype=int32)>, state=(), info=()) ]\n"," # 26 : acción mover(5,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.023, 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([59], dtype=int32)>, state=(), info=()) ]\n"," # 27 : acción mover(3,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.023, 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([33], dtype=int32)>, state=(), info=()) ]\n"," # 28 : acción intercambiar(9,3) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.023, 0.718, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([193], dtype=int32)>, state=(), info=()) ]\n"," # 29 : acción mover(5,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.023, 0.718, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([54], dtype=int32)>, state=(), info=()) ]\n"," # 30 : acción intercambiar(8,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.023, 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([183], dtype=int32)>, state=(), info=()) ]\n"," # 31 : acción mover(2,7) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.   , 0.121, 0.718, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([27], dtype=int32)>, state=(), info=()) ]\n"," # 32 : acción intercambiar(3,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([130], dtype=int32)>, state=(), info=()) ]\n"," # 33 : acción intercambiar(9,8) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([198], dtype=int32)>, state=(), info=()) ]\n"," # 34 : acción mover(7,4) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([74], dtype=int32)>, state=(), info=()) ]\n"," # 35 : acción mover(6,3) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([63], dtype=int32)>, state=(), info=()) ]\n"," # 36 : acción mover(7,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([79], dtype=int32)>, state=(), info=()) ]\n"," # 37 : acción mover(4,1) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.253, 0.   , 0.121, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([41], dtype=int32)>, state=(), info=()) ]\n"," # 38 : acción intercambiar(4,6) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.253, 0.   , 0.121, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([146], dtype=int32)>, state=(), info=()) ]\n"," # 39 : acción intercambiar(8,5) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.253, 0.   , 0.121, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([185], dtype=int32)>, state=(), info=()) ]\n"," # 40 : acción mover(8,2) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.253, 0.023, 0.   , 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([82], dtype=int32)>, state=(), info=()) ]\n"," # 41 : acción mover(6,0) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.253, 0.023, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([60], dtype=int32)>, state=(), info=()) ]\n"," # 42 : acción mover(5,7) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.253, 0.023, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([57], dtype=int32)>, state=(), info=()) ]\n"," # 43 : acción mover(7,2) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.   , 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([72], dtype=int32)>, state=(), info=()) ]\n"," # 44 : acción intercambiar(3,5) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.   , 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([135], dtype=int32)>, state=(), info=()) ]\n"," # 45 : acción mover(6,5) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.   , 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([65], dtype=int32)>, state=(), info=()) ]\n"," # 46 : acción intercambiar(7,1) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.   , 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([171], dtype=int32)>, state=(), info=()) ]\n"," # 47 : acción intercambiar(5,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.   , 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([154], dtype=int32)>, state=(), info=()) ]\n"," # 48 : acción intercambiar(4,5) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.   , 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([145], dtype=int32)>, state=(), info=()) ]\n"," # 49 : acción mover(8,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.   , 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([84], dtype=int32)>, state=(), info=()) ]\n"," # 50 : acción intercambiar(9,7) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.   , 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([197], dtype=int32)>, state=(), info=()) ]\n"," # 51 : acción mover(2,6) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.023, 0.718, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([26], dtype=int32)>, state=(), info=()) ]\n"," # 52 : acción mover(0,4) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.023, 0.718, 0.   , 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=()) ]\n"," # 53 : acción mover(3,2) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.023, 0.   , 0.718, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([32], dtype=int32)>, state=(), info=()) ]\n"," # 54 : acción mover(1,1) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.023, 0.   , 0.718, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=()) ]\n"," # 55 : acción mover(5,1) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.121, 0.023, 0.   , 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([51], dtype=int32)>, state=(), info=()) ]\n"," # 56 : acción mover(4,0) -> Estado/Reward  -10.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-10.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.253, 0.121, 0.023, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([40], dtype=int32)>, state=(), info=()) ]\n"," # 57 : acción intercambiar(1,6) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([116], dtype=int32)>, state=(), info=()) ]\n"," # 58 : acción mover(3,9) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([39], dtype=int32)>, state=(), info=()) ]\n"," # 59 : acción mover(2,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.   , 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n"," # 60 : acción intercambiar(3,5) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.   , 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([135], dtype=int32)>, state=(), info=()) ]\n"," # 61 : acción mover(9,1) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.718, 0.   , 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([91], dtype=int32)>, state=(), info=()) ]\n"," # 62 : acción intercambiar(3,3) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.718, 0.   , 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([133], dtype=int32)>, state=(), info=()) ]\n"," # 63 : acción mover(1,0) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.121, 0.718, 0.   , 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=()) ]\n"," # 64 : acción mover(1,4) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.718, 0.   , 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([14], dtype=int32)>, state=(), info=()) ]\n"," # 65 : acción intercambiar(8,0) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.718, 0.   , 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([180], dtype=int32)>, state=(), info=()) ]\n"," # 66 : acción intercambiar(1,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.   , 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([114], dtype=int32)>, state=(), info=()) ]\n"," # 67 : acción intercambiar(6,7) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.253, 0.   , 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([167], dtype=int32)>, state=(), info=()) ]\n"," # 68 : acción mover(1,8) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.   , 0.023, 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=()) ]\n"," # 69 : acción intercambiar(4,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.   , 0.023, 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([149], dtype=int32)>, state=(), info=()) ]\n"," # 70 : acción intercambiar(0,1) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.121, 0.023, 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([101], dtype=int32)>, state=(), info=()) ]\n"," # 71 : acción intercambiar(3,2) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.121, 0.718, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([132], dtype=int32)>, state=(), info=()) ]\n"," # 72 : acción intercambiar(9,2) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.121, 0.253, 0.023, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([192], dtype=int32)>, state=(), info=()) ]\n"," # 73 : acción intercambiar(1,4) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.253, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([114], dtype=int32)>, state=(), info=()) ]\n"," # 74 : acción intercambiar(1,1) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.253, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([111], dtype=int32)>, state=(), info=()) ]\n"," # 75 : acción mover(8,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.718, 0.253, 0.023, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([88], dtype=int32)>, state=(), info=()) ]\n"," # 76 : acción mover(7,1) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.121, 0.718, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([71], dtype=int32)>, state=(), info=()) ]\n"," # 77 : acción mover(4,0) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.   , 0.121, 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([40], dtype=int32)>, state=(), info=()) ]\n"," # 78 : acción intercambiar(8,8) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.   , 0.121, 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([188], dtype=int32)>, state=(), info=()) ]\n"," # 79 : acción mover(2,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.023, 0.   , 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n"," # 80 : acción intercambiar(4,6) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.023, 0.   , 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([146], dtype=int32)>, state=(), info=()) ]\n"," # 81 : acción mover(1,3) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.121, 0.   , 0.718, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=()) ]\n"," # 82 : acción intercambiar(2,0) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([120], dtype=int32)>, state=(), info=()) ]\n"," # 83 : acción intercambiar(9,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([198], dtype=int32)>, state=(), info=()) ]\n"," # 84 : acción intercambiar(4,4) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.023, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([144], dtype=int32)>, state=(), info=()) ]\n"," # 85 : acción intercambiar(8,3) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([183], dtype=int32)>, state=(), info=()) ]\n"," # 86 : acción mover(0,0) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n"," # 87 : acción intercambiar(8,8) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.   , 0.121, 0.253, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([188], dtype=int32)>, state=(), info=()) ]\n"," # 88 : acción intercambiar(1,3) -> Estado/Reward  -9.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.718, 0.253, 0.121, 0.   , 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([113], dtype=int32)>, state=(), info=()) ]\n"," # 89 : acción intercambiar(6,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.253, 0.121, 0.   , 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([160], dtype=int32)>, state=(), info=()) ]\n"," # 90 : acción mover(5,2) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.253, 0.718, 0.121, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([52], dtype=int32)>, state=(), info=()) ]\n"," # 91 : acción intercambiar(5,4) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.253, 0.718, 0.121, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([154], dtype=int32)>, state=(), info=()) ]\n"," # 92 : acción mover(4,7) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.253, 0.718, 0.121, 0.   , 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([47], dtype=int32)>, state=(), info=()) ]\n"," # 93 : acción intercambiar(9,2) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.253, 0.   , 0.121, 0.718, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([192], dtype=int32)>, state=(), info=()) ]\n"," # 94 : acción intercambiar(1,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.718, 0.   , 0.121, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([114], dtype=int32)>, state=(), info=()) ]\n"," # 95 : acción intercambiar(0,9) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.718, 0.   , 0.121, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([109], dtype=int32)>, state=(), info=()) ]\n"," # 96 : acción mover(7,7) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.253, 0.718, 0.   , 0.121, 0.023, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([77], dtype=int32)>, state=(), info=()) ]\n"," # 97 : acción intercambiar(7,0) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.718, 0.   , 0.121, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([170], dtype=int32)>, state=(), info=()) ]\n"," # 98 : acción intercambiar(8,3) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.718, 0.   , 0.253, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([183], dtype=int32)>, state=(), info=()) ]\n"," # 99 : acción mover(1,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.   , 0.253, 0.718, 0.121, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([13], dtype=int32)>, state=(), info=()) ]\n"," # 100 : acción intercambiar(2,7) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.023, 0.   , 0.121, 0.718, 0.253, 0.977, 0.983, 0.989, 0.994,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([127], dtype=int32)>, state=(), info=()) ]\n"," Recompensa Final =  -2.0\n"," Lista Final =  [-66 -70 -49  55 -26]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-G18iz7flcn","cellView":"form","executionInfo":{"status":"ok","timestamp":1612649069674,"user_tz":180,"elapsed":6722,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"233ca00a-ac52-4e6e-e8da-655a419e86e2"},"source":["#@title Definir Métricas para evaluación\r\n","\r\n","# Se usa el promedio de la recompensa (la más común)\r\n","# See also the metrics module for standard implementations of different metrics.\r\n","# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\r\n","\r\n","def compute_avg_return(environment, policy, num_episodes=10):\r\n","\r\n","  total_return = 0.0\r\n","  for _ in range(num_episodes):\r\n","\r\n","    time_step = environment.reset()\r\n","    episode_return = 0.0\r\n","    while not time_step.is_last():\r\n","      action_step = policy.action(time_step)\r\n","      time_step = environment.step(action_step.action)\r\n","      episode_return += time_step.reward\r\n","    total_return += episode_return\r\n","\r\n","  avg_return = total_return / num_episodes\r\n","  return avg_return.numpy()[0]\r\n","\r\n","print(\"Métricas definidas.\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Métricas definidas.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dLCBLMD4Zsia"},"source":["3) Llevar a cabo el Entrenamiento:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"diEOEg3JaMHa","cellView":"form","executionInfo":{"status":"ok","timestamp":1612649070351,"user_tz":180,"elapsed":7389,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"1ed1cd02-e1c7-4b07-ffc1-f8f27b970c78"},"source":["#@title Definir el Agente tipo DQN\r\n","\r\n","##learning_rate = 1e-3  # @param {type:\"number\"}\r\n","##cant_neuronas_ocultas = 100 # @param {type:\"integer\"}\r\n","learning_rate = 1e-3  # @param {type:\"number\"}\r\n","cant_neuronas_ocultas = \"100, 50, 25\" # @param {type:\"string\"}\r\n","\r\n","# Define cantidad de neuronas ocultas para RNA-Q\r\n","hidden_layers = []\r\n","for val in cant_neuronas_ocultas.split(','):\r\n","  if  int(val) < 1:\r\n","    hidden_layers.append( 10 )\r\n","  else:\r\n","    hidden_layers.append( int(val) )\r\n","fc_layer_params = tuple(hidden_layers, )\r\n","\r\n","# Define RNA-Q\r\n","q_net = q_network.QNetwork(\r\n","    train_env.observation_spec(),\r\n","    train_env.action_spec(),\r\n","    fc_layer_params=fc_layer_params)\r\n","\r\n","optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\r\n","\r\n","train_step_counter = tf.Variable(0)\r\n","\r\n","# Define el agente de tipo Q\r\n","ag = dqn_agent.DqnAgent(\r\n","    train_env.time_step_spec(),\r\n","    train_env.action_spec(),\r\n","    q_network=q_net,\r\n","    optimizer=optimizer,\r\n","    td_errors_loss_fn=common.element_wise_squared_loss,\r\n","    train_step_counter=train_step_counter)\r\n","\r\n","ag.initialize()\r\n","\r\n","# define política para evaluación para el Agente\r\n","eval_policy = ag.policy\r\n","\r\n","# define política para recolección de datos para el Agente\r\n","collect_policy = ag.collect_policy\r\n","\r\n","print(\"Agente DQN inicializado. \")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Agente DQN inicializado. \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EBRZGSkZ5N6","cellView":"form","executionInfo":{"status":"ok","timestamp":1612649072964,"user_tz":180,"elapsed":9992,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"4c80cf19-daf6-41e8-c4ad-0be521024f6a"},"source":["#@title Preparar datos para Entrenamiento\r\n","\r\n","initial_collect_steps =   100# @param {type:\"integer\"} \r\n","collect_steps_per_iteration = 1  # @param {type:\"integer\"}\r\n","replay_buffer_max_length = 100000  # @param {type:\"integer\"}\r\n","batch_size = 64  # @param {type:\"integer\"}\r\n","\r\n","\r\n","# Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\r\n","replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n","    data_spec = ag.collect_data_spec,\r\n","    batch_size = train_env.batch_size,\r\n","    max_length = replay_buffer_max_length)\r\n","\r\n","# Recolecta datos generados al azar\r\n","# This loop is so common in RL, that we provide standard implementations. \r\n","# For more details see the drivers module.\r\n","# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\r\n","\r\n","def collect_step(environment, policy, buffer):\r\n","  time_step = environment.current_time_step()\r\n","  action_step = policy.action(time_step)\r\n","  next_time_step = environment.step(action_step.action)\r\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n","\r\n","  # Add trajectory to the replay buffer\r\n","  buffer.add_batch(traj)\r\n","\r\n","def collect_data(env, policy, buffer, steps):\r\n","  for _ in range(steps):\r\n","    collect_step(env, policy, buffer)\r\n","\r\n","collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\r\n","\r\n","print(\"\\nDatos recolectados.\")\r\n","\r\n","# Muestra ejemplo de los datos recolectados\r\n","##iter(replay_buffer.as_dataset()).next()\r\n","\r\n","# Preparar los datos recolectados con trajectories de shape [Bx2x...]\r\n","dataset = replay_buffer.as_dataset(\r\n","    num_parallel_calls=3, \r\n","    sample_batch_size=batch_size, \r\n","    num_steps=2).prefetch(3)\r\n","iterator = iter(dataset)\r\n","# Muestra ejemplo \r\n","##iterator.next()\r\n","print(\"\\nDataset creado.\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\n","Datos recolectados.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n","\n","Dataset creado.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2k3S5IqGhK-a","cellView":"form","outputId":"c8cab196-599c-45b8-b950-9f8ff5c8dd56"},"source":["#@title Entrenar al Agente\r\n","\r\n","cant_ciclos_entrenamiento =  25000# @param {type:\"integer\"}\r\n","log_cada_ciclos = 200  # @param {type:\"integer\"}\r\n","mostar_recompensa_cada = 600  # @param {type:\"integer\"}\r\n","cant_episodios_evaluacion =  25# @param {type:\"integer\"}\r\n","\r\n","#  Optimize by wrapping some of the code in a graph using TF function (Optional)\r\n","ag.train = common.function(ag.train)\r\n","\r\n","# Reset the train step\r\n","ag.train_step_counter.assign(0)\r\n","\r\n","# Evaluate the agent's policy once before training.\r\n","avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","ar_ciclo = []\r\n","ar_returns = []\r\n","ar_loss = []\r\n","\r\n","print(\"\\n** Comienza el Entrenamiento **\\n\")\r\n","for _ in range(cant_ciclos_entrenamiento):\r\n","\r\n","  # Collect a few steps using collect_policy and save to the replay buffer.\r\n","  collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n","\r\n","  # Sample a batch of data from the buffer and update the agent's network.\r\n","  experience, unused_info = next(iterator)\r\n","  train_loss = ag.train(experience).loss\r\n","\r\n","  step = ag.train_step_counter.numpy()\r\n","\r\n","  if (step == 1) or (step % log_cada_ciclos == 0):\r\n","    print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \r\n","    ar_ciclo.append( step )\r\n","    ar_loss.append( train_loss )\r\n","    avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","    ar_returns.append( avg_return )\r\n","\r\n","    if (step == 1) or (step % mostar_recompensa_cada == 0):\r\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\r\n","\r\n","print(\"\\n** Entrenamiento Finalizado **\\n\")\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","** Comienza el Entrenamiento **\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.foldr(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n","step = 1: loss = 365.640\n","step = 1: Promedio Recompensa = -1238.8\n","step = 200: loss = 10.036\n","step = 400: loss = 11.716\n","step = 600: loss = 8.064\n","step = 600: Promedio Recompensa = -1415.8\n","step = 800: loss = 5.756\n","step = 1000: loss = 79.635\n","step = 1200: loss = 22.386\n","step = 1200: Promedio Recompensa = -1098.1\n","step = 1400: loss = 36.395\n","step = 1600: loss = 35.572\n","step = 1800: loss = 32.441\n","step = 1800: Promedio Recompensa = -1166.9\n","step = 2000: loss = 21.165\n","step = 2200: loss = 42.302\n","step = 2400: loss = 123.981\n","step = 2400: Promedio Recompensa = -733.0\n","step = 2600: loss = 24.748\n","step = 2800: loss = 26.849\n","step = 3000: loss = 141.896\n","step = 3000: Promedio Recompensa = -1040.2\n","step = 3200: loss = 89.494\n","step = 3400: loss = 142.535\n","step = 3600: loss = 42.297\n","step = 3600: Promedio Recompensa = -867.0\n","step = 3800: loss = 22.802\n","step = 4000: loss = 120.183\n","step = 4200: loss = 37.662\n","step = 4200: Promedio Recompensa = -842.0\n","step = 4400: loss = 43.814\n","step = 4600: loss = 19.899\n","step = 4800: loss = 27.552\n","step = 4800: Promedio Recompensa = -1002.4\n","step = 5000: loss = 36.726\n","step = 5200: loss = 104.475\n","step = 5400: loss = 31.645\n","step = 5400: Promedio Recompensa = -779.6\n","step = 5600: loss = 26.824\n","step = 5800: loss = 31.736\n","step = 6000: loss = 60.984\n","step = 6000: Promedio Recompensa = -992.5\n","step = 6200: loss = 144.018\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EBBl7mRkQYa","cellView":"form"},"source":["#@title Mostrar Gráficos del Entrenamiento\r\n","\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","plt.plot( ar_ciclo, ar_returns)\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","#plt.plot( ar_ciclo, ar_returns)\r\n","plt.plot( ar_ciclo, ar_loss, color=\"red\" )\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j79yPUetlUbs"},"source":["4) Probar entrenamiento comparando resultados:"]},{"cell_type":"code","metadata":{"id":"lLkdkcBjl3Xs","cellView":"form"},"source":["#@title Probar el Agente Entrenado contra el Azar\r\n","cantidad_probar = 1 # @param {type:\"integer\"}\r\n","promAzar = 0\r\n","promAgente = 0\r\n","\r\n","# determina política a usar\r\n","policy_agente_entrenado = None\r\n","if not('ag' in vars() or 'ag' in globals()) or ag is None:\r\n","  if not('saved_policy' in vars() or 'saved_policy' in globals()) or saved_policy is None:\r\n","    ValueError(\"No hay política entrenada definida.\")\r\n","  else:\r\n","    policy_agente_entrenado = saved_policy\r\n","    print(\"- Se usa la política recuperada del drive.\")\r\n","else:\r\n","  policy_agente_entrenado = ag.policy\r\n","  print(\"- Se usa la política del modelo entrenado.\")\r\n","\r\n","for i in range(cantidad_probar):\r\n","\r\n","  print(\"\\n> Prueba \", i+1, \":\")\r\n","\r\n","  # crea nuevo entorno que mantiene la misma lista\r\n","  prueba_env =  tf_py_environment.TFPyEnvironment( OrdenarListasEnv(False) )\r\n","\r\n","  # Probar Aleatorio\r\n","  valorAzar = SimularEntorno(prueba_env, random_policy, \"Resultados Aleatorio\") \r\n","  promAzar = promAzar + valorAzar\r\n","\r\n","  # Probar Agente Entrenado\r\n","  valorAgente = SimularEntorno(prueba_env, policy_agente_entrenado, \"Resultados de Agente Entrenado\") \r\n","  promAgente = promAgente + valorAgente\r\n","\r\n","  # Decide Ganador\r\n","  if valorAzar < valorAgente:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera MEJOR resultado que el azar (\", valorAzar,\")\")\r\n","  else:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera PEOR resultado que el azar (\", valorAzar,\")\")\r\n","\r\n","# Decide Ganador General\r\n","if cantidad_probar > 0:\r\n","  promAgente = promAgente / cantidad_probar\r\n","  promAzar = promAzar / cantidad_probar\r\n","  print(\"\\n================================================================================================\\n\")\r\n","  if promAzar < promAgente:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene MEJORES resultado que  el azar (\", promAzar,\")\")\r\n","  else:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene PEORES resultados que el azar (\", promAzar,\")\")\r\n","  print(\"\\n================================================================================================\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rjj7bRe7AB-l"},"source":["5) Cargar / Graba el modelo de las políticas entrenadas:"]},{"cell_type":"code","metadata":{"cellView":"form","id":"6V2EiqdwAy_R"},"source":["#@title Cargar o Guardar el Modelo\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demo Agentes/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-OrdenarLista\" #@param {type:\"string\"}\n","accion_realizar = \"-\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","# determina lugar donde se guarda el modelo\n","policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","\n","if accion_realizar != \"-\":\n","  # Montar Drive\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","\n","if accion_realizar == \"Grabar Modelo\":\n","  # guarda la politica del agente entrenado\n","  tf_policy_saver = policy_saver.PolicySaver(ag.policy)\n","  tf_policy_saver.save(policy_dir)\n","  print(\"\\nPolítica del modelo guardada en \", policy_dir)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  saved_policy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica del modelo recuperada de \", policy_dir)"],"execution_count":null,"outputs":[]}]}