{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN Ordenar Lista.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPVGSROF36Nxk3aOLSNQjR/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para ordenar elmentos de una lista usando DQN:\r\n"]},{"cell_type":"markdown","metadata":{"id":"dliJD0WRUMWV"},"source":["0) Preparar el ambiente:"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Qxbe02w0T0ip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612791204814,"user_tz":180,"elapsed":2946,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"007d66fd-3df2-49cd-ceab-9f67a5e088e9"},"source":["#@title Instalar Paquete de TF-Agents\r\n","!pip install -q tf-agents\r\n","print(\"TF-Agentes instalado.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TF-Agentes instalado.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wJl4YsniURev","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612791206979,"user_tz":180,"elapsed":5097,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"61f8b2b3-02e0-4f66-af2b-e4bb8b7b7c19"},"source":["#@title Cargar Librerías\r\n","from __future__ import absolute_import\r\n","from __future__ import division\r\n","from __future__ import print_function\r\n","\r\n","import abc\r\n","import tensorflow as tf\r\n","import numpy as np\r\n","import matplotlib\r\n","import matplotlib.pyplot as plt\r\n","from random import randint\r\n","from sklearn import preprocessing\r\n","import copy\r\n","\r\n","from tf_agents.environments import py_environment\r\n","from tf_agents.environments import tf_py_environment\r\n","\r\n","from tf_agents.environments import utils\r\n","from tf_agents.specs import array_spec\r\n","\r\n","from tf_agents.policies import random_tf_policy\r\n","\r\n","from tf_agents.trajectories import time_step as ts\r\n","\r\n","from tf_agents.agents.dqn import dqn_agent\r\n","from tf_agents.networks import q_network\r\n","from tf_agents.utils import common\r\n","\r\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n","from tf_agents.trajectories import trajectory\r\n","\r\n","import os\r\n","from tf_agents.policies import policy_saver\r\n","\r\n","tf.compat.v1.enable_v2_behavior()\r\n","\r\n","print(\"Librerías cargadas.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Librerías cargadas.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3ONe5w_nUYME"},"source":["1) Establecer las clases sobre el Problema a resolver:"]},{"cell_type":"code","metadata":{"id":"6TQx1eodsvKj","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612791206982,"user_tz":180,"elapsed":5089,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"f6dd6cae-d0c2-4c44-9027-5886f52c4060"},"source":["#@title Definir las primitivas a usar para ordenar la lista\r\n","\r\n","# intercambia valores de pos1 con pos2 \r\n","def intercambiar(lista, pos1, pos2):\r\n","  # chequea que las posiciones no se encuentran fuera de la lista\r\n","  if (pos1 < 0) or (pos1 >= len(lista)):\r\n","    if (pos1 < 0):\r\n","      pos1 = 0\r\n","    else:\r\n","      pos1 =  len(lista) - 1\r\n","  if (pos2 < 0) or (pos2 >= len(lista)):\r\n","    if (pos2 < 0):\r\n","      pos2 = 0\r\n","    else:\r\n","      pos2 =  len(lista) - 1\r\n","  # chequea que no sean las mismas posiciones\r\n","  if pos1 == pos2:\r\n","    return lista\r\n","  else:    \r\n","    # realiza el intercambio\r\n","    lista[pos1], lista[pos2] = lista[pos2], lista[pos1]\r\n","    return lista\r\n","\r\n","# mueve el valor de posAnt a posNueva\r\n","def mover(lista, posAnt, posNueva):\r\n","  # chequea que las posiciones no se encuentran fuera de la lista\r\n","  if (posAnt < 0) or (posAnt >= len(lista)):\r\n","    if (posAnt < 0):\r\n","      posAnt = 0\r\n","    else:\r\n","      posAnt =  len(lista) - 1\r\n","  if (posNueva < 0) or (posNueva >= len(lista)):\r\n","    if (posNueva < 0):\r\n","      posNueva = 0\r\n","    else:\r\n","      posNueva =  len(lista) - 1\r\n","  # chequea que no sean las mismas posiciones\r\n","  if posAnt == posNueva:\r\n","    return lista\r\n","  else:    \r\n","    # realiza el intercambio\r\n","    lista.insert(posNueva, lista.pop(posAnt))\r\n","    return lista\r\n","\r\n","# función auxiliar para contar la cantidad de desordenados\r\n","# (debe ser de menor a mayor)\r\n","def contarDesordenados(lista):  \r\n","  cantError = 0\r\n","  if len(lista) > 0:\r\n","    i = 0 \r\n","    while i < len(lista):\r\n","      ant = lista[i]\r\n","      j = i + 1\r\n","      while j < len(lista):\r\n","        actual = lista[j]\r\n","        if actual < ant:\r\n","          cantError = cantError + 1\r\n","        j = j + 1      \r\n","      i = i + 1\r\n","  return cantError\r\n","\r\n","# variable auxiliar para determinar máximo de acciones a probar antes de abortar\r\n","POSIBLES_ACCIONES_DESC = [ \"mover\", \"intercambiar\" ]\r\n","POSIBLES_ACCIONES = [ mover,  intercambiar ]\r\n","\r\n","print(\"Primitivas de acciones definidas: \", POSIBLES_ACCIONES_DESC)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Primitivas de acciones definidas:  ['mover', 'intercambiar']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_R9SyNuiUjyT","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1612791208419,"user_tz":180,"elapsed":6517,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"542fec51-1204-4595-ec1f-913979e9e58a"},"source":["#@title Definir Entorno del Problema \n","\n","# parámetros generales\n","MAX_ITERACIONES_REALIZAR = 100\n","MAX_ITERACIONES_ACCIONES_IGUALES = 4\n","TAMANIO_MINIMO_LISTA = 3\n","TAMANIO_MAXIMO_LISTA = 10\n","MAXIMO_VALOR_ACTION = ((len(POSIBLES_ACCIONES)-1)*100) + (TAMANIO_MAXIMO_LISTA-1) * 10) + (TAMANIO_MAXIMO_LISTA-1)\n","\n","\n","def parsearAccion(action):\n","  # como DQN sólo permite 1 action numérica\n","  # esta función se ocupa de parsearla para determinar:\n","  #    tipo de acción\n","  #    param1\n","  #    param2\n","  aux = action\n","  idAccion = aux // 100\n","  aux = aux - idAccion * 100\n","  param1 = aux // 10\n","  aux = aux - param1 * 10\n","  param2 = aux\n","  #print(action, idAccion, param1, param2)\n","  return idAccion, param1, param2\n","\n","# Un entorno que represente el juego podría verse así:\n","class OrdenarListasEnv(py_environment.PyEnvironment):\n","\n","  def __init__(self, reGenerarReset=True):\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=MAXIMO_VALOR_ACTION, name='action')\n","    self._observation_spec = array_spec.BoundedArraySpec(\n","        shape=(TAMANIO_MAXIMO_LISTA,), dtype=np.float32, name='observation')      \n","    self._state = 0\n","    self._antAction = -1\n","    self._episode_ended = False\n","    self._reGenerarListaReset = reGenerarReset\n","    if self._reGenerarListaReset:\n","      # inicializa vacía porque se define en el reset\n","      self._listaOriginal = []\n","    else:\n","      # la lista se define sólo al principio, luego se vuelve a desordenar\n","      self._listaOriginal = self.crearLista()\n","    self._lista = []\n","\n","  def action_spec(self):\n","    # devuelve la forma de las acciones\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    # devuelve la forma de las observaciones   \n","    return self._observation_spec\n","\n","  def render(self, mode = 'human'):\n","    # devuelve la lista para mostsrar\n","    return np.array(self._lista, dtype=np.int32)\n","\n","  def _reset(self):\n","    # resetea el entorno\n","    if self._reGenerarListaReset:\n","      # cada vez que se reseta, se define la lista\n","      self._listaOriginal = self.crearLista()\n","    # siempre la lista de trabajo se copia de la original\n","    self._lista = copy.deepcopy( self._listaOriginal ) \n","    # actualiza el estado considerando cantidad de ordenados\n","    self.actualizarEstado()\n","    self._cantIteraciones = 0\n","    self._episode_ended = False\n","    self._antAction = []\n","    return ts.restart(self.devolverObsActual())\n","\n","  def crearLista(self):\n","    # genera los valores de las listas al azar\n","    cantElemRnd = randint(TAMANIO_MINIMO_LISTA, TAMANIO_MAXIMO_LISTA)\n","    #cantElemRnd = TAMANIO_MAXIMO_LISTA\n","    lista = []\n","    for j in range(cantElemRnd): \n","      lista.append( randint(-99, 99) )\n","    return lista\n","\n","  def actualizarEstado(self):\n","    # actualiza el valor del estado del entorno\n","    # teniendo en cuenta la cantidad de errores negativos\n","    self._state = - contarDesordenados(self._lista)\n","    return self._state\n","\n","  def devolverObsActual(self):\n","    # devuelve valores para la observación actual\n","    # los valores de la lista (rellenando con cero)\n","    # para que el agente sepa el estado real del entorno\n","    res = []\n","    res.extend( self._lista )\n","    val = 100\n","    while (len(res) < TAMANIO_MAXIMO_LISTA):\n","      res.append( val )\n","      val = val + 1\n","    # nota: para DQN parece ser que conviene \n","    # normalizar los valores para que sean más homogeneos \n","    # y no demasiado dispares entre sí \n","    # (sino genera un 'loss' demasiado grande)\n","    r = (res - np.min(res)) / (np.max(res) - np.min(res))\n","    return  np.array([round(v,3) for v in r], dtype=np.float32)\n","\n","  def _step(self, action):\n","    # aplica una acción sobre el entorno\n","    \n","    if self._episode_ended:\n","      # si el entorno está finalizado, lo resetea\n","      return self.reset()\n","\n","    # actualiza cantidad de interacciones \n","    self._cantIteraciones = self._cantIteraciones - 1\n","\n","    # parsea la accion para determinar acción con sus parámetros\n","    idAccion, param1, param2 = parsearAccion(action)\n","\n","    # si es un id de acción válida\n","    if idAccion >= 0 and idAccion < len(POSIBLES_ACCIONES):\n","      # aplica la acción correspondiente en cada lista\n","      # y calculando la cantidad de desordenados como error\n","      self._lista = POSIBLES_ACCIONES[idAccion](self._lista, param1, param2)\n","      \n","      # actualiza el estado con la cantidad de valores correctos\n","      self.actualizarEstado()\n","\n","      # controla que no sea una acción repetida\n","      # de forma que si empieza a repetir más de una cantidad máxima\n","      # finaliza la operatoria\n","      i = 0\n","      cantActionIguales = 0 \n","      while i < len(self._antAction):\n","        if self._antAction[i] == action:\n","          cantActionIguales = cantActionIguales + 1\n","        i = i + 1             \n","      # siempre mantiene 3 (saca la más vieja)\n","      # y agrega la nueva\n","      if len(self._antAction) >= MAX_ITERACIONES_ACCIONES_IGUALES:\n","        self._antAction.pop( 0 )\n","      self._antAction.append( action )\n","\n","    # determina si debe finalizar o no\n","    if (self._state == 0) or (abs(self._cantIteraciones) >= abs(MAX_ITERACIONES_REALIZAR)) or (cantActionIguales >= MAX_ITERACIONES_ACCIONES_IGUALES):\n","      # si está todo ordenado \n","      # o si la cantidad de iteraciones llega al límite\n","      # o se repitió muchas veces la misma action\n","      # fuerza que finaliza\n","      self._episode_ended = True\n","\n","    if self._episode_ended:\n","      # si finaliza\n","      # devuelve el reward (siempre se maximiza):\n","      # si logra ordenar\n","      # se calcula penalizando la cantidad de iteraciones \n","      if (self._state == 0):\n","        reward = MAX_ITERACIONES_REALIZAR + self._cantIteraciones\n","      else:\n","        reward = self._state\n","      return ts.termination(self.devolverObsActual(), reward)\n","    else:\n","      # si no finaliza\n","      reward = self._state\n","      return ts.transition(\n","         self.devolverObsActual(), reward=self._state, discount=0.9)\n","         # notar que no se usa discount=1.0 porque sino genera problema de 'loss' muy grande\n","\n","print(\"\\nEntorno del Problema definido.\")\n","\n","# Definir entornos de entrenamiento y de evaluación\n","# (ambos con lista que se cambia cada vez que se resetea)\n","train_py_env = OrdenarListasEnv(True)\n","eval_py_env = OrdenarListasEnv(True)\n","\n","# Definir wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","print(\"\\nEntornos de entrenamiento y prueba definidos. \")\n","\n","# definir simulador para probar el entorno\n","def SimularEntorno(env, policy, titulo, mostrarDetalle=True):\n","    print(\"\\n** \", titulo, \"**\")                   \n","    # muesta estado inicial\n","    time_step = env.reset()      \n","    #ob = time_step.observation.numpy()[0]\n","    if mostrarDetalle:\n","      print(\" Ini: [\", time_step, \"]\")    \n","    print(\" Lista Inicial = \", env.pyenv.render()[0] )\n","    j = 1\n","    while not time_step.is_last():\n","      # la política determina la acción a realizar\n","      action_step = policy.action(time_step)\n","      time_step = env.step(action_step.action)\n","      # recupera la observación y muestra el nuevo estado \n","      ac = action_step.action.numpy()[0]\n","      idAccion, param1, param2 = parsearAccion(ac)\n","      r = time_step.reward.numpy()[0]\n","      ##ob = time_step.observation.numpy()[0]\n","      descAccion = \"acción \" +  POSIBLES_ACCIONES_DESC[ idAccion ] + \"(\" + str(param1) + \",\" + str(param2) + \")\"\n","      if mostrarDetalle:\n","        print(\"  #\", j, \":\", descAccion, \"-> Estado/Reward \", r, \"[\", time_step, \",\", action_step, \"]\")\n","      else:\n","        print(\"  #\", j, \":\", descAccion, \"-> Estado/Reward \", r)\n","      ### print(\"    Lista = \", env.pyenv.render()[0] )\n","      j = j + 1\n","    # muestra estado final\n","    print(\" Recompensa Final = \", r )\n","    print(\" Lista Final = \", env.pyenv.render()[0] )\n","    return r\n","\n","print(\"Simulador del entorno definido.\")\n","\n","# Probar el entorno definido con Política Aleatoria (opcional)\n","Probar_Entorno = True #@param {type:\"boolean\"}\n","if Probar_Entorno:\n","  SimularEntorno(eval_env, random_policy, \"Probando el entorno del problema con política al azar\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Entorno del Problema definido.\n","\n","Entornos de entrenamiento y prueba definidos. \n","Simulador del entorno definido.\n","\n","**  Probando el entorno del problema con política al azar **\n"," Ini: [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.278, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) ]\n"," Lista Inicial =  [ 21  27 -35 -89]\n","  # 1 : acción intercambiar(6,5) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.278, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([165], dtype=int32)>, state=(), info=()) ]\n","  # 2 : acción intercambiar(3,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.278, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([138], dtype=int32)>, state=(), info=()) ]\n","  # 3 : acción mover(1,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.278, 0.   , 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>, state=(), info=()) ]\n","  # 4 : acción mover(2,0) -> Estado/Reward  -1.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.567, 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n","  # 5 : acción intercambiar(7,1) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([171], dtype=int32)>, state=(), info=()) ]\n","  # 6 : acción intercambiar(4,9) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([149], dtype=int32)>, state=(), info=()) ]\n","  # 7 : acción mover(5,7) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([57], dtype=int32)>, state=(), info=()) ]\n","  # 8 : acción mover(7,6) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([76], dtype=int32)>, state=(), info=()) ]\n","  # 9 : acción intercambiar(8,3) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([183], dtype=int32)>, state=(), info=()) ]\n","  # 10 : acción intercambiar(9,5) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([195], dtype=int32)>, state=(), info=()) ]\n","  # 11 : acción mover(9,3) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([93], dtype=int32)>, state=(), info=()) ]\n","  # 12 : acción intercambiar(7,3) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([173], dtype=int32)>, state=(), info=()) ]\n","  # 13 : acción mover(4,8) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([48], dtype=int32)>, state=(), info=()) ]\n","  # 14 : acción mover(7,5) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([75], dtype=int32)>, state=(), info=()) ]\n","  # 15 : acción intercambiar(0,8) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.278, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([108], dtype=int32)>, state=(), info=()) ]\n","  # 16 : acción intercambiar(3,2) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([132], dtype=int32)>, state=(), info=()) ]\n","  # 17 : acción mover(8,1) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.278, 0.598, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([81], dtype=int32)>, state=(), info=()) ]\n","  # 18 : acción intercambiar(8,9) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.278, 0.598, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([189], dtype=int32)>, state=(), info=()) ]\n","  # 19 : acción intercambiar(1,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([116], dtype=int32)>, state=(), info=()) ]\n","  # 20 : acción mover(5,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([59], dtype=int32)>, state=(), info=()) ]\n","  # 21 : acción intercambiar(8,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([184], dtype=int32)>, state=(), info=()) ]\n","  # 22 : acción intercambiar(4,7) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([147], dtype=int32)>, state=(), info=()) ]\n","  # 23 : acción mover(8,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([86], dtype=int32)>, state=(), info=()) ]\n","  # 24 : acción mover(6,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([66], dtype=int32)>, state=(), info=()) ]\n","  # 25 : acción intercambiar(3,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([136], dtype=int32)>, state=(), info=()) ]\n","  # 26 : acción mover(6,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([64], dtype=int32)>, state=(), info=()) ]\n","  # 27 : acción intercambiar(7,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([179], dtype=int32)>, state=(), info=()) ]\n","  # 28 : acción mover(5,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([59], dtype=int32)>, state=(), info=()) ]\n","  # 29 : acción mover(8,8) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([88], dtype=int32)>, state=(), info=()) ]\n","  # 30 : acción mover(6,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([64], dtype=int32)>, state=(), info=()) ]\n","  # 31 : acción mover(3,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([39], dtype=int32)>, state=(), info=()) ]\n","  # 32 : acción mover(9,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([99], dtype=int32)>, state=(), info=()) ]\n","  # 33 : acción intercambiar(5,8) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([158], dtype=int32)>, state=(), info=()) ]\n","  # 34 : acción intercambiar(3,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([134], dtype=int32)>, state=(), info=()) ]\n","  # 35 : acción intercambiar(9,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([194], dtype=int32)>, state=(), info=()) ]\n","  # 36 : acción mover(8,4) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([84], dtype=int32)>, state=(), info=()) ]\n","  # 37 : acción mover(9,7) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([97], dtype=int32)>, state=(), info=()) ]\n","  # 38 : acción intercambiar(0,0) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([100], dtype=int32)>, state=(), info=()) ]\n","  # 39 : acción intercambiar(1,7) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.278, 0.598, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([117], dtype=int32)>, state=(), info=()) ]\n","  # 40 : acción mover(1,8) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([18], dtype=int32)>, state=(), info=()) ]\n","  # 41 : acción intercambiar(3,6) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([136], dtype=int32)>, state=(), info=()) ]\n","  # 42 : acción mover(2,2) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([22], dtype=int32)>, state=(), info=()) ]\n","  # 43 : acción mover(6,9) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([69], dtype=int32)>, state=(), info=()) ]\n","  # 44 : acción intercambiar(5,1) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.278, 0.   , 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([151], dtype=int32)>, state=(), info=()) ]\n","  # 45 : acción intercambiar(8,1) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([181], dtype=int32)>, state=(), info=()) ]\n","  # 46 : acción mover(1,2) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([12], dtype=int32)>, state=(), info=()) ]\n","  # 47 : acción mover(7,7) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([77], dtype=int32)>, state=(), info=()) ]\n","  # 48 : acción intercambiar(5,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([155], dtype=int32)>, state=(), info=()) ]\n","  # 49 : acción mover(0,4) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=()) ]\n","  # 50 : acción mover(2,9) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.567, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, state=(), info=()) ]\n","  # 51 : acción mover(5,6) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.598, 0.567, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([56], dtype=int32)>, state=(), info=()) ]\n","  # 52 : acción mover(0,3) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.567, 0.278, 0.   , 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=()) ]\n","  # 53 : acción intercambiar(1,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.   , 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([115], dtype=int32)>, state=(), info=()) ]\n","  # 54 : acción intercambiar(9,7) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.   , 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([197], dtype=int32)>, state=(), info=()) ]\n","  # 55 : acción intercambiar(5,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.   , 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([155], dtype=int32)>, state=(), info=()) ]\n","  # 56 : acción intercambiar(4,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.   , 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([143], dtype=int32)>, state=(), info=()) ]\n","  # 57 : acción intercambiar(1,1) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.   , 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([111], dtype=int32)>, state=(), info=()) ]\n","  # 58 : acción mover(7,7) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.   , 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([77], dtype=int32)>, state=(), info=()) ]\n","  # 59 : acción mover(9,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.   , 0.278, 0.567, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([93], dtype=int32)>, state=(), info=()) ]\n","  # 60 : acción mover(4,1) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.567, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([41], dtype=int32)>, state=(), info=()) ]\n","  # 61 : acción mover(0,8) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n","  # 62 : acción intercambiar(2,5) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([125], dtype=int32)>, state=(), info=()) ]\n","  # 63 : acción mover(3,2) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([32], dtype=int32)>, state=(), info=()) ]\n","  # 64 : acción intercambiar(4,8) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([148], dtype=int32)>, state=(), info=()) ]\n","  # 65 : acción intercambiar(9,5) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([195], dtype=int32)>, state=(), info=()) ]\n","  # 66 : acción mover(8,7) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([87], dtype=int32)>, state=(), info=()) ]\n","  # 67 : acción mover(5,3) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([53], dtype=int32)>, state=(), info=()) ]\n","  # 68 : acción mover(7,0) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.567, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([70], dtype=int32)>, state=(), info=()) ]\n","  # 69 : acción intercambiar(8,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.598, 0.567, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([189], dtype=int32)>, state=(), info=()) ]\n","  # 70 : acción mover(0,8) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n","  # 71 : acción mover(7,8) -> Estado/Reward  -2.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-2.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.278, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([78], dtype=int32)>, state=(), info=()) ]\n","  # 72 : acción intercambiar(2,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.   , 0.598, 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([123], dtype=int32)>, state=(), info=()) ]\n","  # 73 : acción intercambiar(1,2) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([112], dtype=int32)>, state=(), info=()) ]\n","  # 74 : acción mover(5,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([54], dtype=int32)>, state=(), info=()) ]\n","  # 75 : acción intercambiar(7,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([174], dtype=int32)>, state=(), info=()) ]\n","  # 76 : acción intercambiar(8,6) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.598, 0.   , 0.278, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([186], dtype=int32)>, state=(), info=()) ]\n","  # 77 : acción intercambiar(1,3) -> Estado/Reward  -3.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.567, 0.278, 0.   , 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([113], dtype=int32)>, state=(), info=()) ]\n","  # 78 : acción intercambiar(0,2) -> Estado/Reward  22.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([22.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.278, 0.567, 0.598, 0.974, 0.979, 0.985, 0.99 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([102], dtype=int32)>, state=(), info=()) ]\n"," Recompensa Final =  22.0\n"," Lista Final =  [-89 -35  21  27]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"33A7XFXEVwS_"},"source":["2) Establecer clase para el Agente:"]},{"cell_type":"code","metadata":{"id":"diEOEg3JaMHa","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612791208709,"user_tz":180,"elapsed":6755,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"7c56e24f-88cf-4936-d951-6d530aa9c784"},"source":["#@title Definir el Agente tipo DQN\r\n","\r\n","##learning_rate = 1e-3  # @param {type:\"number\"}\r\n","##cant_neuronas_ocultas = 100 # @param {type:\"integer\"}\r\n","learning_rate = 1e-3  # @param {type:\"number\"}\r\n","cant_neuronas_ocultas = \"100, 50, 25\" # @param {type:\"string\"}\r\n","\r\n","# Define cantidad de neuronas ocultas para RNA-Q\r\n","hidden_layers = []\r\n","for val in cant_neuronas_ocultas.split(','):\r\n","  if  int(val) < 1:\r\n","    hidden_layers.append( 10 )\r\n","  else:\r\n","    hidden_layers.append( int(val) )\r\n","fc_layer_params = tuple(hidden_layers, )\r\n","\r\n","# Define RNA-Q\r\n","q_net = q_network.QNetwork(\r\n","    train_env.observation_spec(),\r\n","    train_env.action_spec(),\r\n","    fc_layer_params=fc_layer_params)\r\n","\r\n","optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\r\n","\r\n","train_step_counter = tf.Variable(0)\r\n","\r\n","# Define el agente de tipo Q\r\n","ag = dqn_agent.DqnAgent(\r\n","    train_env.time_step_spec(),\r\n","    train_env.action_spec(),\r\n","    q_network=q_net,\r\n","    optimizer=optimizer,\r\n","    td_errors_loss_fn=common.element_wise_squared_loss,\r\n","    train_step_counter=train_step_counter)\r\n","\r\n","ag.initialize()\r\n","\r\n","# define política para evaluación para el Agente\r\n","eval_policy = ag.policy\r\n","\r\n","# define política para recolección de datos para el Agente\r\n","collect_policy = ag.collect_policy\r\n","\r\n","print(\"Agente DQN inicializado. \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Agente DQN inicializado. \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dLCBLMD4Zsia"},"source":["3) Llevar a cabo el Entrenamiento:"]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1612791208421,"user_tz":180,"elapsed":6510,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"c34679f5-3e41-46fd-e408-937c40338c28"},"source":["#@title Definir Métricas para evaluación\r\n","\r\n","# Se usa el promedio de la recompensa (la más común)\r\n","# See also the metrics module for standard implementations of different metrics.\r\n","# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\r\n","\r\n","def compute_avg_return(environment, policy, num_episodes=10):\r\n","\r\n","  total_return = 0.0\r\n","  for _ in range(num_episodes):\r\n","\r\n","    time_step = environment.reset()\r\n","    episode_return = 0.0\r\n","    while not time_step.is_last():\r\n","      action_step = policy.action(time_step)\r\n","      time_step = environment.step(action_step.action)\r\n","      episode_return += time_step.reward\r\n","    total_return += episode_return\r\n","\r\n","  avg_return = total_return / num_episodes\r\n","  return avg_return.numpy()[0]\r\n","\r\n","print(\"Métricas definidas.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Métricas definidas.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EBRZGSkZ5N6","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612791210133,"user_tz":180,"elapsed":8168,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"50f1c794-5358-429a-b00a-ac24550bb911"},"source":["#@title Preparar datos para Entrenamiento\r\n","\r\n","initial_collect_steps =   100# @param {type:\"integer\"} \r\n","collect_steps_per_iteration = 10  # @param {type:\"integer\"}\r\n","replay_buffer_max_length = 100000  # @param {type:\"integer\"}\r\n","batch_size = 64  # @param {type:\"integer\"}\r\n","\r\n","\r\n","# Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\r\n","replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n","    data_spec = ag.collect_data_spec,\r\n","    batch_size = train_env.batch_size,\r\n","    max_length = replay_buffer_max_length)\r\n","\r\n","# Recolecta datos generados al azar\r\n","# This loop is so common in RL, that we provide standard implementations. \r\n","# For more details see the drivers module.\r\n","# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\r\n","\r\n","def collect_step(environment, policy, buffer):\r\n","  time_step = environment.current_time_step()\r\n","  action_step = policy.action(time_step)\r\n","  next_time_step = environment.step(action_step.action)\r\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n","\r\n","  # Add trajectory to the replay buffer\r\n","  buffer.add_batch(traj)\r\n","\r\n","def collect_data(env, policy, buffer, steps):\r\n","  for _ in range(steps):\r\n","    collect_step(env, policy, buffer)\r\n","\r\n","collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\r\n","\r\n","print(\"\\nDatos recolectados.\")\r\n","\r\n","# Muestra ejemplo de los datos recolectados\r\n","##iter(replay_buffer.as_dataset()).next()\r\n","\r\n","# Preparar los datos recolectados con trajectories de shape [Bx2x...]\r\n","dataset = replay_buffer.as_dataset(\r\n","    num_parallel_calls=3, \r\n","    sample_batch_size=batch_size, \r\n","    num_steps=2).prefetch(3)\r\n","iterator = iter(dataset)\r\n","# Muestra ejemplo \r\n","##iterator.next()\r\n","print(\"\\nDataset creado.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Datos recolectados.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n","\n","Dataset creado.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2k3S5IqGhK-a","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612795399605,"user_tz":180,"elapsed":4197595,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"5589fd9f-3d92-4d03-bdbf-c1b0aa3de8d6"},"source":["#@title Entrenar al Agente\r\n","\r\n","cant_ciclos_entrenamiento =  50000# @param {type:\"integer\"}\r\n","log_cada_ciclos = 500  # @param {type:\"integer\"}\r\n","mostar_recompensa_cada = 1000  # @param {type:\"integer\"}\r\n","cant_episodios_evaluacion =  25# @param {type:\"integer\"}\r\n","\r\n","#  Optimize by wrapping some of the code in a graph using TF function (Optional)\r\n","ag.train = common.function(ag.train)\r\n","\r\n","# Reset the train step\r\n","ag.train_step_counter.assign(0)\r\n","\r\n","# Evaluate the agent's policy once before training.\r\n","avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","ar_ciclo = []\r\n","ar_returns = []\r\n","ar_loss = []\r\n","\r\n","print(\"\\n** Comienza el Entrenamiento **\\n\")\r\n","for _ in range(cant_ciclos_entrenamiento):\r\n","\r\n","  # Collect a few steps using collect_policy and save to the replay buffer.\r\n","  collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n","\r\n","  # Sample a batch of data from the buffer and update the agent's network.\r\n","  experience, unused_info = next(iterator)\r\n","  train_loss = ag.train(experience).loss\r\n","\r\n","  step = ag.train_step_counter.numpy()\r\n","\r\n","  if (step == 1) or (step == cant_ciclos_entrenamiento) or (step % log_cada_ciclos == 0):\r\n","    print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \r\n","    ar_ciclo.append( step )\r\n","    ar_loss.append( train_loss )\r\n","    avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","    ar_returns.append( avg_return )\r\n","\r\n","    if (step == 1) or (step == cant_ciclos_entrenamiento) or (step % mostar_recompensa_cada == 0):\r\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\r\n","\r\n","print(\"\\n** Entrenamiento Finalizado **\\n\")\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","** Comienza el Entrenamiento **\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.foldr(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n","step = 1: loss = 561.176\n","step = 1: Promedio Recompensa = -104.2\n","step = 500: loss = 37.522\n","step = 1000: loss = 74.668\n","step = 1000: Promedio Recompensa = -50.1\n","step = 1500: loss = 118.469\n","step = 2000: loss = 125.413\n","step = 2000: Promedio Recompensa = -134.7\n","step = 2500: loss = 59.758\n","step = 3000: loss = 50.443\n","step = 3000: Promedio Recompensa = -61.3\n","step = 3500: loss = 157.769\n","step = 4000: loss = 226.962\n","step = 4000: Promedio Recompensa = -89.5\n","step = 4500: loss = 98.120\n","step = 5000: loss = 97.286\n","step = 5000: Promedio Recompensa = -38.1\n","step = 5500: loss = 92.209\n","step = 6000: loss = 250.842\n","step = 6000: Promedio Recompensa = -117.2\n","step = 6500: loss = 86.145\n","step = 7000: loss = 155.036\n","step = 7000: Promedio Recompensa = -182.9\n","step = 7500: loss = 180.695\n","step = 8000: loss = 94.583\n","step = 8000: Promedio Recompensa = -45.8\n","step = 8500: loss = 130.925\n","step = 9000: loss = 157.091\n","step = 9000: Promedio Recompensa = -2.9\n","step = 9500: loss = 175.803\n","step = 10000: loss = 165.488\n","step = 10000: Promedio Recompensa = -52.4\n","step = 10500: loss = 305.315\n","step = 11000: loss = 117.768\n","step = 11000: Promedio Recompensa = -7.7\n","step = 11500: loss = 113.501\n","step = 12000: loss = 190.195\n","step = 12000: Promedio Recompensa = -8.9\n","step = 12500: loss = 123.682\n","step = 13000: loss = 147.854\n","step = 13000: Promedio Recompensa = -29.3\n","step = 13500: loss = 119.638\n","step = 14000: loss = 34.950\n","step = 14000: Promedio Recompensa = -12.9\n","step = 14500: loss = 58.849\n","step = 15000: loss = 136.429\n","step = 15000: Promedio Recompensa = -9.0\n","step = 15500: loss = 52.462\n","step = 16000: loss = 101.149\n","step = 16000: Promedio Recompensa = -30.0\n","step = 16500: loss = 61.966\n","step = 17000: loss = 192.460\n","step = 17000: Promedio Recompensa = -39.8\n","step = 17500: loss = 173.856\n","step = 18000: loss = 46.462\n","step = 18000: Promedio Recompensa = -6.9\n","step = 18500: loss = 35.658\n","step = 19000: loss = 233.847\n","step = 19000: Promedio Recompensa = -2.0\n","step = 19500: loss = 106.710\n","step = 20000: loss = 321.001\n","step = 20000: Promedio Recompensa = -2.2\n","step = 20500: loss = 153.903\n","step = 21000: loss = 63.357\n","step = 21000: Promedio Recompensa = 0.9\n","step = 21500: loss = 86.538\n","step = 22000: loss = 298.934\n","step = 22000: Promedio Recompensa = 14.1\n","step = 22500: loss = 149.014\n","step = 23000: loss = 84.561\n","step = 23000: Promedio Recompensa = 10.9\n","step = 23500: loss = 107.058\n","step = 24000: loss = 48.630\n","step = 24000: Promedio Recompensa = -13.7\n","step = 24500: loss = 183.448\n","step = 25000: loss = 119.433\n","step = 25000: Promedio Recompensa = -14.3\n","step = 25500: loss = 175.321\n","step = 26000: loss = 100.315\n","step = 26000: Promedio Recompensa = -33.1\n","step = 26500: loss = 169.613\n","step = 27000: loss = 230.629\n","step = 27000: Promedio Recompensa = 14.4\n","step = 27500: loss = 196.030\n","step = 28000: loss = 53.413\n","step = 28000: Promedio Recompensa = 25.2\n","step = 28500: loss = 107.847\n","step = 29000: loss = 210.096\n","step = 29000: Promedio Recompensa = -10.6\n","step = 29500: loss = 162.110\n","step = 30000: loss = 69.540\n","step = 30000: Promedio Recompensa = -20.1\n","step = 30500: loss = 297.430\n","step = 31000: loss = 33.031\n","step = 31000: Promedio Recompensa = 49.9\n","step = 31500: loss = 162.454\n","step = 32000: loss = 100.863\n","step = 32000: Promedio Recompensa = -2.0\n","step = 32500: loss = 83.568\n","step = 33000: loss = 206.975\n","step = 33000: Promedio Recompensa = 31.4\n","step = 33500: loss = 146.547\n","step = 34000: loss = 476.147\n","step = 34000: Promedio Recompensa = 55.8\n","step = 34500: loss = 77.715\n","step = 35000: loss = 82.753\n","step = 35000: Promedio Recompensa = 26.1\n","step = 35500: loss = 69.011\n","step = 36000: loss = 101.716\n","step = 36000: Promedio Recompensa = 42.8\n","step = 36500: loss = 40.959\n","step = 37000: loss = 132.522\n","step = 37000: Promedio Recompensa = 68.4\n","step = 37500: loss = 140.034\n","step = 38000: loss = 151.277\n","step = 38000: Promedio Recompensa = 10.6\n","step = 38500: loss = 54.420\n","step = 39000: loss = 196.959\n","step = 39000: Promedio Recompensa = 51.1\n","step = 39500: loss = 152.531\n","step = 40000: loss = 358.924\n","step = 40000: Promedio Recompensa = 49.0\n","step = 40500: loss = 206.318\n","step = 41000: loss = 154.645\n","step = 41000: Promedio Recompensa = 23.0\n","step = 41500: loss = 94.687\n","step = 42000: loss = 111.428\n","step = 42000: Promedio Recompensa = 50.7\n","step = 42500: loss = 150.289\n","step = 43000: loss = 165.226\n","step = 43000: Promedio Recompensa = 45.3\n","step = 43500: loss = 87.199\n","step = 44000: loss = 116.353\n","step = 44000: Promedio Recompensa = 54.8\n","step = 44500: loss = 250.587\n","step = 45000: loss = 109.070\n","step = 45000: Promedio Recompensa = 72.3\n","step = 45500: loss = 286.398\n","step = 46000: loss = 231.807\n","step = 46000: Promedio Recompensa = 62.1\n","step = 46500: loss = 107.390\n","step = 47000: loss = 97.700\n","step = 47000: Promedio Recompensa = 40.8\n","step = 47500: loss = 102.317\n","step = 48000: loss = 143.948\n","step = 48000: Promedio Recompensa = 67.8\n","step = 48500: loss = 238.919\n","step = 49000: loss = 157.754\n","step = 49000: Promedio Recompensa = 56.4\n","step = 49500: loss = 50.754\n","step = 50000: loss = 152.748\n","step = 50000: Promedio Recompensa = 68.8\n","\n","** Entrenamiento Finalizado **\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EBBl7mRkQYa","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"status":"ok","timestamp":1612795400283,"user_tz":180,"elapsed":4198264,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"4498f6df-2950-4653-f1b4-72cb37c9dfe3"},"source":["#@title Mostrar Gráficos del Entrenamiento\r\n","\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","plt.plot( ar_ciclo, ar_returns)\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","#plt.plot( ar_ciclo, ar_returns)\r\n","plt.plot( ar_ciclo, ar_loss, color=\"red\" )\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Rjj7bRe7AB-l"},"source":["4) Cargar / Graba el modelo de las políticas entrenadas:"]},{"cell_type":"code","metadata":{"cellView":"form","id":"6V2EiqdwAy_R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612796559605,"user_tz":180,"elapsed":26790,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"7337d6b0-7964-4e18-91ec-9c58e2b65f2c"},"source":["#@title Cargar o Guardar el Modelo\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demoAgentes/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-OrdenarLista\" #@param {type:\"string\"}\n","accion_realizar = \"-\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","# determina lugar donde se guarda el modelo\n","policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","\n","if accion_realizar != \"-\":\n","  # Montar Drive\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  # guarda la politica del agente entrenado\n","  tf_policy_saver = policy_saver.PolicySaver(ag.policy)\n","  tf_policy_saver.save(policy_dir)\n","  print(\"\\nPolítica del modelo guardada en \", policy_dir)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  saved_policy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica del modelo recuperada de \", policy_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as QNetwork_layer_call_and_return_conditional_losses, QNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 35). These functions will not be directly callable after loading.\n","WARNING:absl:Found untraced functions such as QNetwork_layer_call_and_return_conditional_losses, QNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 35). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista/assets\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Política del modelo guardada en  /content/gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j79yPUetlUbs"},"source":["5) Probar entrenamiento comparando resultados:"]},{"cell_type":"code","metadata":{"id":"lLkdkcBjl3Xs","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612796604833,"user_tz":180,"elapsed":3760,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"a422755a-da46-4043-eb20-395d00705cb0"},"source":["#@title Probar el Agente Entrenado contra el Azar\r\n","cantidad_probar =  10# @param {type:\"integer\"}\r\n","promAzar = 0\r\n","promAgente = 0\r\n","\r\n","# determina política a usar\r\n","policy_agente_entrenado = None\r\n","if not('ag' in vars() or 'ag' in globals()) or ag is None:\r\n","  if not('saved_policy' in vars() or 'saved_policy' in globals()) or saved_policy is None:\r\n","    ValueError(\"No hay política entrenada definida.\")\r\n","  else:\r\n","    policy_agente_entrenado = saved_policy\r\n","    print(\"- Se usa la política recuperada del drive.\")\r\n","else:\r\n","  policy_agente_entrenado = ag.policy\r\n","  print(\"- Se usa la política del modelo entrenado.\")\r\n","\r\n","for i in range(cantidad_probar):\r\n","\r\n","  print(\"\\n> Prueba \", i+1, \":\")\r\n","\r\n","  # crea nuevo entorno que mantiene la misma lista\r\n","  prueba_env =  tf_py_environment.TFPyEnvironment( OrdenarListasEnv(False) )\r\n","\r\n","  # Probar Aleatorio\r\n","  valorAzar = SimularEntorno(prueba_env, random_policy, \"Resultados Aleatorio\", False) \r\n","  promAzar = promAzar + valorAzar\r\n","\r\n","  # Probar Agente Entrenado\r\n","  valorAgente = SimularEntorno(prueba_env, policy_agente_entrenado, \"Resultados de Agente Entrenado\", False) \r\n","  promAgente = promAgente + valorAgente\r\n","\r\n","  # Decide Ganador\r\n","  if valorAzar < valorAgente:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera MEJOR resultado que el azar (\", valorAzar,\")\")\r\n","  else:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera PEOR resultado que el azar (\", valorAzar,\")\")\r\n","\r\n","# Decide Ganador General\r\n","if cantidad_probar > 0:\r\n","  promAgente = promAgente / cantidad_probar\r\n","  promAzar = promAzar / cantidad_probar\r\n","  print(\"\\n================================================================================================\\n\")\r\n","  if promAzar < promAgente:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene MEJORES resultado que  el azar (\", promAzar,\")\")\r\n","  else:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene PEORES resultados que el azar (\", promAzar,\")\")\r\n","  print(\"\\n================================================================================================\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["- Se usa la política del modelo entrenado.\n","\n","> Prueba  1 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [-88  51  12 -52 -50]\n","  # 1 : acción intercambiar(2,4) -> Estado/Reward  -4.0\n","  # 2 : acción intercambiar(4,0) -> Estado/Reward  -9.0\n","  # 3 : acción intercambiar(6,7) -> Estado/Reward  -9.0\n","  # 4 : acción mover(6,9) -> Estado/Reward  -9.0\n","  # 5 : acción mover(7,2) -> Estado/Reward  -7.0\n","  # 6 : acción mover(4,5) -> Estado/Reward  -7.0\n","  # 7 : acción intercambiar(1,2) -> Estado/Reward  -6.0\n","  # 8 : acción mover(7,3) -> Estado/Reward  -5.0\n","  # 9 : acción mover(0,1) -> Estado/Reward  -4.0\n","  # 10 : acción mover(9,5) -> Estado/Reward  -4.0\n","  # 11 : acción mover(1,8) -> Estado/Reward  -3.0\n","  # 12 : acción intercambiar(4,3) -> Estado/Reward  -4.0\n","  # 13 : acción intercambiar(5,8) -> Estado/Reward  -4.0\n","  # 14 : acción mover(4,9) -> Estado/Reward  -4.0\n","  # 15 : acción intercambiar(8,1) -> Estado/Reward  -1.0\n","  # 16 : acción mover(1,5) -> Estado/Reward  -2.0\n","  # 17 : acción mover(1,8) -> Estado/Reward  -5.0\n","  # 18 : acción intercambiar(5,1) -> Estado/Reward  -2.0\n","  # 19 : acción mover(2,0) -> Estado/Reward  -4.0\n","  # 20 : acción mover(0,7) -> Estado/Reward  80.0\n"," Recompensa Final =  80.0\n"," Lista Final =  [-88 -52 -50  12  51]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [-88  51  12 -52 -50]\n","  # 1 : acción intercambiar(1,3) -> Estado/Reward  -2.0\n","  # 2 : acción mover(4,2) -> Estado/Reward  98.0\n"," Recompensa Final =  98.0\n"," Lista Final =  [-88 -52 -50  12  51]\n","\n","--> El Agente Entrenado ( 98.0 ) genera MEJOR resultado que el azar ( 80.0 )\n","\n","> Prueba  2 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [-34  26  72]\n","  # 1 : acción mover(7,8) -> Estado/Reward  99.0\n"," Recompensa Final =  99.0\n"," Lista Final =  [-34  26  72]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [-34  26  72]\n","  # 1 : acción intercambiar(5,8) -> Estado/Reward  99.0\n"," Recompensa Final =  99.0\n"," Lista Final =  [-34  26  72]\n","\n","--> El Agente Entrenado ( 99.0 ) genera PEOR resultado que el azar ( 99.0 )\n","\n","> Prueba  3 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 28  29 -52 -10  -9 -81  88]\n","  # 1 : acción mover(7,7) -> Estado/Reward  -11.0\n","  # 2 : acción intercambiar(3,1) -> Estado/Reward  -10.0\n","  # 3 : acción mover(1,0) -> Estado/Reward  -9.0\n","  # 4 : acción intercambiar(6,5) -> Estado/Reward  -10.0\n","  # 5 : acción intercambiar(5,9) -> Estado/Reward  -9.0\n","  # 6 : acción intercambiar(4,7) -> Estado/Reward  -10.0\n","  # 7 : acción mover(6,9) -> Estado/Reward  -10.0\n","  # 8 : acción intercambiar(4,3) -> Estado/Reward  -11.0\n","  # 9 : acción intercambiar(4,5) -> Estado/Reward  -10.0\n","  # 10 : acción mover(8,9) -> Estado/Reward  -10.0\n","  # 11 : acción mover(8,6) -> Estado/Reward  -10.0\n","  # 12 : acción intercambiar(2,5) -> Estado/Reward  -11.0\n","  # 13 : acción mover(9,5) -> Estado/Reward  -12.0\n","  # 14 : acción intercambiar(6,4) -> Estado/Reward  -13.0\n","  # 15 : acción mover(6,3) -> Estado/Reward  -10.0\n","  # 16 : acción intercambiar(1,9) -> Estado/Reward  -9.0\n","  # 17 : acción mover(5,3) -> Estado/Reward  -9.0\n","  # 18 : acción intercambiar(8,6) -> Estado/Reward  -9.0\n","  # 19 : acción mover(0,2) -> Estado/Reward  -11.0\n","  # 20 : acción intercambiar(7,3) -> Estado/Reward  -12.0\n","  # 21 : acción intercambiar(7,9) -> Estado/Reward  -12.0\n","  # 22 : acción intercambiar(1,2) -> Estado/Reward  -11.0\n","  # 23 : acción mover(4,8) -> Estado/Reward  -13.0\n","  # 24 : acción intercambiar(8,7) -> Estado/Reward  -13.0\n","  # 25 : acción mover(5,6) -> Estado/Reward  -12.0\n","  # 26 : acción mover(3,3) -> Estado/Reward  -12.0\n","  # 27 : acción intercambiar(2,1) -> Estado/Reward  -13.0\n","  # 28 : acción mover(7,0) -> Estado/Reward  -9.0\n","  # 29 : acción mover(9,5) -> Estado/Reward  -8.0\n","  # 30 : acción mover(3,6) -> Estado/Reward  -9.0\n","  # 31 : acción intercambiar(3,6) -> Estado/Reward  -8.0\n","  # 32 : acción mover(9,8) -> Estado/Reward  -8.0\n","  # 33 : acción intercambiar(9,0) -> Estado/Reward  -13.0\n","  # 34 : acción intercambiar(0,9) -> Estado/Reward  -8.0\n","  # 35 : acción mover(1,5) -> Estado/Reward  -8.0\n","  # 36 : acción intercambiar(6,2) -> Estado/Reward  -11.0\n","  # 37 : acción intercambiar(6,7) -> Estado/Reward  -11.0\n","  # 38 : acción mover(1,5) -> Estado/Reward  -9.0\n","  # 39 : acción mover(5,6) -> Estado/Reward  -8.0\n","  # 40 : acción mover(1,9) -> Estado/Reward  -7.0\n","  # 41 : acción intercambiar(9,7) -> Estado/Reward  -7.0\n","  # 42 : acción intercambiar(4,6) -> Estado/Reward  -8.0\n","  # 43 : acción mover(1,3) -> Estado/Reward  -10.0\n","  # 44 : acción intercambiar(8,6) -> Estado/Reward  -10.0\n","  # 45 : acción mover(3,4) -> Estado/Reward  -11.0\n","  # 46 : acción mover(6,6) -> Estado/Reward  -11.0\n","  # 47 : acción intercambiar(0,7) -> Estado/Reward  -12.0\n","  # 48 : acción mover(7,4) -> Estado/Reward  -12.0\n","  # 49 : acción mover(2,4) -> Estado/Reward  -12.0\n","  # 50 : acción intercambiar(3,7) -> Estado/Reward  -15.0\n","  # 51 : acción mover(5,3) -> Estado/Reward  -13.0\n","  # 52 : acción mover(7,1) -> Estado/Reward  -10.0\n","  # 53 : acción mover(4,9) -> Estado/Reward  -12.0\n","  # 54 : acción intercambiar(5,3) -> Estado/Reward  -11.0\n","  # 55 : acción mover(0,0) -> Estado/Reward  -11.0\n","  # 56 : acción mover(5,6) -> Estado/Reward  -10.0\n","  # 57 : acción mover(2,3) -> Estado/Reward  -9.0\n","  # 58 : acción mover(1,1) -> Estado/Reward  -9.0\n","  # 59 : acción intercambiar(2,5) -> Estado/Reward  -8.0\n","  # 60 : acción intercambiar(9,4) -> Estado/Reward  -7.0\n","  # 61 : acción intercambiar(5,3) -> Estado/Reward  -4.0\n","  # 62 : acción mover(5,3) -> Estado/Reward  -6.0\n","  # 63 : acción mover(6,1) -> Estado/Reward  -9.0\n","  # 64 : acción intercambiar(2,7) -> Estado/Reward  -12.0\n","  # 65 : acción mover(5,8) -> Estado/Reward  -11.0\n","  # 66 : acción intercambiar(4,8) -> Estado/Reward  -10.0\n","  # 67 : acción intercambiar(6,4) -> Estado/Reward  -11.0\n","  # 68 : acción intercambiar(1,5) -> Estado/Reward  -8.0\n","  # 69 : acción intercambiar(0,1) -> Estado/Reward  -7.0\n","  # 70 : acción intercambiar(6,1) -> Estado/Reward  -8.0\n","  # 71 : acción mover(7,1) -> Estado/Reward  -5.0\n","  # 72 : acción mover(7,6) -> Estado/Reward  -5.0\n","  # 73 : acción mover(6,9) -> Estado/Reward  -5.0\n","  # 74 : acción mover(2,1) -> Estado/Reward  -6.0\n","  # 75 : acción mover(6,6) -> Estado/Reward  -6.0\n","  # 76 : acción intercambiar(7,8) -> Estado/Reward  -6.0\n","  # 77 : acción mover(1,1) -> Estado/Reward  -6.0\n","  # 78 : acción mover(9,3) -> Estado/Reward  -7.0\n","  # 79 : acción intercambiar(8,2) -> Estado/Reward  -12.0\n","  # 80 : acción intercambiar(4,0) -> Estado/Reward  -15.0\n","  # 81 : acción mover(2,3) -> Estado/Reward  -14.0\n","  # 82 : acción intercambiar(8,5) -> Estado/Reward  -15.0\n","  # 83 : acción mover(3,0) -> Estado/Reward  -18.0\n","  # 84 : acción intercambiar(2,3) -> Estado/Reward  -19.0\n","  # 85 : acción intercambiar(8,1) -> Estado/Reward  -12.0\n","  # 86 : acción mover(8,8) -> Estado/Reward  -12.0\n","  # 87 : acción mover(0,9) -> Estado/Reward  -6.0\n","  # 88 : acción intercambiar(1,1) -> Estado/Reward  -6.0\n","  # 89 : acción mover(3,0) -> Estado/Reward  -5.0\n","  # 90 : acción intercambiar(6,8) -> Estado/Reward  -5.0\n","  # 91 : acción intercambiar(6,8) -> Estado/Reward  -5.0\n","  # 92 : acción mover(8,8) -> Estado/Reward  -5.0\n","  # 93 : acción intercambiar(5,9) -> Estado/Reward  -6.0\n","  # 94 : acción mover(8,3) -> Estado/Reward  -7.0\n","  # 95 : acción intercambiar(7,9) -> Estado/Reward  -7.0\n","  # 96 : acción mover(7,4) -> Estado/Reward  -9.0\n","  # 97 : acción mover(8,7) -> Estado/Reward  -9.0\n","  # 98 : acción intercambiar(1,5) -> Estado/Reward  -10.0\n","  # 99 : acción mover(5,3) -> Estado/Reward  -8.0\n","  # 100 : acción mover(6,9) -> Estado/Reward  -8.0\n"," Recompensa Final =  -8.0\n"," Lista Final =  [-52  -9  29 -81  28  88 -10]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 28  29 -52 -10  -9 -81  88]\n","  # 1 : acción intercambiar(1,5) -> Estado/Reward  -4.0\n","  # 2 : acción mover(0,3) -> Estado/Reward  -1.0\n","  # 3 : acción mover(3,4) -> Estado/Reward  97.0\n"," Recompensa Final =  97.0\n"," Lista Final =  [-81 -52 -10  -9  28  29  88]\n","\n","--> El Agente Entrenado ( 97.0 ) genera MEJOR resultado que el azar ( -8.0 )\n","\n","> Prueba  4 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [82  3 37]\n","  # 1 : acción mover(0,0) -> Estado/Reward  -2.0\n","  # 2 : acción mover(0,4) -> Estado/Reward  98.0\n"," Recompensa Final =  98.0\n"," Lista Final =  [ 3 37 82]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [82  3 37]\n","  # 1 : acción mover(0,2) -> Estado/Reward  99.0\n"," Recompensa Final =  99.0\n"," Lista Final =  [ 3 37 82]\n","\n","--> El Agente Entrenado ( 99.0 ) genera MEJOR resultado que el azar ( 98.0 )\n","\n","> Prueba  5 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 48  29 -94  -7  49]\n","  # 1 : acción intercambiar(5,2) -> Estado/Reward  -8.0\n","  # 2 : acción mover(2,3) -> Estado/Reward  -7.0\n","  # 3 : acción mover(4,0) -> Estado/Reward  -3.0\n","  # 4 : acción intercambiar(6,2) -> Estado/Reward  -4.0\n","  # 5 : acción mover(8,4) -> Estado/Reward  -4.0\n","  # 6 : acción intercambiar(4,3) -> Estado/Reward  -5.0\n","  # 7 : acción intercambiar(7,8) -> Estado/Reward  -5.0\n","  # 8 : acción mover(8,9) -> Estado/Reward  -5.0\n","  # 9 : acción intercambiar(9,6) -> Estado/Reward  -5.0\n","  # 10 : acción intercambiar(7,1) -> Estado/Reward  -2.0\n","  # 11 : acción intercambiar(5,4) -> Estado/Reward  -2.0\n","  # 12 : acción mover(4,7) -> Estado/Reward  -2.0\n","  # 13 : acción mover(4,1) -> Estado/Reward  -3.0\n","  # 14 : acción mover(6,1) -> Estado/Reward  -2.0\n","  # 15 : acción intercambiar(1,5) -> Estado/Reward  -5.0\n","  # 16 : acción mover(9,8) -> Estado/Reward  -5.0\n","  # 17 : acción mover(0,1) -> Estado/Reward  -6.0\n","  # 18 : acción intercambiar(2,1) -> Estado/Reward  -7.0\n","  # 19 : acción intercambiar(7,4) -> Estado/Reward  -7.0\n","  # 20 : acción mover(8,7) -> Estado/Reward  -7.0\n","  # 21 : acción mover(8,5) -> Estado/Reward  -7.0\n","  # 22 : acción mover(9,2) -> Estado/Reward  -9.0\n","  # 23 : acción mover(5,0) -> Estado/Reward  -7.0\n","  # 24 : acción mover(4,4) -> Estado/Reward  -7.0\n","  # 25 : acción mover(6,5) -> Estado/Reward  -7.0\n","  # 26 : acción mover(2,8) -> Estado/Reward  -5.0\n","  # 27 : acción intercambiar(9,2) -> Estado/Reward  -6.0\n","  # 28 : acción mover(6,6) -> Estado/Reward  -6.0\n","  # 29 : acción mover(6,8) -> Estado/Reward  -6.0\n","  # 30 : acción mover(8,7) -> Estado/Reward  -6.0\n","  # 31 : acción intercambiar(3,9) -> Estado/Reward  -7.0\n","  # 32 : acción mover(3,6) -> Estado/Reward  -6.0\n","  # 33 : acción intercambiar(8,8) -> Estado/Reward  -6.0\n","  # 34 : acción mover(6,7) -> Estado/Reward  -6.0\n","  # 35 : acción intercambiar(4,4) -> Estado/Reward  -6.0\n","  # 36 : acción intercambiar(1,4) -> Estado/Reward  -3.0\n","  # 37 : acción intercambiar(5,7) -> Estado/Reward  -3.0\n","  # 38 : acción mover(4,7) -> Estado/Reward  -3.0\n","  # 39 : acción mover(5,3) -> Estado/Reward  -4.0\n","  # 40 : acción mover(2,6) -> Estado/Reward  -4.0\n","  # 41 : acción intercambiar(0,6) -> Estado/Reward  -7.0\n","  # 42 : acción mover(2,5) -> Estado/Reward  -5.0\n","  # 43 : acción intercambiar(3,2) -> Estado/Reward  -6.0\n","  # 44 : acción mover(6,0) -> Estado/Reward  -10.0\n","  # 45 : acción mover(1,8) -> Estado/Reward  -7.0\n","  # 46 : acción mover(4,2) -> Estado/Reward  -9.0\n","  # 47 : acción mover(3,6) -> Estado/Reward  -8.0\n","  # 48 : acción mover(8,4) -> Estado/Reward  -8.0\n","  # 49 : acción intercambiar(9,5) -> Estado/Reward  -8.0\n","  # 50 : acción intercambiar(9,5) -> Estado/Reward  -8.0\n","  # 51 : acción intercambiar(4,0) -> Estado/Reward  -3.0\n","  # 52 : acción mover(7,6) -> Estado/Reward  -3.0\n","  # 53 : acción mover(5,4) -> Estado/Reward  -3.0\n","  # 54 : acción intercambiar(3,2) -> Estado/Reward  -2.0\n","  # 55 : acción intercambiar(0,5) -> Estado/Reward  -7.0\n","  # 56 : acción mover(3,9) -> Estado/Reward  -6.0\n","  # 57 : acción mover(6,8) -> Estado/Reward  -6.0\n","  # 58 : acción mover(4,6) -> Estado/Reward  -6.0\n","  # 59 : acción mover(5,0) -> Estado/Reward  -8.0\n","  # 60 : acción intercambiar(2,9) -> Estado/Reward  -7.0\n","  # 61 : acción intercambiar(7,9) -> Estado/Reward  -7.0\n","  # 62 : acción intercambiar(2,3) -> Estado/Reward  -6.0\n","  # 63 : acción intercambiar(7,1) -> Estado/Reward  -5.0\n","  # 64 : acción intercambiar(9,6) -> Estado/Reward  -5.0\n","  # 65 : acción mover(5,9) -> Estado/Reward  -5.0\n","  # 66 : acción intercambiar(3,1) -> Estado/Reward  -4.0\n","  # 67 : acción intercambiar(0,5) -> Estado/Reward  -5.0\n","  # 68 : acción mover(9,3) -> Estado/Reward  -6.0\n","  # 69 : acción intercambiar(8,3) -> Estado/Reward  -5.0\n","  # 70 : acción intercambiar(8,6) -> Estado/Reward  -5.0\n","  # 71 : acción mover(4,8) -> Estado/Reward  -5.0\n","  # 72 : acción intercambiar(7,8) -> Estado/Reward  -5.0\n","  # 73 : acción mover(5,7) -> Estado/Reward  -5.0\n","  # 74 : acción intercambiar(8,1) -> Estado/Reward  -8.0\n","  # 75 : acción intercambiar(0,8) -> Estado/Reward  -3.0\n","  # 76 : acción mover(6,6) -> Estado/Reward  -3.0\n","  # 77 : acción intercambiar(7,1) -> Estado/Reward  -4.0\n","  # 78 : acción mover(8,4) -> Estado/Reward  -4.0\n","  # 79 : acción mover(0,3) -> Estado/Reward  -5.0\n","  # 80 : acción intercambiar(6,5) -> Estado/Reward  -5.0\n","  # 81 : acción intercambiar(1,3) -> Estado/Reward  -6.0\n","  # 82 : acción mover(7,5) -> Estado/Reward  -6.0\n","  # 83 : acción intercambiar(5,1) -> Estado/Reward  -9.0\n","  # 84 : acción mover(5,9) -> Estado/Reward  -9.0\n","  # 85 : acción intercambiar(7,0) -> Estado/Reward  -4.0\n","  # 86 : acción mover(6,4) -> Estado/Reward  -4.0\n","  # 87 : acción mover(7,1) -> Estado/Reward  -7.0\n","  # 88 : acción mover(1,2) -> Estado/Reward  -6.0\n","  # 89 : acción mover(8,9) -> Estado/Reward  -6.0\n","  # 90 : acción mover(3,2) -> Estado/Reward  -5.0\n","  # 91 : acción intercambiar(6,1) -> Estado/Reward  -2.0\n","  # 92 : acción mover(7,8) -> Estado/Reward  -2.0\n","  # 93 : acción mover(5,7) -> Estado/Reward  -2.0\n","  # 94 : acción intercambiar(5,3) -> Estado/Reward  -1.0\n","  # 95 : acción mover(5,6) -> Estado/Reward  -1.0\n","  # 96 : acción intercambiar(7,7) -> Estado/Reward  -1.0\n","  # 97 : acción mover(5,5) -> Estado/Reward  -1.0\n","  # 98 : acción mover(0,3) -> Estado/Reward  -2.0\n","  # 99 : acción mover(7,2) -> Estado/Reward  -4.0\n","  # 100 : acción mover(9,1) -> Estado/Reward  -1.0\n"," Recompensa Final =  -1.0\n"," Lista Final =  [-94  -7  29  49  48]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 48  29 -94  -7  49]\n","  # 1 : acción mover(0,3) -> Estado/Reward  -2.0\n","  # 2 : acción mover(0,2) -> Estado/Reward  98.0\n"," Recompensa Final =  98.0\n"," Lista Final =  [-94  -7  29  48  49]\n","\n","--> El Agente Entrenado ( 98.0 ) genera MEJOR resultado que el azar ( -1.0 )\n","\n","> Prueba  6 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [-79  47  83  84  84 -79 -98 -55 -84 -96]\n","  # 1 : acción intercambiar(5,8) -> Estado/Reward  -28.0\n","  # 2 : acción mover(5,7) -> Estado/Reward  -28.0\n","  # 3 : acción mover(2,0) -> Estado/Reward  -30.0\n","  # 4 : acción mover(2,5) -> Estado/Reward  -31.0\n","  # 5 : acción intercambiar(8,9) -> Estado/Reward  -30.0\n","  # 6 : acción mover(7,6) -> Estado/Reward  -29.0\n","  # 7 : acción intercambiar(8,4) -> Estado/Reward  -30.0\n","  # 8 : acción intercambiar(1,9) -> Estado/Reward  -30.0\n","  # 9 : acción mover(2,8) -> Estado/Reward  -25.0\n","  # 10 : acción mover(0,4) -> Estado/Reward  -23.0\n","  # 11 : acción intercambiar(1,2) -> Estado/Reward  -22.0\n","  # 12 : acción mover(4,2) -> Estado/Reward  -22.0\n","  # 13 : acción mover(1,2) -> Estado/Reward  -23.0\n","  # 14 : acción mover(3,3) -> Estado/Reward  -23.0\n","  # 15 : acción mover(4,2) -> Estado/Reward  -23.0\n","  # 16 : acción intercambiar(3,6) -> Estado/Reward  -26.0\n","  # 17 : acción mover(8,3) -> Estado/Reward  -30.0\n","  # 18 : acción mover(4,0) -> Estado/Reward  -28.0\n","  # 19 : acción mover(5,2) -> Estado/Reward  -30.0\n","  # 20 : acción mover(6,5) -> Estado/Reward  -29.0\n","  # 21 : acción intercambiar(5,0) -> Estado/Reward  -26.0\n","  # 22 : acción intercambiar(6,5) -> Estado/Reward  -27.0\n","  # 23 : acción mover(9,5) -> Estado/Reward  -27.0\n","  # 24 : acción intercambiar(1,5) -> Estado/Reward  -27.0\n","  # 25 : acción intercambiar(9,9) -> Estado/Reward  -27.0\n","  # 26 : acción mover(8,9) -> Estado/Reward  -26.0\n","  # 27 : acción mover(0,8) -> Estado/Reward  -32.0\n","  # 28 : acción mover(5,4) -> Estado/Reward  -33.0\n","  # 29 : acción intercambiar(2,4) -> Estado/Reward  -34.0\n","  # 30 : acción mover(7,6) -> Estado/Reward  -33.0\n","  # 31 : acción intercambiar(5,9) -> Estado/Reward  -30.0\n","  # 32 : acción intercambiar(1,3) -> Estado/Reward  -28.0\n","  # 33 : acción mover(8,2) -> Estado/Reward  -26.0\n","  # 34 : acción mover(2,5) -> Estado/Reward  -29.0\n","  # 35 : acción intercambiar(6,6) -> Estado/Reward  -29.0\n","  # 36 : acción mover(6,5) -> Estado/Reward  -28.0\n","  # 37 : acción intercambiar(2,8) -> Estado/Reward  -24.0\n","  # 38 : acción intercambiar(4,8) -> Estado/Reward  -25.0\n","  # 39 : acción intercambiar(7,7) -> Estado/Reward  -25.0\n","  # 40 : acción intercambiar(2,5) -> Estado/Reward  -24.0\n","  # 41 : acción intercambiar(1,6) -> Estado/Reward  -21.0\n","  # 42 : acción mover(9,9) -> Estado/Reward  -21.0\n","  # 43 : acción intercambiar(3,4) -> Estado/Reward  -21.0\n","  # 44 : acción mover(1,4) -> Estado/Reward  -22.0\n","  # 45 : acción mover(8,7) -> Estado/Reward  -23.0\n","  # 46 : acción mover(1,1) -> Estado/Reward  -23.0\n","  # 47 : acción intercambiar(2,4) -> Estado/Reward  -21.0\n","  # 48 : acción mover(7,4) -> Estado/Reward  -22.0\n","  # 49 : acción mover(6,9) -> Estado/Reward  -21.0\n","  # 50 : acción mover(8,9) -> Estado/Reward  -22.0\n","  # 51 : acción mover(2,6) -> Estado/Reward  -26.0\n","  # 52 : acción intercambiar(9,2) -> Estado/Reward  -18.0\n","  # 53 : acción mover(9,8) -> Estado/Reward  -19.0\n","  # 54 : acción intercambiar(6,4) -> Estado/Reward  -16.0\n","  # 55 : acción intercambiar(8,3) -> Estado/Reward  -18.0\n","  # 56 : acción intercambiar(1,8) -> Estado/Reward  -25.0\n","  # 57 : acción intercambiar(4,3) -> Estado/Reward  -24.0\n","  # 58 : acción mover(1,5) -> Estado/Reward  -22.0\n","  # 59 : acción mover(7,0) -> Estado/Reward  -15.0\n","  # 60 : acción mover(7,2) -> Estado/Reward  -19.0\n","  # 61 : acción mover(1,9) -> Estado/Reward  -22.0\n","  # 62 : acción mover(3,9) -> Estado/Reward  -26.0\n","  # 63 : acción intercambiar(2,2) -> Estado/Reward  -26.0\n","  # 64 : acción intercambiar(9,5) -> Estado/Reward  -21.0\n","  # 65 : acción intercambiar(8,9) -> Estado/Reward  -22.0\n","  # 66 : acción intercambiar(4,7) -> Estado/Reward  -21.0\n","  # 67 : acción mover(4,3) -> Estado/Reward  -20.0\n","  # 68 : acción intercambiar(8,4) -> Estado/Reward  -19.0\n","  # 69 : acción mover(2,2) -> Estado/Reward  -19.0\n","  # 70 : acción mover(4,9) -> Estado/Reward  -16.0\n","  # 71 : acción intercambiar(3,7) -> Estado/Reward  -19.0\n","  # 72 : acción intercambiar(3,2) -> Estado/Reward  -20.0\n","  # 73 : acción intercambiar(6,5) -> Estado/Reward  -21.0\n","  # 74 : acción mover(9,5) -> Estado/Reward  -25.0\n","  # 75 : acción mover(1,3) -> Estado/Reward  -24.0\n","  # 76 : acción intercambiar(2,4) -> Estado/Reward  -23.0\n","  # 77 : acción mover(7,5) -> Estado/Reward  -21.0\n","  # 78 : acción intercambiar(6,0) -> Estado/Reward  -28.0\n","  # 79 : acción intercambiar(0,6) -> Estado/Reward  -21.0\n","  # 80 : acción intercambiar(1,1) -> Estado/Reward  -21.0\n","  # 81 : acción mover(4,8) -> Estado/Reward  -23.0\n","  # 82 : acción intercambiar(9,2) -> Estado/Reward  -25.0\n","  # 83 : acción mover(8,1) -> Estado/Reward  -21.0\n","  # 84 : acción intercambiar(2,8) -> Estado/Reward  -15.0\n","  # 85 : acción mover(0,9) -> Estado/Reward  -24.0\n","  # 86 : acción mover(7,3) -> Estado/Reward  -27.0\n","  # 87 : acción mover(4,9) -> Estado/Reward  -22.0\n","  # 88 : acción intercambiar(7,7) -> Estado/Reward  -22.0\n","  # 89 : acción intercambiar(0,6) -> Estado/Reward  -26.0\n","  # 90 : acción mover(6,6) -> Estado/Reward  -26.0\n","  # 91 : acción intercambiar(7,3) -> Estado/Reward  -21.0\n","  # 92 : acción intercambiar(1,2) -> Estado/Reward  -20.0\n","  # 93 : acción mover(0,0) -> Estado/Reward  -20.0\n","  # 94 : acción mover(7,2) -> Estado/Reward  -25.0\n","  # 95 : acción mover(9,6) -> Estado/Reward  -28.0\n","  # 96 : acción intercambiar(0,5) -> Estado/Reward  -21.0\n","  # 97 : acción mover(3,5) -> Estado/Reward  -21.0\n","  # 98 : acción intercambiar(9,4) -> Estado/Reward  -16.0\n","  # 99 : acción mover(7,8) -> Estado/Reward  -15.0\n","  # 100 : acción mover(0,0) -> Estado/Reward  -15.0\n"," Recompensa Final =  -15.0\n"," Lista Final =  [-96 -79  84 -84 -98 -55  84 -79  83  47]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [-79  47  83  84  84 -79 -98 -55 -84 -96]\n","  # 1 : acción intercambiar(6,2) -> Estado/Reward  -26.0\n","  # 2 : acción intercambiar(9,1) -> Estado/Reward  -19.0\n","  # 3 : acción intercambiar(3,8) -> Estado/Reward  -11.0\n","  # 4 : acción intercambiar(4,7) -> Estado/Reward  -8.0\n","  # 5 : acción mover(5,0) -> Estado/Reward  -10.0\n","  # 6 : acción mover(9,8) -> Estado/Reward  -9.0\n","  # 7 : acción intercambiar(2,0) -> Estado/Reward  -7.0\n","  # 8 : acción mover(9,8) -> Estado/Reward  -8.0\n","  # 9 : acción mover(9,7) -> Estado/Reward  -6.0\n","  # 10 : acción mover(9,8) -> Estado/Reward  -6.0\n","  # 11 : acción mover(9,8) -> Estado/Reward  -6.0\n","  # 12 : acción mover(9,8) -> Estado/Reward  -6.0\n","  # 13 : acción mover(9,8) -> Estado/Reward  -6.0\n","  # 14 : acción mover(9,8) -> Estado/Reward  -6.0\n"," Recompensa Final =  -6.0\n"," Lista Final =  [-96 -79 -79 -98 -84 -55  83  47  84  84]\n","\n","--> El Agente Entrenado ( -6.0 ) genera MEJOR resultado que el azar ( -15.0 )\n","\n","> Prueba  7 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [  8 -11 -76  58  33]\n","  # 1 : acción mover(6,6) -> Estado/Reward  -4.0\n","  # 2 : acción intercambiar(2,9) -> Estado/Reward  -5.0\n","  # 3 : acción intercambiar(5,9) -> Estado/Reward  -5.0\n","  # 4 : acción mover(4,9) -> Estado/Reward  -5.0\n","  # 5 : acción intercambiar(9,8) -> Estado/Reward  -5.0\n","  # 6 : acción mover(3,3) -> Estado/Reward  -5.0\n","  # 7 : acción intercambiar(2,0) -> Estado/Reward  -6.0\n","  # 8 : acción intercambiar(9,6) -> Estado/Reward  -6.0\n","  # 9 : acción mover(8,2) -> Estado/Reward  -4.0\n","  # 10 : acción mover(6,7) -> Estado/Reward  -4.0\n","  # 11 : acción mover(3,2) -> Estado/Reward  -5.0\n","  # 12 : acción mover(5,5) -> Estado/Reward  -5.0\n","  # 13 : acción mover(6,5) -> Estado/Reward  -5.0\n","  # 14 : acción mover(9,3) -> Estado/Reward  -6.0\n","  # 15 : acción intercambiar(1,2) -> Estado/Reward  -7.0\n","  # 16 : acción intercambiar(7,0) -> Estado/Reward  -2.0\n","  # 17 : acción intercambiar(7,3) -> Estado/Reward  -1.0\n","  # 18 : acción mover(1,3) -> Estado/Reward  -1.0\n","  # 19 : acción mover(6,4) -> Estado/Reward  -1.0\n","  # 20 : acción mover(6,7) -> Estado/Reward  -1.0\n","  # 21 : acción mover(6,7) -> Estado/Reward  -1.0\n","  # 22 : acción mover(3,1) -> Estado/Reward  -1.0\n","  # 23 : acción mover(7,5) -> Estado/Reward  -1.0\n","  # 24 : acción mover(9,6) -> Estado/Reward  -1.0\n","  # 25 : acción mover(5,8) -> Estado/Reward  -1.0\n","  # 26 : acción mover(8,3) -> Estado/Reward  -2.0\n","  # 27 : acción intercambiar(4,3) -> Estado/Reward  -1.0\n","  # 28 : acción mover(3,7) -> Estado/Reward  -2.0\n","  # 29 : acción intercambiar(2,1) -> Estado/Reward  -1.0\n","  # 30 : acción mover(7,4) -> Estado/Reward  -1.0\n","  # 31 : acción mover(4,0) -> Estado/Reward  -3.0\n","  # 32 : acción intercambiar(7,2) -> Estado/Reward  -6.0\n","  # 33 : acción intercambiar(8,5) -> Estado/Reward  -6.0\n","  # 34 : acción mover(6,4) -> Estado/Reward  -6.0\n","  # 35 : acción intercambiar(6,1) -> Estado/Reward  -7.0\n","  # 36 : acción intercambiar(2,8) -> Estado/Reward  -4.0\n","  # 37 : acción intercambiar(6,8) -> Estado/Reward  -4.0\n","  # 38 : acción intercambiar(5,7) -> Estado/Reward  -4.0\n","  # 39 : acción mover(0,5) -> Estado/Reward  -2.0\n","  # 40 : acción intercambiar(6,7) -> Estado/Reward  -2.0\n","  # 41 : acción intercambiar(4,0) -> Estado/Reward  -5.0\n","  # 42 : acción intercambiar(6,1) -> Estado/Reward  -6.0\n","  # 43 : acción mover(2,2) -> Estado/Reward  -6.0\n","  # 44 : acción intercambiar(6,6) -> Estado/Reward  -6.0\n","  # 45 : acción intercambiar(0,6) -> Estado/Reward  -1.0\n","  # 46 : acción intercambiar(7,1) -> Estado/Reward  -4.0\n","  # 47 : acción intercambiar(0,3) -> Estado/Reward  -9.0\n","  # 48 : acción intercambiar(4,3) -> Estado/Reward  -10.0\n","  # 49 : acción intercambiar(5,2) -> Estado/Reward  -7.0\n","  # 50 : acción mover(5,1) -> Estado/Reward  -8.0\n","  # 51 : acción intercambiar(0,7) -> Estado/Reward  -3.0\n","  # 52 : acción intercambiar(5,2) -> Estado/Reward  -4.0\n","  # 53 : acción mover(5,2) -> Estado/Reward  -4.0\n","  # 54 : acción mover(0,0) -> Estado/Reward  -4.0\n","  # 55 : acción mover(6,8) -> Estado/Reward  -4.0\n","  # 56 : acción intercambiar(4,1) -> Estado/Reward  -3.0\n","  # 57 : acción mover(6,9) -> Estado/Reward  -3.0\n","  # 58 : acción intercambiar(0,6) -> Estado/Reward  -4.0\n","  # 59 : acción mover(9,4) -> Estado/Reward  -4.0\n","  # 60 : acción intercambiar(6,1) -> Estado/Reward  -5.0\n","  # 61 : acción mover(5,3) -> Estado/Reward  -4.0\n","  # 62 : acción intercambiar(4,5) -> Estado/Reward  -4.0\n","  # 63 : acción mover(0,6) -> Estado/Reward  -4.0\n","  # 64 : acción mover(9,8) -> Estado/Reward  -4.0\n","  # 65 : acción mover(1,7) -> Estado/Reward  -3.0\n","  # 66 : acción mover(5,3) -> Estado/Reward  -4.0\n","  # 67 : acción intercambiar(4,6) -> Estado/Reward  -4.0\n","  # 68 : acción intercambiar(0,6) -> Estado/Reward  -5.0\n","  # 69 : acción intercambiar(5,1) -> Estado/Reward  -6.0\n","  # 70 : acción mover(5,4) -> Estado/Reward  -6.0\n","  # 71 : acción mover(1,3) -> Estado/Reward  -8.0\n","  # 72 : acción mover(5,6) -> Estado/Reward  -8.0\n","  # 73 : acción mover(5,1) -> Estado/Reward  -5.0\n","  # 74 : acción mover(1,4) -> Estado/Reward  -8.0\n","  # 75 : acción intercambiar(9,3) -> Estado/Reward  -7.0\n","  # 76 : acción mover(2,8) -> Estado/Reward  -5.0\n","  # 77 : acción intercambiar(3,5) -> Estado/Reward  -6.0\n","  # 78 : acción intercambiar(0,9) -> Estado/Reward  -5.0\n","  # 79 : acción mover(9,4) -> Estado/Reward  -5.0\n","  # 80 : acción intercambiar(2,1) -> Estado/Reward  -4.0\n","  # 81 : acción mover(7,1) -> Estado/Reward  -3.0\n","  # 82 : acción intercambiar(3,4) -> Estado/Reward  -2.0\n","  # 83 : acción mover(5,2) -> Estado/Reward  -4.0\n","  # 84 : acción intercambiar(6,2) -> Estado/Reward  -3.0\n","  # 85 : acción mover(4,1) -> Estado/Reward  -6.0\n","  # 86 : acción intercambiar(6,7) -> Estado/Reward  -6.0\n","  # 87 : acción mover(9,3) -> Estado/Reward  -5.0\n","  # 88 : acción intercambiar(0,7) -> Estado/Reward  -8.0\n","  # 89 : acción mover(6,8) -> Estado/Reward  -8.0\n","  # 90 : acción intercambiar(6,7) -> Estado/Reward  -8.0\n","  # 91 : acción intercambiar(8,1) -> Estado/Reward  -5.0\n","  # 92 : acción intercambiar(7,4) -> Estado/Reward  -5.0\n","  # 93 : acción mover(0,3) -> Estado/Reward  -2.0\n","  # 94 : acción intercambiar(8,7) -> Estado/Reward  -2.0\n","  # 95 : acción mover(2,7) -> Estado/Reward  -4.0\n","  # 96 : acción mover(9,7) -> Estado/Reward  -4.0\n","  # 97 : acción intercambiar(9,7) -> Estado/Reward  -4.0\n","  # 98 : acción intercambiar(5,6) -> Estado/Reward  -4.0\n","  # 99 : acción intercambiar(7,7) -> Estado/Reward  -4.0\n","  # 100 : acción mover(6,0) -> Estado/Reward  0.0\n"," Recompensa Final =  0.0\n"," Lista Final =  [-76 -11   8  33  58]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [  8 -11 -76  58  33]\n","  # 1 : acción intercambiar(0,2) -> Estado/Reward  -1.0\n","  # 2 : acción mover(3,4) -> Estado/Reward  98.0\n"," Recompensa Final =  98.0\n"," Lista Final =  [-76 -11   8  33  58]\n","\n","--> El Agente Entrenado ( 98.0 ) genera MEJOR resultado que el azar ( 0.0 )\n","\n","> Prueba  8 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [-97 -27 -50 -62 -28 -80 -52  77 -86 -11]\n","  # 1 : acción mover(9,2) -> Estado/Reward  -24.0\n","  # 2 : acción mover(6,0) -> Estado/Reward  -20.0\n","  # 3 : acción intercambiar(2,6) -> Estado/Reward  -19.0\n","  # 4 : acción intercambiar(5,9) -> Estado/Reward  -18.0\n","  # 5 : acción mover(5,5) -> Estado/Reward  -18.0\n","  # 6 : acción mover(4,4) -> Estado/Reward  -18.0\n","  # 7 : acción intercambiar(9,4) -> Estado/Reward  -15.0\n","  # 8 : acción intercambiar(4,4) -> Estado/Reward  -15.0\n","  # 9 : acción intercambiar(5,8) -> Estado/Reward  -20.0\n","  # 10 : acción intercambiar(0,7) -> Estado/Reward  -23.0\n","  # 11 : acción mover(5,8) -> Estado/Reward  -20.0\n","  # 12 : acción mover(7,8) -> Estado/Reward  -21.0\n","  # 13 : acción intercambiar(9,3) -> Estado/Reward  -18.0\n","  # 14 : acción intercambiar(4,9) -> Estado/Reward  -21.0\n","  # 15 : acción intercambiar(7,0) -> Estado/Reward  -30.0\n","  # 16 : acción intercambiar(3,6) -> Estado/Reward  -29.0\n","  # 17 : acción intercambiar(6,2) -> Estado/Reward  -28.0\n","  # 18 : acción intercambiar(7,8) -> Estado/Reward  -27.0\n","  # 19 : acción mover(8,7) -> Estado/Reward  -28.0\n","  # 20 : acción intercambiar(5,3) -> Estado/Reward  -29.0\n","  # 21 : acción intercambiar(3,8) -> Estado/Reward  -22.0\n","  # 22 : acción intercambiar(2,7) -> Estado/Reward  -21.0\n","  # 23 : acción intercambiar(0,1) -> Estado/Reward  -20.0\n","  # 24 : acción intercambiar(9,6) -> Estado/Reward  -17.0\n","  # 25 : acción intercambiar(7,2) -> Estado/Reward  -18.0\n","  # 26 : acción mover(5,4) -> Estado/Reward  -17.0\n","  # 27 : acción intercambiar(0,7) -> Estado/Reward  -24.0\n","  # 28 : acción intercambiar(8,0) -> Estado/Reward  -27.0\n","  # 29 : acción intercambiar(9,5) -> Estado/Reward  -26.0\n","  # 30 : acción mover(0,7) -> Estado/Reward  -21.0\n","  # 31 : acción intercambiar(9,5) -> Estado/Reward  -26.0\n","  # 32 : acción mover(6,8) -> Estado/Reward  -28.0\n","  # 33 : acción intercambiar(7,1) -> Estado/Reward  -27.0\n","  # 34 : acción mover(3,6) -> Estado/Reward  -30.0\n","  # 35 : acción intercambiar(7,5) -> Estado/Reward  -29.0\n","  # 36 : acción mover(1,0) -> Estado/Reward  -28.0\n","  # 37 : acción mover(2,3) -> Estado/Reward  -29.0\n","  # 38 : acción intercambiar(8,9) -> Estado/Reward  -30.0\n","  # 39 : acción intercambiar(2,2) -> Estado/Reward  -30.0\n","  # 40 : acción intercambiar(0,6) -> Estado/Reward  -29.0\n","  # 41 : acción mover(6,7) -> Estado/Reward  -30.0\n","  # 42 : acción mover(9,9) -> Estado/Reward  -30.0\n","  # 43 : acción mover(7,6) -> Estado/Reward  -29.0\n","  # 44 : acción intercambiar(9,2) -> Estado/Reward  -20.0\n","  # 45 : acción mover(3,6) -> Estado/Reward  -23.0\n","  # 46 : acción intercambiar(6,5) -> Estado/Reward  -22.0\n","  # 47 : acción intercambiar(3,4) -> Estado/Reward  -21.0\n","  # 48 : acción intercambiar(8,4) -> Estado/Reward  -16.0\n","  # 49 : acción mover(7,0) -> Estado/Reward  -21.0\n","  # 50 : acción intercambiar(5,7) -> Estado/Reward  -22.0\n","  # 51 : acción intercambiar(3,1) -> Estado/Reward  -21.0\n","  # 52 : acción intercambiar(5,4) -> Estado/Reward  -20.0\n","  # 53 : acción mover(8,9) -> Estado/Reward  -19.0\n","  # 54 : acción mover(5,1) -> Estado/Reward  -21.0\n","  # 55 : acción mover(4,6) -> Estado/Reward  -21.0\n","  # 56 : acción intercambiar(7,4) -> Estado/Reward  -20.0\n","  # 57 : acción mover(5,4) -> Estado/Reward  -19.0\n","  # 58 : acción intercambiar(5,4) -> Estado/Reward  -20.0\n","  # 59 : acción mover(6,5) -> Estado/Reward  -21.0\n","  # 60 : acción intercambiar(3,3) -> Estado/Reward  -21.0\n","  # 61 : acción mover(9,6) -> Estado/Reward  -24.0\n","  # 62 : acción intercambiar(5,4) -> Estado/Reward  -23.0\n","  # 63 : acción mover(9,5) -> Estado/Reward  -25.0\n","  # 64 : acción intercambiar(7,8) -> Estado/Reward  -24.0\n","  # 65 : acción intercambiar(1,9) -> Estado/Reward  -23.0\n","  # 66 : acción mover(9,5) -> Estado/Reward  -23.0\n","  # 67 : acción mover(8,2) -> Estado/Reward  -19.0\n","  # 68 : acción intercambiar(2,2) -> Estado/Reward  -19.0\n","  # 69 : acción intercambiar(4,8) -> Estado/Reward  -14.0\n","  # 70 : acción mover(9,7) -> Estado/Reward  -14.0\n","  # 71 : acción mover(5,1) -> Estado/Reward  -14.0\n","  # 72 : acción mover(9,6) -> Estado/Reward  -17.0\n","  # 73 : acción mover(1,0) -> Estado/Reward  -16.0\n","  # 74 : acción mover(5,1) -> Estado/Reward  -16.0\n","  # 75 : acción intercambiar(3,4) -> Estado/Reward  -15.0\n","  # 76 : acción intercambiar(9,1) -> Estado/Reward  -20.0\n","  # 77 : acción mover(3,3) -> Estado/Reward  -20.0\n","  # 78 : acción mover(4,1) -> Estado/Reward  -19.0\n","  # 79 : acción mover(8,5) -> Estado/Reward  -20.0\n","  # 80 : acción intercambiar(0,1) -> Estado/Reward  -21.0\n","  # 81 : acción intercambiar(8,0) -> Estado/Reward  -22.0\n","  # 82 : acción mover(2,9) -> Estado/Reward  -21.0\n","  # 83 : acción mover(6,2) -> Estado/Reward  -25.0\n","  # 84 : acción intercambiar(5,8) -> Estado/Reward  -22.0\n","  # 85 : acción intercambiar(3,9) -> Estado/Reward  -21.0\n","  # 86 : acción mover(5,5) -> Estado/Reward  -21.0\n","  # 87 : acción mover(6,5) -> Estado/Reward  -20.0\n","  # 88 : acción intercambiar(1,7) -> Estado/Reward  -23.0\n","  # 89 : acción intercambiar(6,2) -> Estado/Reward  -20.0\n","  # 90 : acción intercambiar(4,5) -> Estado/Reward  -19.0\n","  # 91 : acción intercambiar(4,5) -> Estado/Reward  -20.0\n","  # 92 : acción intercambiar(3,2) -> Estado/Reward  -21.0\n","  # 93 : acción mover(7,5) -> Estado/Reward  -21.0\n","  # 94 : acción intercambiar(9,3) -> Estado/Reward  -22.0\n","  # 95 : acción mover(3,0) -> Estado/Reward  -25.0\n","  # 96 : acción mover(1,4) -> Estado/Reward  -24.0\n","  # 97 : acción intercambiar(4,2) -> Estado/Reward  -23.0\n","  # 98 : acción intercambiar(6,3) -> Estado/Reward  -22.0\n","  # 99 : acción mover(6,0) -> Estado/Reward  -18.0\n","  # 100 : acción intercambiar(4,0) -> Estado/Reward  -17.0\n"," Recompensa Final =  -17.0\n"," Lista Final =  [-97 -27 -52 -50 -86 -28 -80  77 -11 -62]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [-97 -27 -50 -62 -28 -80 -52  77 -86 -11]\n","  # 1 : acción intercambiar(7,9) -> Estado/Reward  -18.0\n","  # 2 : acción intercambiar(8,1) -> Estado/Reward  -7.0\n","  # 3 : acción mover(5,3) -> Estado/Reward  -5.0\n","  # 4 : acción intercambiar(2,3) -> Estado/Reward  -4.0\n","  # 5 : acción mover(7,8) -> Estado/Reward  -3.0\n","  # 6 : acción mover(2,2) -> Estado/Reward  -3.0\n","  # 7 : acción mover(2,2) -> Estado/Reward  -3.0\n","  # 8 : acción mover(2,2) -> Estado/Reward  -3.0\n","  # 9 : acción mover(2,2) -> Estado/Reward  -3.0\n","  # 10 : acción mover(2,2) -> Estado/Reward  -3.0\n"," Recompensa Final =  -3.0\n"," Lista Final =  [-97 -86 -80 -50 -62 -28 -52 -27 -11  77]\n","\n","--> El Agente Entrenado ( -3.0 ) genera MEJOR resultado que el azar ( -17.0 )\n","\n","> Prueba  9 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [-77  98  37  44  15]\n","  # 1 : acción intercambiar(2,4) -> Estado/Reward  -4.0\n","  # 2 : acción intercambiar(8,2) -> Estado/Reward  -5.0\n","  # 3 : acción mover(3,3) -> Estado/Reward  -5.0\n","  # 4 : acción mover(9,4) -> Estado/Reward  -5.0\n","  # 5 : acción intercambiar(2,7) -> Estado/Reward  -4.0\n","  # 6 : acción intercambiar(5,5) -> Estado/Reward  -4.0\n","  # 7 : acción mover(3,5) -> Estado/Reward  -3.0\n","  # 8 : acción mover(8,2) -> Estado/Reward  -5.0\n","  # 9 : acción intercambiar(8,2) -> Estado/Reward  -4.0\n","  # 10 : acción intercambiar(7,8) -> Estado/Reward  -4.0\n","  # 11 : acción intercambiar(1,4) -> Estado/Reward  -3.0\n","  # 12 : acción intercambiar(5,0) -> Estado/Reward  -10.0\n","  # 13 : acción mover(4,6) -> Estado/Reward  -10.0\n","  # 14 : acción intercambiar(5,8) -> Estado/Reward  -10.0\n","  # 15 : acción intercambiar(7,9) -> Estado/Reward  -10.0\n","  # 16 : acción intercambiar(1,3) -> Estado/Reward  -7.0\n","  # 17 : acción mover(0,1) -> Estado/Reward  -6.0\n","  # 18 : acción mover(3,4) -> Estado/Reward  -5.0\n","  # 19 : acción intercambiar(0,5) -> Estado/Reward  -8.0\n","  # 20 : acción intercambiar(7,2) -> Estado/Reward  -7.0\n","  # 21 : acción intercambiar(2,4) -> Estado/Reward  -8.0\n","  # 22 : acción intercambiar(8,8) -> Estado/Reward  -8.0\n","  # 23 : acción intercambiar(3,5) -> Estado/Reward  -9.0\n","  # 24 : acción intercambiar(1,2) -> Estado/Reward  -8.0\n","  # 25 : acción mover(9,2) -> Estado/Reward  -6.0\n","  # 26 : acción intercambiar(9,4) -> Estado/Reward  -6.0\n","  # 27 : acción mover(2,9) -> Estado/Reward  -8.0\n","  # 28 : acción intercambiar(1,2) -> Estado/Reward  -9.0\n","  # 29 : acción intercambiar(4,4) -> Estado/Reward  -9.0\n","  # 30 : acción intercambiar(0,2) -> Estado/Reward  -8.0\n","  # 31 : acción mover(1,3) -> Estado/Reward  -6.0\n","  # 32 : acción mover(2,3) -> Estado/Reward  -7.0\n","  # 33 : acción intercambiar(1,3) -> Estado/Reward  -6.0\n","  # 34 : acción intercambiar(9,8) -> Estado/Reward  -6.0\n","  # 35 : acción mover(3,9) -> Estado/Reward  -5.0\n","  # 36 : acción intercambiar(0,6) -> Estado/Reward  -6.0\n","  # 37 : acción intercambiar(3,2) -> Estado/Reward  -5.0\n","  # 38 : acción intercambiar(1,9) -> Estado/Reward  -6.0\n","  # 39 : acción mover(0,1) -> Estado/Reward  -5.0\n","  # 40 : acción intercambiar(0,3) -> Estado/Reward  -8.0\n","  # 41 : acción intercambiar(0,0) -> Estado/Reward  -8.0\n","  # 42 : acción mover(5,9) -> Estado/Reward  -8.0\n","  # 43 : acción intercambiar(0,1) -> Estado/Reward  -7.0\n","  # 44 : acción mover(0,7) -> Estado/Reward  -5.0\n","  # 45 : acción mover(9,3) -> Estado/Reward  -6.0\n","  # 46 : acción mover(3,7) -> Estado/Reward  -5.0\n","  # 47 : acción intercambiar(5,8) -> Estado/Reward  -5.0\n","  # 48 : acción mover(3,4) -> Estado/Reward  -6.0\n","  # 49 : acción mover(6,4) -> Estado/Reward  -6.0\n","  # 50 : acción mover(8,6) -> Estado/Reward  -6.0\n","  # 51 : acción mover(1,2) -> Estado/Reward  -7.0\n","  # 52 : acción mover(8,3) -> Estado/Reward  -6.0\n","  # 53 : acción intercambiar(0,7) -> Estado/Reward  -5.0\n","  # 54 : acción mover(0,0) -> Estado/Reward  -5.0\n","  # 55 : acción intercambiar(2,2) -> Estado/Reward  -5.0\n","  # 56 : acción mover(7,3) -> Estado/Reward  -6.0\n","  # 57 : acción intercambiar(0,3) -> Estado/Reward  -7.0\n","  # 58 : acción intercambiar(3,0) -> Estado/Reward  -6.0\n","  # 59 : acción intercambiar(0,1) -> Estado/Reward  -5.0\n","  # 60 : acción intercambiar(0,3) -> Estado/Reward  -8.0\n","  # 61 : acción intercambiar(1,3) -> Estado/Reward  -7.0\n","  # 62 : acción mover(2,6) -> Estado/Reward  -9.0\n","  # 63 : acción intercambiar(5,5) -> Estado/Reward  -9.0\n","  # 64 : acción mover(4,2) -> Estado/Reward  -7.0\n","  # 65 : acción mover(4,1) -> Estado/Reward  -6.0\n","  # 66 : acción mover(0,9) -> Estado/Reward  -2.0\n","  # 67 : acción mover(2,7) -> Estado/Reward  -4.0\n","  # 68 : acción intercambiar(3,6) -> Estado/Reward  -3.0\n","  # 69 : acción mover(0,0) -> Estado/Reward  -3.0\n","  # 70 : acción mover(0,2) -> Estado/Reward  -5.0\n","  # 71 : acción mover(7,7) -> Estado/Reward  -5.0\n","  # 72 : acción intercambiar(1,2) -> Estado/Reward  -4.0\n","  # 73 : acción intercambiar(8,8) -> Estado/Reward  -4.0\n","  # 74 : acción intercambiar(0,9) -> Estado/Reward  -7.0\n","  # 75 : acción mover(4,1) -> Estado/Reward  -8.0\n","  # 76 : acción mover(8,1) -> Estado/Reward  -5.0\n","  # 77 : acción intercambiar(5,0) -> Estado/Reward  -4.0\n","  # 78 : acción mover(9,0) -> Estado/Reward  -8.0\n","  # 79 : acción intercambiar(7,6) -> Estado/Reward  -8.0\n","  # 80 : acción intercambiar(3,7) -> Estado/Reward  -7.0\n","  # 81 : acción mover(3,2) -> Estado/Reward  -8.0\n","  # 82 : acción intercambiar(0,9) -> Estado/Reward  -5.0\n","  # 83 : acción intercambiar(0,5) -> Estado/Reward  -8.0\n","  # 84 : acción mover(9,3) -> Estado/Reward  -9.0\n","  # 85 : acción intercambiar(5,7) -> Estado/Reward  -9.0\n","  # 86 : acción mover(2,7) -> Estado/Reward  -9.0\n","  # 87 : acción mover(9,2) -> Estado/Reward  -9.0\n","  # 88 : acción mover(4,0) -> Estado/Reward  -5.0\n","  # 89 : acción mover(3,5) -> Estado/Reward  -6.0\n","  # 90 : acción mover(8,3) -> Estado/Reward  -5.0\n","  # 91 : acción intercambiar(6,3) -> Estado/Reward  -6.0\n","  # 92 : acción intercambiar(8,1) -> Estado/Reward  -1.0\n","  # 93 : acción mover(6,7) -> Estado/Reward  -1.0\n","  # 94 : acción mover(8,5) -> Estado/Reward  -1.0\n","  # 95 : acción intercambiar(3,8) -> Estado/Reward  -2.0\n","  # 96 : acción mover(9,3) -> Estado/Reward  -1.0\n","  # 97 : acción intercambiar(2,0) -> Estado/Reward  -4.0\n","  # 98 : acción mover(1,1) -> Estado/Reward  -4.0\n","  # 99 : acción intercambiar(4,4) -> Estado/Reward  -4.0\n","  # 100 : acción intercambiar(6,6) -> Estado/Reward  -4.0\n"," Recompensa Final =  -4.0\n"," Lista Final =  [ 44  15 -77  37  98]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [-77  98  37  44  15]\n","  # 1 : acción intercambiar(1,3) -> Estado/Reward  -4.0\n","  # 2 : acción mover(4,0) -> Estado/Reward  -2.0\n","  # 3 : acción intercambiar(0,1) -> Estado/Reward  -1.0\n","  # 4 : acción mover(2,3) -> Estado/Reward  96.0\n"," Recompensa Final =  96.0\n"," Lista Final =  [-77  15  37  44  98]\n","\n","--> El Agente Entrenado ( 96.0 ) genera MEJOR resultado que el azar ( -4.0 )\n","\n","> Prueba  10 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 78 -84 -53 -78  50]\n","  # 1 : acción mover(4,4) -> Estado/Reward  -5.0\n","  # 2 : acción intercambiar(5,8) -> Estado/Reward  -5.0\n","  # 3 : acción mover(5,6) -> Estado/Reward  -5.0\n","  # 4 : acción intercambiar(0,6) -> Estado/Reward  -4.0\n","  # 5 : acción mover(2,4) -> Estado/Reward  -4.0\n","  # 6 : acción mover(8,8) -> Estado/Reward  -4.0\n","  # 7 : acción mover(4,8) -> Estado/Reward  -4.0\n","  # 8 : acción intercambiar(9,5) -> Estado/Reward  -4.0\n","  # 9 : acción mover(1,9) -> Estado/Reward  -7.0\n","  # 10 : acción mover(7,3) -> Estado/Reward  -6.0\n","  # 11 : acción mover(5,9) -> Estado/Reward  -6.0\n","  # 12 : acción intercambiar(6,8) -> Estado/Reward  -6.0\n","  # 13 : acción intercambiar(8,9) -> Estado/Reward  -6.0\n","  # 14 : acción mover(5,9) -> Estado/Reward  -6.0\n","  # 15 : acción intercambiar(1,8) -> Estado/Reward  -7.0\n","  # 16 : acción mover(0,6) -> Estado/Reward  -5.0\n","  # 17 : acción mover(9,7) -> Estado/Reward  -5.0\n","  # 18 : acción mover(3,3) -> Estado/Reward  -5.0\n","  # 19 : acción mover(1,8) -> Estado/Reward  -2.0\n","  # 20 : acción intercambiar(0,5) -> Estado/Reward  -5.0\n","  # 21 : acción mover(9,5) -> Estado/Reward  -5.0\n","  # 22 : acción intercambiar(3,5) -> Estado/Reward  -4.0\n","  # 23 : acción mover(8,9) -> Estado/Reward  -4.0\n","  # 24 : acción intercambiar(5,9) -> Estado/Reward  -4.0\n","  # 25 : acción intercambiar(0,8) -> Estado/Reward  -3.0\n","  # 26 : acción intercambiar(5,4) -> Estado/Reward  -3.0\n","  # 27 : acción intercambiar(9,7) -> Estado/Reward  -3.0\n","  # 28 : acción mover(5,4) -> Estado/Reward  -3.0\n","  # 29 : acción mover(9,4) -> Estado/Reward  -3.0\n","  # 30 : acción intercambiar(4,0) -> Estado/Reward  -4.0\n","  # 31 : acción mover(8,5) -> Estado/Reward  -4.0\n","  # 32 : acción mover(2,3) -> Estado/Reward  -5.0\n","  # 33 : acción mover(1,3) -> Estado/Reward  -7.0\n","  # 34 : acción intercambiar(2,1) -> Estado/Reward  -6.0\n","  # 35 : acción mover(6,7) -> Estado/Reward  -6.0\n","  # 36 : acción mover(6,5) -> Estado/Reward  -6.0\n","  # 37 : acción intercambiar(4,6) -> Estado/Reward  -6.0\n","  # 38 : acción mover(9,3) -> Estado/Reward  -7.0\n","  # 39 : acción intercambiar(8,9) -> Estado/Reward  -7.0\n","  # 40 : acción mover(8,9) -> Estado/Reward  -7.0\n","  # 41 : acción mover(7,0) -> Estado/Reward  -3.0\n","  # 42 : acción intercambiar(9,1) -> Estado/Reward  -2.0\n","  # 43 : acción intercambiar(4,8) -> Estado/Reward  -2.0\n","  # 44 : acción mover(8,9) -> Estado/Reward  -2.0\n","  # 45 : acción intercambiar(1,2) -> Estado/Reward  -1.0\n","  # 46 : acción intercambiar(6,1) -> Estado/Reward  -6.0\n","  # 47 : acción mover(4,0) -> Estado/Reward  -4.0\n","  # 48 : acción intercambiar(6,8) -> Estado/Reward  -4.0\n","  # 49 : acción intercambiar(9,6) -> Estado/Reward  -4.0\n","  # 50 : acción intercambiar(3,6) -> Estado/Reward  -3.0\n","  # 51 : acción intercambiar(5,0) -> Estado/Reward  -6.0\n","  # 52 : acción mover(6,6) -> Estado/Reward  -6.0\n","  # 53 : acción mover(3,8) -> Estado/Reward  -5.0\n","  # 54 : acción intercambiar(9,9) -> Estado/Reward  -5.0\n","  # 55 : acción intercambiar(2,3) -> Estado/Reward  -4.0\n","  # 56 : acción intercambiar(5,6) -> Estado/Reward  -4.0\n","  # 57 : acción intercambiar(3,8) -> Estado/Reward  -3.0\n","  # 58 : acción mover(1,5) -> Estado/Reward  -6.0\n","  # 59 : acción mover(2,9) -> Estado/Reward  -6.0\n","  # 60 : acción intercambiar(7,3) -> Estado/Reward  -7.0\n","  # 61 : acción intercambiar(4,5) -> Estado/Reward  -7.0\n","  # 62 : acción mover(8,4) -> Estado/Reward  -7.0\n","  # 63 : acción intercambiar(8,3) -> Estado/Reward  -6.0\n","  # 64 : acción intercambiar(8,3) -> Estado/Reward  -7.0\n","  # 65 : acción mover(4,4) -> Estado/Reward  -7.0\n","  # 66 : acción mover(0,5) -> Estado/Reward  -5.0\n","  # 67 : acción mover(9,3) -> Estado/Reward  -6.0\n","  # 68 : acción mover(8,0) -> Estado/Reward  -2.0\n","  # 69 : acción intercambiar(0,2) -> Estado/Reward  -5.0\n","  # 70 : acción intercambiar(7,9) -> Estado/Reward  -5.0\n","  # 71 : acción mover(6,5) -> Estado/Reward  -5.0\n","  # 72 : acción mover(4,3) -> Estado/Reward  -6.0\n","  # 73 : acción mover(4,8) -> Estado/Reward  -6.0\n","  # 74 : acción intercambiar(1,2) -> Estado/Reward  -5.0\n","  # 75 : acción intercambiar(3,8) -> Estado/Reward  -4.0\n","  # 76 : acción mover(4,6) -> Estado/Reward  -4.0\n","  # 77 : acción mover(1,3) -> Estado/Reward  -6.0\n","  # 78 : acción intercambiar(3,6) -> Estado/Reward  -7.0\n","  # 79 : acción mover(4,4) -> Estado/Reward  -7.0\n","  # 80 : acción mover(4,9) -> Estado/Reward  -7.0\n","  # 81 : acción mover(8,4) -> Estado/Reward  -7.0\n","  # 82 : acción intercambiar(3,2) -> Estado/Reward  -8.0\n","  # 83 : acción intercambiar(8,7) -> Estado/Reward  -8.0\n","  # 84 : acción mover(2,7) -> Estado/Reward  -6.0\n","  # 85 : acción mover(2,9) -> Estado/Reward  -6.0\n","  # 86 : acción mover(4,0) -> Estado/Reward  -6.0\n","  # 87 : acción mover(8,5) -> Estado/Reward  -6.0\n","  # 88 : acción mover(9,3) -> Estado/Reward  -7.0\n","  # 89 : acción mover(5,3) -> Estado/Reward  -6.0\n","  # 90 : acción mover(6,4) -> Estado/Reward  -6.0\n","  # 91 : acción mover(0,5) -> Estado/Reward  -6.0\n","  # 92 : acción mover(2,5) -> Estado/Reward  -8.0\n","  # 93 : acción intercambiar(4,3) -> Estado/Reward  -7.0\n","  # 94 : acción mover(7,7) -> Estado/Reward  -7.0\n","  # 95 : acción intercambiar(6,7) -> Estado/Reward  -7.0\n","  # 96 : acción intercambiar(5,9) -> Estado/Reward  -7.0\n","  # 97 : acción intercambiar(6,9) -> Estado/Reward  -7.0\n","  # 98 : acción mover(7,5) -> Estado/Reward  -7.0\n","  # 99 : acción intercambiar(0,5) -> Estado/Reward  -4.0\n","  # 100 : acción mover(5,3) -> Estado/Reward  -5.0\n"," Recompensa Final =  -5.0\n"," Lista Final =  [-53 -78  50  78 -84]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 78 -84 -53 -78  50]\n","  # 1 : acción mover(0,6) -> Estado/Reward  -1.0\n","  # 2 : acción intercambiar(2,1) -> Estado/Reward  98.0\n"," Recompensa Final =  98.0\n"," Lista Final =  [-84 -78 -53  50  78]\n","\n","--> El Agente Entrenado ( 98.0 ) genera MEJOR resultado que el azar ( -5.0 )\n","\n","================================================================================================\n","\n","= En promedio, el Agente Entrenado ( 77.4 ) tiene MEJORES resultado que  el azar ( 22.7 )\n","\n","================================================================================================\n","\n"],"name":"stdout"}]}]}