{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN Ordenar Lista.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPtyqjp/+ayuhrfG4wMEVwu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5KbquQTFT4jD"},"source":["#Demo de TF-Agents para ordenar elmentos de una lista usando DQN:\r\n"]},{"cell_type":"markdown","metadata":{"id":"dliJD0WRUMWV"},"source":["0) Preparar el ambiente:"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Qxbe02w0T0ip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612782190235,"user_tz":180,"elapsed":6808,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"e8411003-e12d-439e-b6b3-d7993b0541b6"},"source":["#@title Instalar Paquete de TF-Agents\r\n","!pip install -q tf-agents\r\n","print(\"TF-Agentes instalado.\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |▎                               | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 11.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 8.3MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 737kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 808kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 880kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 952kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 5.7MB/s \n","\u001b[?25hTF-Agentes instalado.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wJl4YsniURev","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612782194608,"user_tz":180,"elapsed":11171,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"9fe2f0c9-5342-4468-94ab-c6b0ba578f29"},"source":["#@title Cargar Librerías\r\n","from __future__ import absolute_import\r\n","from __future__ import division\r\n","from __future__ import print_function\r\n","\r\n","import abc\r\n","import tensorflow as tf\r\n","import numpy as np\r\n","import matplotlib\r\n","import matplotlib.pyplot as plt\r\n","from random import randint\r\n","from sklearn import preprocessing\r\n","import copy\r\n","\r\n","from tf_agents.environments import py_environment\r\n","from tf_agents.environments import tf_py_environment\r\n","\r\n","from tf_agents.environments import utils\r\n","from tf_agents.specs import array_spec\r\n","\r\n","from tf_agents.policies import random_tf_policy\r\n","\r\n","from tf_agents.trajectories import time_step as ts\r\n","\r\n","from tf_agents.agents.dqn import dqn_agent\r\n","from tf_agents.networks import q_network\r\n","from tf_agents.utils import common\r\n","\r\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n","from tf_agents.trajectories import trajectory\r\n","\r\n","import os\r\n","from tf_agents.policies import policy_saver\r\n","\r\n","tf.compat.v1.enable_v2_behavior()\r\n","\r\n","print(\"Librerías cargadas.\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Librerías cargadas.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3ONe5w_nUYME"},"source":["1) Establecer las clases del Problema y del Agente:"]},{"cell_type":"code","metadata":{"id":"6TQx1eodsvKj","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612782194612,"user_tz":180,"elapsed":11171,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"43376750-5618-410b-f87e-4d6c5f6abc9a"},"source":["#@title Definir las primitivas a usar para ordenar la lista\r\n","\r\n","# intercambia valores de pos1 con pos2 \r\n","def intercambiar(lista, pos1, pos2):\r\n","  # chequea que las posiciones no se encuentran fuera de la lista\r\n","  if (pos1 < 0) or (pos1 >= len(lista)):\r\n","    if (pos1 < 0):\r\n","      pos1 = 0\r\n","    else:\r\n","      pos1 =  len(lista) - 1\r\n","  if (pos2 < 0) or (pos2 >= len(lista)):\r\n","    if (pos2 < 0):\r\n","      pos2 = 0\r\n","    else:\r\n","      pos2 =  len(lista) - 1\r\n","  # chequea que no sean las mismas posiciones\r\n","  if pos1 == pos2:\r\n","    return lista\r\n","  else:    \r\n","    # realiza el intercambio\r\n","    lista[pos1], lista[pos2] = lista[pos2], lista[pos1]\r\n","    return lista\r\n","\r\n","# mueve el valor de posAnt a posNueva\r\n","def mover(lista, posAnt, posNueva):\r\n","  # chequea que las posiciones no se encuentran fuera de la lista\r\n","  if (posAnt < 0) or (posAnt >= len(lista)):\r\n","    if (posAnt < 0):\r\n","      posAnt = 0\r\n","    else:\r\n","      posAnt =  len(lista) - 1\r\n","  if (posNueva < 0) or (posNueva >= len(lista)):\r\n","    if (posNueva < 0):\r\n","      posNueva = 0\r\n","    else:\r\n","      posNueva =  len(lista) - 1\r\n","  # chequea que no sean las mismas posiciones\r\n","  if posAnt == posNueva:\r\n","    return lista\r\n","  else:    \r\n","    # realiza el intercambio\r\n","    lista.insert(posNueva, lista.pop(posAnt))\r\n","    return lista\r\n","\r\n","# función auxiliar para contar la cantidad de desordenados\r\n","# (debe ser de menor a mayor)\r\n","def contarDesordenados(lista):  \r\n","  cantError = 0\r\n","  if len(lista) > 0:\r\n","    i = 0 \r\n","    while i < len(lista):\r\n","      ant = lista[i]\r\n","      j = i + 1\r\n","      while j < len(lista):\r\n","        actual = lista[j]\r\n","        if actual < ant:\r\n","          cantError = cantError + 1\r\n","        j = j + 1      \r\n","      i = i + 1\r\n","  return cantError\r\n","\r\n","# variable auxiliar para determinar máximo de acciones a probar antes de abortar\r\n","POSIBLES_ACCIONES_DESC = [ \"mover\", \"intercambiar\" ]\r\n","POSIBLES_ACCIONES = [ mover,  intercambiar ]\r\n","\r\n","print(\"Primitivas de acciones definidas: \", POSIBLES_ACCIONES_DESC)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Primitivas de acciones definidas:  ['mover', 'intercambiar']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_R9SyNuiUjyT","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612786234401,"user_tz":180,"elapsed":2163,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"86862877-fd67-4686-b652-e661a113c532"},"source":["#@title Definir Entorno del Problema \n","\n","# parámetros generales\n","MAX_ITERACIONES_REALIZAR = 100\n","MAX_ITERACIONES_ACCIONES_IGUALES = 4\n","TAMANIO_MINIMO_LISTA = 3\n","TAMANIO_MAXIMO_LISTA = 10\n","MAXIMO_VALOR_ACTION = (100 + (TAMANIO_MAXIMO_LISTA-1) * 10 + (TAMANIO_MAXIMO_LISTA-1))\n","\n","\n","def parsearAccion(action):\n","  # como DQN sólo permite 1 action numérica\n","  # esta función se ocupa de parsearla para determinar:\n","  #    tipo de acción\n","  #    param1\n","  #    param2\n","  aux = action\n","  idAccion = aux // 100\n","  aux = aux - idAccion * 100\n","  param1 = aux // 10\n","  aux = aux - param1 * 10\n","  param2 = aux\n","  #print(action, idAccion, param1, param2)\n","  return idAccion, param1, param2\n","\n","# Un entorno que represente el juego podría verse así:\n","class OrdenarListasEnv(py_environment.PyEnvironment):\n","\n","  def __init__(self, reGenerarReset=True):\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=MAXIMO_VALOR_ACTION, name='action')\n","    self._observation_spec = array_spec.BoundedArraySpec(\n","        shape=(TAMANIO_MAXIMO_LISTA,), dtype=np.float32, name='observation')      \n","    self._state = 0\n","    self._antAction = -1\n","    self._episode_ended = False\n","    self._reGenerarListaReset = reGenerarReset\n","    if self._reGenerarListaReset:\n","      # inicializa vacía porque se define en el reset\n","      self._listaOriginal = []\n","    else:\n","      # la lista se define sólo al principio, luego se vuelve a desordenar\n","      self._listaOriginal = self.crearLista()\n","    self._lista = []\n","\n","  def action_spec(self):\n","    # devuelve la forma de las acciones\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    # devuelve la forma de las observaciones   \n","    return self._observation_spec\n","\n","  def render(self, mode = 'human'):\n","    # devuelve la lista para mostsrar\n","    return np.array(self._lista, dtype=np.int32)\n","\n","  def _reset(self):\n","    # resetea el entorno\n","    if self._reGenerarListaReset:\n","      # cada vez que se reseta, se define la lista\n","      self._listaOriginal = self.crearLista()\n","    # siempre la lista de trabajo se copia de la original\n","    self._lista = copy.deepcopy( self._listaOriginal ) \n","    # actualiza el estado considerando cantidad de ordenados\n","    self.actualizarEstado()\n","    self._cantIteraciones = 0\n","    self._episode_ended = False\n","    self._antAction = []\n","    return ts.restart(self.devolverObsActual())\n","\n","  def crearLista(self):\n","    # genera los valores de las listas al azar\n","    cantElemRnd = randint(TAMANIO_MINIMO_LISTA, TAMANIO_MAXIMO_LISTA)\n","    #cantElemRnd = TAMANIO_MAXIMO_LISTA\n","    lista = []\n","    for j in range(cantElemRnd): \n","      lista.append( randint(-99, 99) )\n","    return lista\n","\n","  def actualizarEstado(self):\n","    # actualiza el valor del estado del entorno\n","    # teniendo en cuenta la cantidad de errores negativos\n","    self._state = - contarDesordenados(self._lista)\n","    return self._state\n","\n","  def devolverObsActual(self):\n","    # devuelve valores para la observación actual\n","    # los valores de la lista (rellenando con cero)\n","    # para que el agente sepa el estado real del entorno\n","    res = []\n","    res.extend( self._lista )\n","    val = 100\n","    while (len(res) < TAMANIO_MAXIMO_LISTA):\n","      res.append( val )\n","      val = val + 1\n","    # nota: para DQN parece ser que conviene \n","    # normalizar los valores para que sean más homogeneos \n","    # y no demasiado dispares entre sí \n","    # (sino genera un 'loss' demasiado grande)\n","    r = (res - np.min(res)) / (np.max(res) - np.min(res))\n","    return  np.array([round(v,3) for v in r], dtype=np.float32)\n","\n","  def _step(self, action):\n","    # aplica una acción sobre el entorno\n","    \n","    if self._episode_ended:\n","      # si el entorno está finalizado, lo resetea\n","      return self.reset()\n","\n","    # actualiza cantidad de interacciones \n","    self._cantIteraciones = self._cantIteraciones - 1\n","\n","    # parsea la accion para determinar acción con sus parámetros\n","    idAccion, param1, param2 = parsearAccion(action)\n","\n","    # si es un id de acción válida\n","    if idAccion >= 0 and idAccion < len(POSIBLES_ACCIONES):\n","      # aplica la acción correspondiente en cada lista\n","      # y calculando la cantidad de desordenados como error\n","      self._lista = POSIBLES_ACCIONES[idAccion](self._lista, param1, param2)\n","      \n","      # actualiza el estado con la cantidad de valores correctos\n","      self.actualizarEstado()\n","\n","      # controla que no sea una acción repetida\n","      # de forma que si empieza a repetir más de una cantidad máxima\n","      # finaliza la operatoria\n","      i = 0\n","      cantActionIguales = 0 \n","      while i < len(self._antAction):\n","        if self._antAction[i] == action:\n","          cantActionIguales = cantActionIguales + 1\n","        i = i + 1             \n","      # siempre mantiene 3 (saca la más vieja)\n","      # y agrega la nueva\n","      if len(self._antAction) >= MAX_ITERACIONES_ACCIONES_IGUALES:\n","        self._antAction.pop( 0 )\n","      self._antAction.append( action )\n","\n","    # determina si debe finalizar o no\n","    if (self._state == 0) or (abs(self._cantIteraciones) >= abs(MAX_ITERACIONES_REALIZAR)) or (cantActionIguales >= MAX_ITERACIONES_ACCIONES_IGUALES):\n","      # si está todo ordenado \n","      # o si la cantidad de iteraciones llega al límite\n","      # o se repitió muchas veces la misma action\n","      # fuerza que finaliza\n","      self._episode_ended = True\n","\n","    if self._episode_ended:\n","      # si finaliza\n","      # devuelve el reward (siempre se maximiza):\n","      # si logra ordenar\n","      # se calcula penalizando la cantidad de iteraciones \n","      if (self._state == 0):\n","        reward = MAX_ITERACIONES_REALIZAR + self._cantIteraciones\n","      else:\n","        reward = self._state\n","      return ts.termination(self.devolverObsActual(), reward)\n","    else:\n","      # si no finaliza\n","      reward = self._state\n","      return ts.transition(\n","         self.devolverObsActual(), reward=self._state, discount=0.9)\n","         # notar que no se usa discount=1.0 porque sino genera problema de 'loss' muy grande\n","\n","print(\"\\nEntorno del Problema definido.\")\n","\n","# Definir entornos de entrenamiento y de evaluación\n","# (ambos con lista que se cambia cada vez que se resetea)\n","train_py_env = OrdenarListasEnv(True)\n","eval_py_env = OrdenarListasEnv(True)\n","\n","# Definir wrapper para convertir en entornos TF\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n","\n","# define política al azar independiente del Agente\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","print(\"\\nEntornos de entrenamiento y prueba definidos. \")\n","\n","# definir simulador para probar el entorno\n","def SimularEntorno(env, policy, titulo, mostrarDetalle=True):\n","    print(\"\\n** \", titulo, \"**\")                   \n","    # muesta estado inicial\n","    time_step = env.reset()      \n","    #ob = time_step.observation.numpy()[0]\n","    if mostrarDetalle:\n","      print(\" Ini: [\", time_step, \"]\")    \n","    print(\" Lista Inicial = \", env.pyenv.render()[0] )\n","    j = 1\n","    while not time_step.is_last():\n","      # la política determina la acción a realizar\n","      action_step = policy.action(time_step)\n","      time_step = env.step(action_step.action)\n","      # recupera la observación y muestra el nuevo estado \n","      ac = action_step.action.numpy()[0]\n","      idAccion, param1, param2 = parsearAccion(ac)\n","      r = time_step.reward.numpy()[0]\n","      ##ob = time_step.observation.numpy()[0]\n","      descAccion = \"acción \" +  POSIBLES_ACCIONES_DESC[ idAccion ] + \"(\" + str(param1) + \",\" + str(param2) + \")\"\n","      if mostrarDetalle:\n","        print(\"  #\", j, \":\", descAccion, \"-> Estado/Reward \", r, \"[\", time_step, \",\", action_step, \"]\")\n","      else:\n","        print(\"  #\", j, \":\", descAccion, \"-> Estado/Reward \", r)\n","      ### print(\"    Lista = \", env.pyenv.render()[0] )\n","      j = j + 1\n","    # muestra estado final\n","    print(\" Recompensa Final = \", r )\n","    print(\" Lista Final = \", env.pyenv.render()[0] )\n","    return r\n","\n","print(\"Simulador del entorno definido.\")\n","\n","# Probar el entorno definido con Política Aleatoria (opcional)\n","Probar_Entorno = True #@param {type:\"boolean\"}\n","if Probar_Entorno:\n","  SimularEntorno(eval_env, random_policy, \"Probando el entorno del problema con política al azar\")\n","\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\n","Entorno del Problema definido.\n","\n","Entornos de entrenamiento y prueba definidos. \n","Simulador del entorno definido.\n","\n","**  Probando el entorno del problema con política al azar **\n"," Ini: [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.285, 0.935, 0.5  , 0.605, 0.   , 0.61 , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) ]\n"," Lista Inicial =  [-37 -42  88   1  22 -99  23 -71]\n","  # 1 : acción mover(5,8) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.285, 0.935, 0.5  , 0.605, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([58], dtype=int32)>, state=(), info=()) ]\n","  # 2 : acción mover(9,6) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.285, 0.935, 0.5  , 0.605, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([96], dtype=int32)>, state=(), info=()) ]\n","  # 3 : acción mover(0,0) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.285, 0.935, 0.5  , 0.605, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n","  # 4 : acción intercambiar(1,2) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.935, 0.285, 0.5  , 0.605, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([112], dtype=int32)>, state=(), info=()) ]\n","  # 5 : acción intercambiar(7,8) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.935, 0.285, 0.5  , 0.605, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([178], dtype=int32)>, state=(), info=()) ]\n","  # 6 : acción intercambiar(9,6) -> Estado/Reward  -18.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-18.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.935, 0.285, 0.5  , 0.605, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([196], dtype=int32)>, state=(), info=()) ]\n","  # 7 : acción intercambiar(3,6) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.935, 0.285, 0.14 , 0.605, 0.61 , 0.5  , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([136], dtype=int32)>, state=(), info=()) ]\n","  # 8 : acción mover(4,9) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.935, 0.285, 0.14 , 0.61 , 0.5  , 0.   , 0.605, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([49], dtype=int32)>, state=(), info=()) ]\n","  # 9 : acción intercambiar(5,4) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.935, 0.285, 0.14 , 0.5  , 0.61 , 0.   , 0.605, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([154], dtype=int32)>, state=(), info=()) ]\n","  # 10 : acción mover(1,0) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.935, 0.31 , 0.285, 0.14 , 0.5  , 0.61 , 0.   , 0.605, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>, state=(), info=()) ]\n","  # 11 : acción intercambiar(1,4) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.935, 0.5  , 0.285, 0.14 , 0.31 , 0.61 , 0.   , 0.605, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([114], dtype=int32)>, state=(), info=()) ]\n","  # 12 : acción intercambiar(0,9) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.5  , 0.285, 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([109], dtype=int32)>, state=(), info=()) ]\n","  # 13 : acción mover(6,6) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.5  , 0.285, 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([66], dtype=int32)>, state=(), info=()) ]\n","  # 14 : acción mover(2,8) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.5  , 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.285, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([28], dtype=int32)>, state=(), info=()) ]\n","  # 15 : acción mover(9,1) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.285, 0.5  , 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([91], dtype=int32)>, state=(), info=()) ]\n","  # 16 : acción mover(6,6) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.285, 0.5  , 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([66], dtype=int32)>, state=(), info=()) ]\n","  # 17 : acción intercambiar(0,0) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.285, 0.5  , 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([100], dtype=int32)>, state=(), info=()) ]\n","  # 18 : acción intercambiar(8,7) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.285, 0.5  , 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([187], dtype=int32)>, state=(), info=()) ]\n","  # 19 : acción intercambiar(0,0) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.285, 0.5  , 0.14 , 0.31 , 0.61 , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([100], dtype=int32)>, state=(), info=()) ]\n","  # 20 : acción intercambiar(2,5) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.285, 0.61 , 0.14 , 0.31 , 0.5  , 0.   , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([125], dtype=int32)>, state=(), info=()) ]\n","  # 21 : acción mover(6,0) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.605, 0.285, 0.61 , 0.14 , 0.31 , 0.5  , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([60], dtype=int32)>, state=(), info=()) ]\n","  # 22 : acción mover(4,2) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.605, 0.14 , 0.285, 0.61 , 0.31 , 0.5  , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([42], dtype=int32)>, state=(), info=()) ]\n","  # 23 : acción intercambiar(1,3) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.14 , 0.605, 0.61 , 0.31 , 0.5  , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([113], dtype=int32)>, state=(), info=()) ]\n","  # 24 : acción intercambiar(8,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.14 , 0.605, 0.61 , 0.31 , 0.5  , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([189], dtype=int32)>, state=(), info=()) ]\n","  # 25 : acción intercambiar(5,5) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.14 , 0.605, 0.61 , 0.31 , 0.5  , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([155], dtype=int32)>, state=(), info=()) ]\n","  # 26 : acción mover(4,2) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.61 , 0.14 , 0.605, 0.31 , 0.5  , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([42], dtype=int32)>, state=(), info=()) ]\n","  # 27 : acción mover(8,0) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.935, 0.   , 0.285, 0.61 , 0.14 , 0.605, 0.31 , 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([80], dtype=int32)>, state=(), info=()) ]\n","  # 28 : acción mover(0,3) -> Estado/Reward  -11.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-11.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.61 , 0.935, 0.14 , 0.605, 0.31 , 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>, state=(), info=()) ]\n","  # 29 : acción intercambiar(2,6) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.31 , 0.935, 0.14 , 0.605, 0.61 , 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([126], dtype=int32)>, state=(), info=()) ]\n","  # 30 : acción mover(3,9) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.31 , 0.14 , 0.605, 0.61 , 0.5  , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([39], dtype=int32)>, state=(), info=()) ]\n","  # 31 : acción mover(7,3) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.   , 0.285, 0.31 , 0.935, 0.14 , 0.605, 0.61 , 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([73], dtype=int32)>, state=(), info=()) ]\n","  # 32 : acción mover(0,7) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.31 , 0.935, 0.14 , 0.605, 0.61 , 0.5  , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([7], dtype=int32)>, state=(), info=()) ]\n","  # 33 : acción intercambiar(7,5) -> Estado/Reward  -12.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-12.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.31 , 0.935, 0.14 , 0.605, 0.   , 0.5  , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([175], dtype=int32)>, state=(), info=()) ]\n","  # 34 : acción intercambiar(6,2) -> Estado/Reward  -9.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.31 , 0.5  , 0.14 , 0.605, 0.   , 0.935, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([162], dtype=int32)>, state=(), info=()) ]\n","  # 35 : acción mover(9,7) -> Estado/Reward  -9.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.31 , 0.5  , 0.14 , 0.605, 0.   , 0.935, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([97], dtype=int32)>, state=(), info=()) ]\n","  # 36 : acción intercambiar(5,6) -> Estado/Reward  -10.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-10.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.31 , 0.5  , 0.14 , 0.605, 0.935, 0.   , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([156], dtype=int32)>, state=(), info=()) ]\n","  # 37 : acción intercambiar(0,4) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.5  , 0.14 , 0.285, 0.935, 0.   , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([104], dtype=int32)>, state=(), info=()) ]\n","  # 38 : acción mover(4,7) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.5  , 0.14 , 0.935, 0.   , 0.61 , 0.285, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([47], dtype=int32)>, state=(), info=()) ]\n","  # 39 : acción mover(8,4) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.5  , 0.14 , 0.285, 0.935, 0.   , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([84], dtype=int32)>, state=(), info=()) ]\n","  # 40 : acción mover(9,6) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.5  , 0.14 , 0.285, 0.935, 0.61 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([96], dtype=int32)>, state=(), info=()) ]\n","  # 41 : acción mover(0,8) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.14 , 0.285, 0.935, 0.61 , 0.   , 0.605, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n","  # 42 : acción intercambiar(6,8) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.14 , 0.285, 0.935, 0.61 , 0.605, 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([168], dtype=int32)>, state=(), info=()) ]\n","  # 43 : acción mover(3,2) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.14 , 0.935, 0.61 , 0.605, 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([32], dtype=int32)>, state=(), info=()) ]\n","  # 44 : acción intercambiar(6,7) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.14 , 0.935, 0.61 , 0.   , 0.605, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([167], dtype=int32)>, state=(), info=()) ]\n","  # 45 : acción intercambiar(3,9) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.605, 0.935, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([139], dtype=int32)>, state=(), info=()) ]\n","  # 46 : acción mover(3,3) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.605, 0.935, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([33], dtype=int32)>, state=(), info=()) ]\n","  # 47 : acción mover(4,4) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.605, 0.935, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([44], dtype=int32)>, state=(), info=()) ]\n","  # 48 : acción mover(1,1) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.605, 0.935, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=()) ]\n","  # 49 : acción mover(7,6) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.605, 0.935, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([76], dtype=int32)>, state=(), info=()) ]\n","  # 50 : acción intercambiar(4,3) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.935, 0.605, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([143], dtype=int32)>, state=(), info=()) ]\n","  # 51 : acción intercambiar(8,7) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.285, 0.935, 0.605, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([187], dtype=int32)>, state=(), info=()) ]\n","  # 52 : acción mover(2,4) -> Estado/Reward  -19.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-19.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.5  , 0.935, 0.605, 0.285, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([24], dtype=int32)>, state=(), info=()) ]\n","  # 53 : acción intercambiar(1,0) -> Estado/Reward  -20.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-20.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.5  , 0.31 , 0.935, 0.605, 0.285, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([110], dtype=int32)>, state=(), info=()) ]\n","  # 54 : acción mover(8,6) -> Estado/Reward  -19.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-19.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.5  , 0.31 , 0.935, 0.605, 0.285, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([86], dtype=int32)>, state=(), info=()) ]\n","  # 55 : acción intercambiar(3,0) -> Estado/Reward  -20.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-20.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.935, 0.5  , 0.285, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([130], dtype=int32)>, state=(), info=()) ]\n","  # 56 : acción mover(7,6) -> Estado/Reward  -21.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-21.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.935, 0.5  , 0.285, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([76], dtype=int32)>, state=(), info=()) ]\n","  # 57 : acción intercambiar(5,6) -> Estado/Reward  -20.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-20.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.935, 0.5  , 0.285, 0.14 , 0.61 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([156], dtype=int32)>, state=(), info=()) ]\n","  # 58 : acción intercambiar(7,5) -> Estado/Reward  -19.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-19.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.935, 0.5  , 0.285, 0.   , 0.61 , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([175], dtype=int32)>, state=(), info=()) ]\n","  # 59 : acción intercambiar(4,0) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.31 , 0.935, 0.5  , 0.605, 0.   , 0.61 , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([140], dtype=int32)>, state=(), info=()) ]\n","  # 60 : acción intercambiar(1,5) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.   , 0.935, 0.5  , 0.605, 0.31 , 0.61 , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([115], dtype=int32)>, state=(), info=()) ]\n","  # 61 : acción mover(8,2) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.   , 0.14 , 0.935, 0.5  , 0.605, 0.31 , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([82], dtype=int32)>, state=(), info=()) ]\n","  # 62 : acción mover(2,9) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.285, 0.   , 0.935, 0.5  , 0.605, 0.31 , 0.61 , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, state=(), info=()) ]\n","  # 63 : acción mover(7,0) -> Estado/Reward  -8.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-8.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.285, 0.   , 0.935, 0.5  , 0.605, 0.31 , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([70], dtype=int32)>, state=(), info=()) ]\n","  # 64 : acción mover(9,6) -> Estado/Reward  -9.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.285, 0.   , 0.935, 0.5  , 0.605, 0.61 , 0.31 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([96], dtype=int32)>, state=(), info=()) ]\n","  # 65 : acción mover(0,0) -> Estado/Reward  -9.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.285, 0.   , 0.935, 0.5  , 0.605, 0.61 , 0.31 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=()) ]\n","  # 66 : acción intercambiar(4,1) -> Estado/Reward  -10.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-10.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.5  , 0.   , 0.935, 0.285, 0.605, 0.61 , 0.31 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([141], dtype=int32)>, state=(), info=()) ]\n","  # 67 : acción intercambiar(7,1) -> Estado/Reward  -9.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.935, 0.285, 0.605, 0.61 , 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([171], dtype=int32)>, state=(), info=()) ]\n","  # 68 : acción mover(3,6) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.285, 0.605, 0.61 , 0.935, 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([36], dtype=int32)>, state=(), info=()) ]\n","  # 69 : acción mover(9,8) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.285, 0.605, 0.61 , 0.935, 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([98], dtype=int32)>, state=(), info=()) ]\n","  # 70 : acción intercambiar(2,2) -> Estado/Reward  -6.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-6.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.285, 0.605, 0.61 , 0.935, 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([122], dtype=int32)>, state=(), info=()) ]\n","  # 71 : acción intercambiar(5,9) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.285, 0.605, 0.5  , 0.935, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([159], dtype=int32)>, state=(), info=()) ]\n","  # 72 : acción intercambiar(1,1) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.285, 0.605, 0.5  , 0.935, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([111], dtype=int32)>, state=(), info=()) ]\n","  # 73 : acción mover(1,1) -> Estado/Reward  -5.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.285, 0.605, 0.5  , 0.935, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([11], dtype=int32)>, state=(), info=()) ]\n","  # 74 : acción mover(5,4) -> Estado/Reward  -4.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-4.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.285, 0.5  , 0.605, 0.935, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([54], dtype=int32)>, state=(), info=()) ]\n","  # 75 : acción mover(3,6) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.5  , 0.605, 0.935, 0.285, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([36], dtype=int32)>, state=(), info=()) ]\n","  # 76 : acción intercambiar(3,8) -> Estado/Reward  -10.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-10.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.61 , 0.605, 0.935, 0.285, 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([138], dtype=int32)>, state=(), info=()) ]\n","  # 77 : acción mover(7,8) -> Estado/Reward  -10.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-10.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.61 , 0.605, 0.935, 0.285, 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([78], dtype=int32)>, state=(), info=()) ]\n","  # 78 : acción intercambiar(5,9) -> Estado/Reward  -9.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-9.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.   , 0.61 , 0.605, 0.5  , 0.285, 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([159], dtype=int32)>, state=(), info=()) ]\n","  # 79 : acción mover(2,9) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.61 , 0.605, 0.5  , 0.285, 0.935, 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, state=(), info=()) ]\n","  # 80 : acción mover(0,8) -> Estado/Reward  -19.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-19.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.61 , 0.605, 0.5  , 0.285, 0.935, 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n","  # 81 : acción intercambiar(8,1) -> Estado/Reward  -12.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-12.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.31 , 0.14 , 0.605, 0.5  , 0.285, 0.935, 0.   , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([181], dtype=int32)>, state=(), info=()) ]\n","  # 82 : acción mover(0,1) -> Estado/Reward  -11.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-11.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.605, 0.5  , 0.285, 0.935, 0.   , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=()) ]\n","  # 83 : acción mover(2,5) -> Estado/Reward  -10.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-10.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.5  , 0.285, 0.935, 0.605, 0.   , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([25], dtype=int32)>, state=(), info=()) ]\n","  # 84 : acción mover(6,3) -> Estado/Reward  -7.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-7.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.14 , 0.31 , 0.5  , 0.   , 0.285, 0.935, 0.605, 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([63], dtype=int32)>, state=(), info=()) ]\n","  # 85 : acción intercambiar(0,6) -> Estado/Reward  -14.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-14.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.5  , 0.   , 0.285, 0.935, 0.14 , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([106], dtype=int32)>, state=(), info=()) ]\n","  # 86 : acción intercambiar(5,2) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.935, 0.   , 0.285, 0.5  , 0.14 , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([152], dtype=int32)>, state=(), info=()) ]\n","  # 87 : acción mover(2,0) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.935, 0.605, 0.31 , 0.   , 0.285, 0.5  , 0.14 , 0.61 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([20], dtype=int32)>, state=(), info=()) ]\n","  # 88 : acción mover(0,8) -> Estado/Reward  -10.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-10.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.   , 0.285, 0.5  , 0.14 , 0.61 , 0.935, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, state=(), info=()) ]\n","  # 89 : acción intercambiar(8,5) -> Estado/Reward  -13.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-13.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.   , 0.285, 0.5  , 0.935, 0.61 , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([185], dtype=int32)>, state=(), info=()) ]\n","  # 90 : acción mover(2,9) -> Estado/Reward  -18.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-18.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.285, 0.5  , 0.935, 0.61 , 0.14 , 0.   , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, state=(), info=()) ]\n","  # 91 : acción mover(9,5) -> Estado/Reward  -16.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-16.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.285, 0.5  , 0.935, 0.   , 0.61 , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([95], dtype=int32)>, state=(), info=()) ]\n","  # 92 : acción mover(6,5) -> Estado/Reward  -17.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.31 , 0.285, 0.5  , 0.935, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([65], dtype=int32)>, state=(), info=()) ]\n","  # 93 : acción mover(3,1) -> Estado/Reward  -19.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-19.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.5  , 0.31 , 0.285, 0.935, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([31], dtype=int32)>, state=(), info=()) ]\n","  # 94 : acción intercambiar(7,3) -> Estado/Reward  -18.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-18.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.5  , 0.31 , 0.14 , 0.935, 0.61 , 0.   , 0.285, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([173], dtype=int32)>, state=(), info=()) ]\n","  # 95 : acción intercambiar(3,3) -> Estado/Reward  -18.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-18.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.5  , 0.31 , 0.14 , 0.935, 0.61 , 0.   , 0.285, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([133], dtype=int32)>, state=(), info=()) ]\n","  # 96 : acción intercambiar(9,3) -> Estado/Reward  -19.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-19.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.5  , 0.31 , 0.285, 0.935, 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([193], dtype=int32)>, state=(), info=()) ]\n","  # 97 : acción intercambiar(4,1) -> Estado/Reward  -20.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-20.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.935, 0.31 , 0.285, 0.5  , 0.61 , 0.   , 0.14 , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([141], dtype=int32)>, state=(), info=()) ]\n","  # 98 : acción intercambiar(7,4) -> Estado/Reward  -19.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-19.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.935, 0.31 , 0.285, 0.14 , 0.61 , 0.   , 0.5  , 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([174], dtype=int32)>, state=(), info=()) ]\n","  # 99 : acción intercambiar(3,7) -> Estado/Reward  -20.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-20.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.935, 0.31 , 0.5  , 0.14 , 0.61 , 0.   , 0.285, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([137], dtype=int32)>, state=(), info=()) ]\n","  # 100 : acción mover(6,1) -> Estado/Reward  -15.0 [ TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([-15.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n","array([[0.605, 0.   , 0.935, 0.31 , 0.5  , 0.14 , 0.61 , 0.285, 0.995,\n","        1.   ]], dtype=float32)>) , PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([61], dtype=int32)>, state=(), info=()) ]\n"," Recompensa Final =  -15.0\n"," Lista Final =  [ 22 -99  88 -37   1 -71  23 -42]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dLCBLMD4Zsia"},"source":["3) Llevar a cabo el Entrenamiento:"]},{"cell_type":"code","metadata":{"id":"b-G18iz7flcn","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612782201475,"user_tz":180,"elapsed":18025,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"4959e403-e9ac-48ad-9623-1af6c295f61b"},"source":["#@title Definir Métricas para evaluación\r\n","\r\n","# Se usa el promedio de la recompensa (la más común)\r\n","# See also the metrics module for standard implementations of different metrics.\r\n","# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\r\n","\r\n","def compute_avg_return(environment, policy, num_episodes=10):\r\n","\r\n","  total_return = 0.0\r\n","  for _ in range(num_episodes):\r\n","\r\n","    time_step = environment.reset()\r\n","    episode_return = 0.0\r\n","    while not time_step.is_last():\r\n","      action_step = policy.action(time_step)\r\n","      time_step = environment.step(action_step.action)\r\n","      episode_return += time_step.reward\r\n","    total_return += episode_return\r\n","\r\n","  avg_return = total_return / num_episodes\r\n","  return avg_return.numpy()[0]\r\n","\r\n","print(\"Métricas definidas.\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Métricas definidas.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"diEOEg3JaMHa","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612782202497,"user_tz":180,"elapsed":19044,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"0624710c-e62e-41dd-aec2-f0f998e9f3bb"},"source":["#@title Definir el Agente tipo DQN\r\n","\r\n","##learning_rate = 1e-3  # @param {type:\"number\"}\r\n","##cant_neuronas_ocultas = 100 # @param {type:\"integer\"}\r\n","learning_rate = 1e-3  # @param {type:\"number\"}\r\n","cant_neuronas_ocultas = \"100, 50, 25\" # @param {type:\"string\"}\r\n","\r\n","# Define cantidad de neuronas ocultas para RNA-Q\r\n","hidden_layers = []\r\n","for val in cant_neuronas_ocultas.split(','):\r\n","  if  int(val) < 1:\r\n","    hidden_layers.append( 10 )\r\n","  else:\r\n","    hidden_layers.append( int(val) )\r\n","fc_layer_params = tuple(hidden_layers, )\r\n","\r\n","# Define RNA-Q\r\n","q_net = q_network.QNetwork(\r\n","    train_env.observation_spec(),\r\n","    train_env.action_spec(),\r\n","    fc_layer_params=fc_layer_params)\r\n","\r\n","optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\r\n","\r\n","train_step_counter = tf.Variable(0)\r\n","\r\n","# Define el agente de tipo Q\r\n","ag = dqn_agent.DqnAgent(\r\n","    train_env.time_step_spec(),\r\n","    train_env.action_spec(),\r\n","    q_network=q_net,\r\n","    optimizer=optimizer,\r\n","    td_errors_loss_fn=common.element_wise_squared_loss,\r\n","    train_step_counter=train_step_counter)\r\n","\r\n","ag.initialize()\r\n","\r\n","# define política para evaluación para el Agente\r\n","eval_policy = ag.policy\r\n","\r\n","# define política para recolección de datos para el Agente\r\n","collect_policy = ag.collect_policy\r\n","\r\n","print(\"Agente DQN inicializado. \")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Agente DQN inicializado. \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9EBRZGSkZ5N6","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612782204149,"user_tz":180,"elapsed":20692,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"59e3bff7-71cd-4ed9-bc03-b7740b85a865"},"source":["#@title Preparar datos para Entrenamiento\r\n","\r\n","initial_collect_steps =   100# @param {type:\"integer\"} \r\n","collect_steps_per_iteration = 10  # @param {type:\"integer\"}\r\n","replay_buffer_max_length = 100000  # @param {type:\"integer\"}\r\n","batch_size = 64  # @param {type:\"integer\"}\r\n","\r\n","\r\n","# Define 'Replay Buffer' para que el agente recuerde las observaciones realizadas\r\n","replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n","    data_spec = ag.collect_data_spec,\r\n","    batch_size = train_env.batch_size,\r\n","    max_length = replay_buffer_max_length)\r\n","\r\n","# Recolecta datos generados al azar\r\n","# This loop is so common in RL, that we provide standard implementations. \r\n","# For more details see the drivers module.\r\n","# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\r\n","\r\n","def collect_step(environment, policy, buffer):\r\n","  time_step = environment.current_time_step()\r\n","  action_step = policy.action(time_step)\r\n","  next_time_step = environment.step(action_step.action)\r\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n","\r\n","  # Add trajectory to the replay buffer\r\n","  buffer.add_batch(traj)\r\n","\r\n","def collect_data(env, policy, buffer, steps):\r\n","  for _ in range(steps):\r\n","    collect_step(env, policy, buffer)\r\n","\r\n","collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\r\n","\r\n","print(\"\\nDatos recolectados.\")\r\n","\r\n","# Muestra ejemplo de los datos recolectados\r\n","##iter(replay_buffer.as_dataset()).next()\r\n","\r\n","# Preparar los datos recolectados con trajectories de shape [Bx2x...]\r\n","dataset = replay_buffer.as_dataset(\r\n","    num_parallel_calls=3, \r\n","    sample_batch_size=batch_size, \r\n","    num_steps=2).prefetch(3)\r\n","iterator = iter(dataset)\r\n","# Muestra ejemplo \r\n","##iterator.next()\r\n","print(\"\\nDataset creado.\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\n","Datos recolectados.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n","\n","Dataset creado.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2k3S5IqGhK-a","cellView":"form"},"source":["#@title Entrenar al Agente\r\n","\r\n","cant_ciclos_entrenamiento =  75000# @param {type:\"integer\"}\r\n","log_cada_ciclos = 500  # @param {type:\"integer\"}\r\n","mostar_recompensa_cada = 1000  # @param {type:\"integer\"}\r\n","cant_episodios_evaluacion =  25# @param {type:\"integer\"}\r\n","\r\n","#  Optimize by wrapping some of the code in a graph using TF function (Optional)\r\n","ag.train = common.function(ag.train)\r\n","\r\n","# Reset the train step\r\n","ag.train_step_counter.assign(0)\r\n","\r\n","# Evaluate the agent's policy once before training.\r\n","avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","ar_ciclo = []\r\n","ar_returns = []\r\n","ar_loss = []\r\n","\r\n","print(\"\\n** Comienza el Entrenamiento **\\n\")\r\n","for _ in range(cant_ciclos_entrenamiento):\r\n","\r\n","  # Collect a few steps using collect_policy and save to the replay buffer.\r\n","  collect_data(train_env, ag.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n","\r\n","  # Sample a batch of data from the buffer and update the agent's network.\r\n","  experience, unused_info = next(iterator)\r\n","  train_loss = ag.train(experience).loss\r\n","\r\n","  step = ag.train_step_counter.numpy()\r\n","\r\n","  if (step == 1) or (step == cant_ciclos_entrenamiento) or (step % log_cada_ciclos == 0):\r\n","    print('step = {0}: loss = {1:.3f}'.format(step, train_loss))    \r\n","    ar_ciclo.append( step )\r\n","    ar_loss.append( train_loss )\r\n","    avg_return = compute_avg_return(eval_env, ag.policy, cant_episodios_evaluacion)\r\n","    ar_returns.append( avg_return )\r\n","\r\n","    if (step == 1) or (step == cant_ciclos_entrenamiento) or (step % mostar_recompensa_cada == 0):\r\n","      print('step = {0}: Promedio Recompensa = {1:.1f}'.format(step, avg_return))\r\n","\r\n","print(\"\\n** Entrenamiento Finalizado **\\n\")\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EBBl7mRkQYa","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"status":"ok","timestamp":1612785732303,"user_tz":180,"elapsed":284563,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"7aa4713a-f742-4da7-ec17-001b4e2f5abb"},"source":["#@title Mostrar Gráficos del Entrenamiento\r\n","\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","plt.plot( ar_ciclo, ar_returns)\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Promedio Recompensa\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n","\r\n","plt.figure(figsize=(12,5)) \r\n","#plt.plot( ar_ciclo, ar_returns)\r\n","plt.plot( ar_ciclo, ar_loss, color=\"red\" )\r\n","plt.title(\"Resultados del Entrenamiento del Agente - Loss de Entrenamiento\")\r\n","#plt.legend(['Promedio Recompensa', 'Loss de Entrenamiento'], loc='upper right')\r\n","plt.ylabel('Valor')\r\n","plt.xlabel('Ciclo')\r\n","plt.xlim(right=max(ar_ciclo))   \r\n","plt.grid(True)\r\n","plt.show()\r\n"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Rjj7bRe7AB-l"},"source":["5) Cargar / Graba el modelo de las políticas entrenadas:"]},{"cell_type":"code","metadata":{"cellView":"form","id":"6V2EiqdwAy_R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612786266057,"user_tz":180,"elapsed":28681,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"7c8d918f-41d1-4a7c-eb7f-1df383cdac6d"},"source":["#@title Cargar o Guardar el Modelo\n","# parámetros\n","directorio_modelo = '/content/gdrive/MyDrive/IA/demo Agentes/Modelos' #@param {type:\"string\"}\n","nombre_modelo_grabar = \"policy-OrdenarLista\" #@param {type:\"string\"}\n","accion_realizar = \"-\" #@param [\"-\", \"Cargar Modelo\", \"Grabar Modelo\"]\n","\n","# determina lugar donde se guarda el modelo\n","policy_dir = os.path.join(directorio_modelo, nombre_modelo_grabar)\n","\n","if accion_realizar != \"-\":\n","  # Montar Drive\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","if accion_realizar == \"Grabar Modelo\":\n","  # guarda la politica del agente entrenado\n","  tf_policy_saver = policy_saver.PolicySaver(ag.policy)\n","  tf_policy_saver.save(policy_dir)\n","  print(\"\\nPolítica del modelo guardada en \", policy_dir)\n","elif accion_realizar == \"Cargar Modelo\":\n","  # carga la política del modelo\n","  saved_policy = tf.compat.v2.saved_model.load(policy_dir)\n","  print(\"\\nPolítica del modelo recuperada de \", policy_dir)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as QNetwork_layer_call_and_return_conditional_losses, QNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 35). These functions will not be directly callable after loading.\n","WARNING:absl:Found untraced functions such as QNetwork_layer_call_and_return_conditional_losses, QNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 35). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista/assets\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Política del modelo guardada en  /content/gdrive/MyDrive/IA/demo Agentes/Modelos/policy-OrdenarLista\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j79yPUetlUbs"},"source":["4) Probar entrenamiento comparando resultados:"]},{"cell_type":"code","metadata":{"id":"lLkdkcBjl3Xs","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612786498475,"user_tz":180,"elapsed":6981,"user":{"displayName":"pgp tensorflow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcUd7fOM57tm94W-uJnVjbIVDCdQqTHGrWG-h6xA=s64","userId":"04809512947468796788"}},"outputId":"7640d78c-a15b-4153-e630-6ce44cc7f470"},"source":["#@title Probar el Agente Entrenado contra el Azar\r\n","cantidad_probar =  10# @param {type:\"integer\"}\r\n","promAzar = 0\r\n","promAgente = 0\r\n","\r\n","# determina política a usar\r\n","policy_agente_entrenado = None\r\n","if not('ag' in vars() or 'ag' in globals()) or ag is None:\r\n","  if not('saved_policy' in vars() or 'saved_policy' in globals()) or saved_policy is None:\r\n","    ValueError(\"No hay política entrenada definida.\")\r\n","  else:\r\n","    policy_agente_entrenado = saved_policy\r\n","    print(\"- Se usa la política recuperada del drive.\")\r\n","else:\r\n","  policy_agente_entrenado = ag.policy\r\n","  print(\"- Se usa la política del modelo entrenado.\")\r\n","\r\n","for i in range(cantidad_probar):\r\n","\r\n","  print(\"\\n> Prueba \", i+1, \":\")\r\n","\r\n","  # crea nuevo entorno que mantiene la misma lista\r\n","  prueba_env =  tf_py_environment.TFPyEnvironment( OrdenarListasEnv(False) )\r\n","\r\n","  # Probar Aleatorio\r\n","  valorAzar = SimularEntorno(prueba_env, random_policy, \"Resultados Aleatorio\", False) \r\n","  promAzar = promAzar + valorAzar\r\n","\r\n","  # Probar Agente Entrenado\r\n","  valorAgente = SimularEntorno(prueba_env, policy_agente_entrenado, \"Resultados de Agente Entrenado\", False) \r\n","  promAgente = promAgente + valorAgente\r\n","\r\n","  # Decide Ganador\r\n","  if valorAzar < valorAgente:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera MEJOR resultado que el azar (\", valorAzar,\")\")\r\n","  else:\r\n","    print(\"\\n--> El Agente Entrenado (\", valorAgente,\") genera PEOR resultado que el azar (\", valorAzar,\")\")\r\n","\r\n","# Decide Ganador General\r\n","if cantidad_probar > 0:\r\n","  promAgente = promAgente / cantidad_probar\r\n","  promAzar = promAzar / cantidad_probar\r\n","  print(\"\\n================================================================================================\\n\")\r\n","  if promAzar < promAgente:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene MEJORES resultado que  el azar (\", promAzar,\")\")\r\n","  else:\r\n","    print(\"= En promedio, el Agente Entrenado (\", promAgente,\") tiene PEORES resultados que el azar (\", promAzar,\")\")\r\n","  print(\"\\n================================================================================================\\n\")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["- Se usa la política del modelo entrenado.\n","\n","> Prueba  1 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 33  -7 -79   8  11 -13  21 -84 -41  19]\n","  # 1 : acción intercambiar(8,9) -> Estado/Reward  -26.0\n","  # 2 : acción intercambiar(3,6) -> Estado/Reward  -29.0\n","  # 3 : acción intercambiar(7,8) -> Estado/Reward  -30.0\n","  # 4 : acción intercambiar(6,0) -> Estado/Reward  -25.0\n","  # 5 : acción intercambiar(3,4) -> Estado/Reward  -24.0\n","  # 6 : acción intercambiar(5,6) -> Estado/Reward  -25.0\n","  # 7 : acción intercambiar(4,5) -> Estado/Reward  -26.0\n","  # 8 : acción intercambiar(7,6) -> Estado/Reward  -27.0\n","  # 9 : acción mover(0,5) -> Estado/Reward  -28.0\n","  # 10 : acción mover(1,7) -> Estado/Reward  -34.0\n","  # 11 : acción intercambiar(9,9) -> Estado/Reward  -34.0\n","  # 12 : acción mover(8,2) -> Estado/Reward  -28.0\n","  # 13 : acción mover(3,2) -> Estado/Reward  -29.0\n","  # 14 : acción mover(8,1) -> Estado/Reward  -24.0\n","  # 15 : acción mover(6,0) -> Estado/Reward  -24.0\n","  # 16 : acción intercambiar(1,9) -> Estado/Reward  -21.0\n","  # 17 : acción mover(6,9) -> Estado/Reward  -18.0\n","  # 18 : acción mover(8,6) -> Estado/Reward  -18.0\n","  # 19 : acción intercambiar(5,4) -> Estado/Reward  -17.0\n","  # 20 : acción intercambiar(5,3) -> Estado/Reward  -18.0\n","  # 21 : acción intercambiar(5,1) -> Estado/Reward  -19.0\n","  # 22 : acción intercambiar(4,8) -> Estado/Reward  -22.0\n","  # 23 : acción intercambiar(1,4) -> Estado/Reward  -21.0\n","  # 24 : acción intercambiar(8,9) -> Estado/Reward  -22.0\n","  # 25 : acción intercambiar(8,4) -> Estado/Reward  -25.0\n","  # 26 : acción mover(2,9) -> Estado/Reward  -30.0\n","  # 27 : acción intercambiar(4,6) -> Estado/Reward  -33.0\n","  # 28 : acción mover(4,5) -> Estado/Reward  -32.0\n","  # 29 : acción mover(4,8) -> Estado/Reward  -32.0\n","  # 30 : acción intercambiar(9,3) -> Estado/Reward  -23.0\n","  # 31 : acción intercambiar(8,1) -> Estado/Reward  -24.0\n","  # 32 : acción intercambiar(9,6) -> Estado/Reward  -25.0\n","  # 33 : acción mover(8,7) -> Estado/Reward  -26.0\n","  # 34 : acción intercambiar(9,0) -> Estado/Reward  -27.0\n","  # 35 : acción intercambiar(7,5) -> Estado/Reward  -28.0\n","  # 36 : acción intercambiar(1,9) -> Estado/Reward  -29.0\n","  # 37 : acción intercambiar(7,0) -> Estado/Reward  -24.0\n","  # 38 : acción mover(1,3) -> Estado/Reward  -24.0\n","  # 39 : acción intercambiar(4,4) -> Estado/Reward  -24.0\n","  # 40 : acción mover(3,6) -> Estado/Reward  -25.0\n","  # 41 : acción intercambiar(7,0) -> Estado/Reward  -30.0\n","  # 42 : acción mover(4,3) -> Estado/Reward  -29.0\n","  # 43 : acción mover(8,6) -> Estado/Reward  -27.0\n","  # 44 : acción intercambiar(5,1) -> Estado/Reward  -26.0\n","  # 45 : acción intercambiar(4,0) -> Estado/Reward  -27.0\n","  # 46 : acción mover(0,1) -> Estado/Reward  -28.0\n","  # 47 : acción intercambiar(7,3) -> Estado/Reward  -29.0\n","  # 48 : acción mover(9,3) -> Estado/Reward  -29.0\n","  # 49 : acción mover(7,6) -> Estado/Reward  -28.0\n","  # 50 : acción mover(4,2) -> Estado/Reward  -30.0\n","  # 51 : acción mover(7,3) -> Estado/Reward  -34.0\n","  # 52 : acción intercambiar(9,3) -> Estado/Reward  -27.0\n","  # 53 : acción intercambiar(2,0) -> Estado/Reward  -24.0\n","  # 54 : acción mover(1,5) -> Estado/Reward  -22.0\n","  # 55 : acción mover(5,2) -> Estado/Reward  -25.0\n","  # 56 : acción intercambiar(6,3) -> Estado/Reward  -28.0\n","  # 57 : acción intercambiar(7,6) -> Estado/Reward  -27.0\n","  # 58 : acción intercambiar(4,3) -> Estado/Reward  -26.0\n","  # 59 : acción mover(1,6) -> Estado/Reward  -21.0\n","  # 60 : acción mover(1,3) -> Estado/Reward  -19.0\n","  # 61 : acción mover(7,8) -> Estado/Reward  -20.0\n","  # 62 : acción mover(3,2) -> Estado/Reward  -21.0\n","  # 63 : acción intercambiar(3,7) -> Estado/Reward  -18.0\n","  # 64 : acción mover(1,8) -> Estado/Reward  -23.0\n","  # 65 : acción mover(1,7) -> Estado/Reward  -19.0\n","  # 66 : acción mover(6,9) -> Estado/Reward  -20.0\n","  # 67 : acción intercambiar(4,3) -> Estado/Reward  -21.0\n","  # 68 : acción intercambiar(9,1) -> Estado/Reward  -20.0\n","  # 69 : acción intercambiar(5,1) -> Estado/Reward  -23.0\n","  # 70 : acción mover(4,8) -> Estado/Reward  -27.0\n","  # 71 : acción intercambiar(5,2) -> Estado/Reward  -28.0\n","  # 72 : acción mover(7,5) -> Estado/Reward  -30.0\n","  # 73 : acción intercambiar(6,5) -> Estado/Reward  -29.0\n","  # 74 : acción mover(0,9) -> Estado/Reward  -28.0\n","  # 75 : acción mover(0,4) -> Estado/Reward  -28.0\n","  # 76 : acción intercambiar(9,8) -> Estado/Reward  -29.0\n","  # 77 : acción intercambiar(9,8) -> Estado/Reward  -28.0\n","  # 78 : acción intercambiar(3,5) -> Estado/Reward  -31.0\n","  # 79 : acción mover(6,8) -> Estado/Reward  -31.0\n","  # 80 : acción intercambiar(2,2) -> Estado/Reward  -31.0\n","  # 81 : acción intercambiar(3,1) -> Estado/Reward  -32.0\n","  # 82 : acción mover(9,3) -> Estado/Reward  -34.0\n","  # 83 : acción intercambiar(1,9) -> Estado/Reward  -21.0\n","  # 84 : acción intercambiar(8,4) -> Estado/Reward  -16.0\n","  # 85 : acción mover(1,3) -> Estado/Reward  -18.0\n","  # 86 : acción mover(1,4) -> Estado/Reward  -19.0\n","  # 87 : acción mover(6,1) -> Estado/Reward  -20.0\n","  # 88 : acción mover(6,1) -> Estado/Reward  -25.0\n","  # 89 : acción mover(0,2) -> Estado/Reward  -23.0\n","  # 90 : acción mover(1,4) -> Estado/Reward  -24.0\n","  # 91 : acción intercambiar(7,7) -> Estado/Reward  -24.0\n","  # 92 : acción mover(2,5) -> Estado/Reward  -21.0\n","  # 93 : acción mover(0,6) -> Estado/Reward  -17.0\n","  # 94 : acción intercambiar(9,9) -> Estado/Reward  -17.0\n","  # 95 : acción mover(2,7) -> Estado/Reward  -16.0\n","  # 96 : acción intercambiar(7,3) -> Estado/Reward  -15.0\n","  # 97 : acción mover(5,5) -> Estado/Reward  -15.0\n","  # 98 : acción mover(9,5) -> Estado/Reward  -19.0\n","  # 99 : acción intercambiar(5,0) -> Estado/Reward  -20.0\n","  # 100 : acción intercambiar(4,7) -> Estado/Reward  -19.0\n"," Recompensa Final =  -19.0\n"," Lista Final =  [ 33 -79 -13  -7 -84  19  11 -41   8  21]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 33  -7 -79   8  11 -13  21 -84 -41  19]\n","  # 1 : acción intercambiar(0,8) -> Estado/Reward  -14.0\n","  # 2 : acción mover(7,1) -> Estado/Reward  -8.0\n","  # 3 : acción mover(0,3) -> Estado/Reward  -7.0\n","  # 4 : acción mover(1,2) -> Estado/Reward  -6.0\n","  # 5 : acción intercambiar(2,3) -> Estado/Reward  -5.0\n","  # 6 : acción mover(6,3) -> Estado/Reward  -2.0\n","  # 7 : acción mover(9,6) -> Estado/Reward  -1.0\n","  # 8 : acción intercambiar(6,7) -> Estado/Reward  92.0\n"," Recompensa Final =  92.0\n"," Lista Final =  [-84 -79 -41 -13  -7   8  11  19  21  33]\n","\n","--> El Agente Entrenado ( 92.0 ) genera MEJOR resultado que el azar ( -19.0 )\n","\n","> Prueba  2 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 68  50 -32  -5 -67 -31  97  16]\n","  # 1 : acción mover(6,2) -> Estado/Reward  -19.0\n","  # 2 : acción intercambiar(1,5) -> Estado/Reward  -14.0\n","  # 3 : acción mover(4,0) -> Estado/Reward  -14.0\n","  # 4 : acción intercambiar(4,7) -> Estado/Reward  -17.0\n","  # 5 : acción mover(6,8) -> Estado/Reward  -16.0\n","  # 6 : acción mover(6,3) -> Estado/Reward  -13.0\n","  # 7 : acción intercambiar(5,4) -> Estado/Reward  -12.0\n","  # 8 : acción mover(3,5) -> Estado/Reward  -14.0\n","  # 9 : acción mover(4,4) -> Estado/Reward  -14.0\n","  # 10 : acción mover(3,8) -> Estado/Reward  -14.0\n","  # 11 : acción mover(0,4) -> Estado/Reward  -14.0\n","  # 12 : acción intercambiar(4,3) -> Estado/Reward  -15.0\n","  # 13 : acción mover(0,7) -> Estado/Reward  -10.0\n","  # 14 : acción mover(5,9) -> Estado/Reward  -12.0\n","  # 15 : acción mover(1,6) -> Estado/Reward  -7.0\n","  # 16 : acción intercambiar(8,8) -> Estado/Reward  -7.0\n","  # 17 : acción mover(9,0) -> Estado/Reward  -4.0\n","  # 18 : acción mover(5,9) -> Estado/Reward  -6.0\n","  # 19 : acción mover(9,9) -> Estado/Reward  -6.0\n","  # 20 : acción mover(8,9) -> Estado/Reward  -6.0\n","  # 21 : acción mover(0,7) -> Estado/Reward  -9.0\n","  # 22 : acción mover(6,3) -> Estado/Reward  -6.0\n","  # 23 : acción mover(2,0) -> Estado/Reward  -6.0\n","  # 24 : acción mover(0,8) -> Estado/Reward  -11.0\n","  # 25 : acción mover(8,0) -> Estado/Reward  -6.0\n","  # 26 : acción intercambiar(1,8) -> Estado/Reward  -7.0\n","  # 27 : acción intercambiar(5,3) -> Estado/Reward  -10.0\n","  # 28 : acción mover(5,7) -> Estado/Reward  -10.0\n","  # 29 : acción intercambiar(4,8) -> Estado/Reward  -9.0\n","  # 30 : acción mover(7,5) -> Estado/Reward  -9.0\n","  # 31 : acción intercambiar(8,7) -> Estado/Reward  -9.0\n","  # 32 : acción mover(2,5) -> Estado/Reward  -12.0\n","  # 33 : acción intercambiar(1,9) -> Estado/Reward  -11.0\n","  # 34 : acción mover(0,0) -> Estado/Reward  -11.0\n","  # 35 : acción intercambiar(1,2) -> Estado/Reward  -12.0\n","  # 36 : acción intercambiar(5,5) -> Estado/Reward  -12.0\n","  # 37 : acción mover(1,7) -> Estado/Reward  -8.0\n","  # 38 : acción mover(4,1) -> Estado/Reward  -7.0\n","  # 39 : acción mover(8,7) -> Estado/Reward  -7.0\n","  # 40 : acción mover(6,1) -> Estado/Reward  -4.0\n","  # 41 : acción intercambiar(0,9) -> Estado/Reward  -13.0\n","  # 42 : acción intercambiar(7,6) -> Estado/Reward  -12.0\n","  # 43 : acción intercambiar(5,0) -> Estado/Reward  -11.0\n","  # 44 : acción intercambiar(8,0) -> Estado/Reward  -14.0\n","  # 45 : acción intercambiar(5,3) -> Estado/Reward  -17.0\n","  # 46 : acción intercambiar(1,8) -> Estado/Reward  -22.0\n","  # 47 : acción intercambiar(9,5) -> Estado/Reward  -25.0\n","  # 48 : acción intercambiar(9,3) -> Estado/Reward  -18.0\n","  # 49 : acción intercambiar(9,4) -> Estado/Reward  -19.0\n","  # 50 : acción intercambiar(1,2) -> Estado/Reward  -18.0\n","  # 51 : acción mover(8,0) -> Estado/Reward  -19.0\n","  # 52 : acción mover(2,6) -> Estado/Reward  -19.0\n","  # 53 : acción intercambiar(4,4) -> Estado/Reward  -19.0\n","  # 54 : acción intercambiar(2,8) -> Estado/Reward  -14.0\n","  # 55 : acción intercambiar(3,8) -> Estado/Reward  -19.0\n","  # 56 : acción mover(4,3) -> Estado/Reward  -20.0\n","  # 57 : acción intercambiar(9,0) -> Estado/Reward  -13.0\n","  # 58 : acción intercambiar(3,4) -> Estado/Reward  -12.0\n","  # 59 : acción intercambiar(8,3) -> Estado/Reward  -11.0\n","  # 60 : acción mover(7,0) -> Estado/Reward  -14.0\n","  # 61 : acción mover(2,6) -> Estado/Reward  -10.0\n","  # 62 : acción intercambiar(5,2) -> Estado/Reward  -11.0\n","  # 63 : acción intercambiar(5,3) -> Estado/Reward  -10.0\n","  # 64 : acción intercambiar(9,1) -> Estado/Reward  -15.0\n","  # 65 : acción mover(9,4) -> Estado/Reward  -12.0\n","  # 66 : acción mover(2,9) -> Estado/Reward  -13.0\n","  # 67 : acción intercambiar(4,5) -> Estado/Reward  -12.0\n","  # 68 : acción mover(6,1) -> Estado/Reward  -17.0\n","  # 69 : acción mover(6,1) -> Estado/Reward  -20.0\n","  # 70 : acción intercambiar(8,3) -> Estado/Reward  -19.0\n","  # 71 : acción mover(2,0) -> Estado/Reward  -21.0\n","  # 72 : acción mover(6,6) -> Estado/Reward  -21.0\n","  # 73 : acción mover(8,4) -> Estado/Reward  -22.0\n","  # 74 : acción mover(4,3) -> Estado/Reward  -23.0\n","  # 75 : acción mover(3,9) -> Estado/Reward  -21.0\n","  # 76 : acción intercambiar(6,6) -> Estado/Reward  -21.0\n","  # 77 : acción mover(2,9) -> Estado/Reward  -16.0\n","  # 78 : acción mover(3,0) -> Estado/Reward  -13.0\n","  # 79 : acción mover(6,0) -> Estado/Reward  -13.0\n","  # 80 : acción intercambiar(4,2) -> Estado/Reward  -10.0\n","  # 81 : acción mover(4,4) -> Estado/Reward  -10.0\n","  # 82 : acción mover(4,2) -> Estado/Reward  -12.0\n","  # 83 : acción intercambiar(7,5) -> Estado/Reward  -15.0\n","  # 84 : acción intercambiar(7,1) -> Estado/Reward  -14.0\n","  # 85 : acción intercambiar(0,8) -> Estado/Reward  -11.0\n","  # 86 : acción intercambiar(9,4) -> Estado/Reward  -8.0\n","  # 87 : acción intercambiar(7,7) -> Estado/Reward  -8.0\n","  # 88 : acción mover(0,8) -> Estado/Reward  -13.0\n","  # 89 : acción intercambiar(9,0) -> Estado/Reward  -14.0\n","  # 90 : acción mover(5,9) -> Estado/Reward  -14.0\n","  # 91 : acción mover(4,8) -> Estado/Reward  -11.0\n","  # 92 : acción mover(2,9) -> Estado/Reward  -14.0\n","  # 93 : acción mover(3,4) -> Estado/Reward  -13.0\n","  # 94 : acción mover(7,6) -> Estado/Reward  -12.0\n","  # 95 : acción intercambiar(7,5) -> Estado/Reward  -13.0\n","  # 96 : acción mover(6,5) -> Estado/Reward  -12.0\n","  # 97 : acción mover(5,5) -> Estado/Reward  -12.0\n","  # 98 : acción mover(1,2) -> Estado/Reward  -11.0\n","  # 99 : acción intercambiar(2,5) -> Estado/Reward  -8.0\n","  # 100 : acción mover(2,8) -> Estado/Reward  -11.0\n"," Recompensa Final =  -11.0\n"," Lista Final =  [-32  -5 -67  50  97  68  16 -31]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 68  50 -32  -5 -67 -31  97  16]\n","  # 1 : acción intercambiar(9,0) -> Estado/Reward  -12.0\n","  # 2 : acción intercambiar(5,1) -> Estado/Reward  -9.0\n","  # 3 : acción intercambiar(4,0) -> Estado/Reward  -2.0\n","  # 4 : acción intercambiar(6,7) -> Estado/Reward  -1.0\n","  # 5 : acción intercambiar(1,2) -> Estado/Reward  95.0\n"," Recompensa Final =  95.0\n"," Lista Final =  [-67 -32 -31  -5  16  50  68  97]\n","\n","--> El Agente Entrenado ( 95.0 ) genera MEJOR resultado que el azar ( -11.0 )\n","\n","> Prueba  3 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [-48 -21  86  63   8  58  44  24  28  24]\n","  # 1 : acción mover(4,3) -> Estado/Reward  -20.0\n","  # 2 : acción mover(3,2) -> Estado/Reward  -19.0\n","  # 3 : acción mover(6,1) -> Estado/Reward  -18.0\n","  # 4 : acción intercambiar(7,1) -> Estado/Reward  -17.0\n","  # 5 : acción mover(4,1) -> Estado/Reward  -20.0\n","  # 6 : acción mover(8,7) -> Estado/Reward  -19.0\n","  # 7 : acción mover(6,8) -> Estado/Reward  -17.0\n","  # 8 : acción mover(2,5) -> Estado/Reward  -16.0\n","  # 9 : acción mover(8,4) -> Estado/Reward  -18.0\n","  # 10 : acción mover(2,2) -> Estado/Reward  -18.0\n","  # 11 : acción mover(8,0) -> Estado/Reward  -20.0\n","  # 12 : acción intercambiar(5,4) -> Estado/Reward  -21.0\n","  # 13 : acción intercambiar(2,0) -> Estado/Reward  -22.0\n","  # 14 : acción intercambiar(0,0) -> Estado/Reward  -22.0\n","  # 15 : acción mover(5,9) -> Estado/Reward  -26.0\n","  # 16 : acción mover(7,2) -> Estado/Reward  -25.0\n","  # 17 : acción mover(9,2) -> Estado/Reward  -20.0\n","  # 18 : acción intercambiar(3,5) -> Estado/Reward  -19.0\n","  # 19 : acción intercambiar(4,9) -> Estado/Reward  -15.0\n","  # 20 : acción mover(6,8) -> Estado/Reward  -15.0\n","  # 21 : acción intercambiar(8,0) -> Estado/Reward  -12.0\n","  # 22 : acción intercambiar(5,6) -> Estado/Reward  -13.0\n","  # 23 : acción mover(5,6) -> Estado/Reward  -12.0\n","  # 24 : acción mover(4,7) -> Estado/Reward  -14.0\n","  # 25 : acción mover(0,3) -> Estado/Reward  -11.0\n","  # 26 : acción mover(9,3) -> Estado/Reward  -11.0\n","  # 27 : acción mover(4,8) -> Estado/Reward  -9.0\n","  # 28 : acción intercambiar(3,4) -> Estado/Reward  -8.0\n","  # 29 : acción mover(4,0) -> Estado/Reward  -12.0\n","  # 30 : acción intercambiar(1,4) -> Estado/Reward  -17.0\n","  # 31 : acción intercambiar(8,6) -> Estado/Reward  -19.0\n","  # 32 : acción intercambiar(9,5) -> Estado/Reward  -20.0\n","  # 33 : acción intercambiar(9,3) -> Estado/Reward  -27.0\n","  # 34 : acción intercambiar(7,4) -> Estado/Reward  -28.0\n","  # 35 : acción intercambiar(9,7) -> Estado/Reward  -29.0\n","  # 36 : acción intercambiar(9,7) -> Estado/Reward  -28.0\n","  # 37 : acción intercambiar(7,5) -> Estado/Reward  -25.0\n","  # 38 : acción intercambiar(0,5) -> Estado/Reward  -18.0\n","  # 39 : acción intercambiar(4,7) -> Estado/Reward  -23.0\n","  # 40 : acción intercambiar(4,8) -> Estado/Reward  -17.0\n","  # 41 : acción intercambiar(3,6) -> Estado/Reward  -16.0\n","  # 42 : acción mover(2,0) -> Estado/Reward  -16.0\n","  # 43 : acción mover(2,3) -> Estado/Reward  -17.0\n","  # 44 : acción mover(3,1) -> Estado/Reward  -17.0\n","  # 45 : acción mover(2,7) -> Estado/Reward  -22.0\n","  # 46 : acción intercambiar(1,1) -> Estado/Reward  -22.0\n","  # 47 : acción intercambiar(5,7) -> Estado/Reward  -19.0\n","  # 48 : acción intercambiar(2,4) -> Estado/Reward  -18.0\n","  # 49 : acción mover(5,0) -> Estado/Reward  -13.0\n","  # 50 : acción mover(0,2) -> Estado/Reward  -15.0\n","  # 51 : acción mover(0,3) -> Estado/Reward  -16.0\n","  # 52 : acción intercambiar(9,8) -> Estado/Reward  -15.0\n","  # 53 : acción intercambiar(2,8) -> Estado/Reward  -8.0\n","  # 54 : acción mover(7,5) -> Estado/Reward  -10.0\n","  # 55 : acción intercambiar(1,3) -> Estado/Reward  -13.0\n","  # 56 : acción intercambiar(5,8) -> Estado/Reward  -10.0\n","  # 57 : acción intercambiar(6,6) -> Estado/Reward  -10.0\n","  # 58 : acción mover(7,4) -> Estado/Reward  -8.0\n","  # 59 : acción mover(9,8) -> Estado/Reward  -9.0\n","  # 60 : acción mover(2,8) -> Estado/Reward  -13.0\n","  # 61 : acción mover(3,0) -> Estado/Reward  -14.0\n","  # 62 : acción intercambiar(0,3) -> Estado/Reward  -11.0\n","  # 63 : acción intercambiar(7,9) -> Estado/Reward  -10.0\n","  # 64 : acción mover(5,5) -> Estado/Reward  -10.0\n","  # 65 : acción intercambiar(6,8) -> Estado/Reward  -9.0\n","  # 66 : acción intercambiar(5,4) -> Estado/Reward  -10.0\n","  # 67 : acción mover(9,4) -> Estado/Reward  -15.0\n","  # 68 : acción mover(2,9) -> Estado/Reward  -20.0\n","  # 69 : acción mover(4,4) -> Estado/Reward  -20.0\n","  # 70 : acción intercambiar(9,1) -> Estado/Reward  -15.0\n","  # 71 : acción intercambiar(7,8) -> Estado/Reward  -14.0\n","  # 72 : acción mover(0,4) -> Estado/Reward  -18.0\n","  # 73 : acción intercambiar(5,5) -> Estado/Reward  -18.0\n","  # 74 : acción mover(3,8) -> Estado/Reward  -17.0\n","  # 75 : acción mover(2,3) -> Estado/Reward  -16.0\n","  # 76 : acción mover(0,8) -> Estado/Reward  -20.0\n","  # 77 : acción intercambiar(6,2) -> Estado/Reward  -19.0\n","  # 78 : acción intercambiar(3,2) -> Estado/Reward  -18.0\n","  # 79 : acción intercambiar(3,6) -> Estado/Reward  -19.0\n","  # 80 : acción mover(9,1) -> Estado/Reward  -19.0\n","  # 81 : acción intercambiar(8,4) -> Estado/Reward  -14.0\n","  # 82 : acción intercambiar(2,7) -> Estado/Reward  -23.0\n","  # 83 : acción intercambiar(4,4) -> Estado/Reward  -23.0\n","  # 84 : acción intercambiar(4,9) -> Estado/Reward  -22.0\n","  # 85 : acción mover(6,9) -> Estado/Reward  -21.0\n","  # 86 : acción mover(3,8) -> Estado/Reward  -20.0\n","  # 87 : acción intercambiar(7,0) -> Estado/Reward  -23.0\n","  # 88 : acción mover(0,2) -> Estado/Reward  -23.0\n","  # 89 : acción mover(5,3) -> Estado/Reward  -21.0\n","  # 90 : acción intercambiar(5,8) -> Estado/Reward  -23.0\n","  # 91 : acción intercambiar(8,8) -> Estado/Reward  -23.0\n","  # 92 : acción mover(2,0) -> Estado/Reward  -23.0\n","  # 93 : acción mover(3,8) -> Estado/Reward  -28.0\n","  # 94 : acción mover(2,1) -> Estado/Reward  -29.0\n","  # 95 : acción mover(9,6) -> Estado/Reward  -32.0\n","  # 96 : acción mover(2,9) -> Estado/Reward  -29.0\n","  # 97 : acción mover(8,6) -> Estado/Reward  -27.0\n","  # 98 : acción mover(8,7) -> Estado/Reward  -26.0\n","  # 99 : acción intercambiar(1,6) -> Estado/Reward  -19.0\n","  # 100 : acción mover(2,2) -> Estado/Reward  -19.0\n"," Recompensa Final =  -19.0\n"," Lista Final =  [ 44 -48   8  24  86  58  63 -21  24  28]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [-48 -21  86  63   8  58  44  24  28  24]\n","  # 1 : acción intercambiar(2,7) -> Estado/Reward  -14.0\n","  # 2 : acción intercambiar(9,3) -> Estado/Reward  -7.0\n","  # 3 : acción mover(8,3) -> Estado/Reward  -6.0\n","  # 4 : acción mover(5,1) -> Estado/Reward  -4.0\n","  # 5 : acción intercambiar(1,2) -> Estado/Reward  -3.0\n","  # 6 : acción mover(4,5) -> Estado/Reward  -2.0\n","  # 7 : acción mover(8,9) -> Estado/Reward  -1.0\n","  # 8 : acción intercambiar(7,6) -> Estado/Reward  92.0\n"," Recompensa Final =  92.0\n"," Lista Final =  [-48 -21   8  24  24  28  44  58  63  86]\n","\n","--> El Agente Entrenado ( 92.0 ) genera MEJOR resultado que el azar ( -19.0 )\n","\n","> Prueba  4 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [  2 -71 -76  34 -81  70   7]\n","  # 1 : acción mover(2,8) -> Estado/Reward  -11.0\n","  # 2 : acción mover(9,0) -> Estado/Reward  -7.0\n","  # 3 : acción mover(1,0) -> Estado/Reward  -8.0\n","  # 4 : acción mover(2,4) -> Estado/Reward  -8.0\n","  # 5 : acción intercambiar(4,1) -> Estado/Reward  -9.0\n","  # 6 : acción mover(1,4) -> Estado/Reward  -8.0\n","  # 7 : acción mover(4,6) -> Estado/Reward  -10.0\n","  # 8 : acción intercambiar(6,8) -> Estado/Reward  -10.0\n","  # 9 : acción intercambiar(3,1) -> Estado/Reward  -9.0\n","  # 10 : acción intercambiar(6,5) -> Estado/Reward  -8.0\n","  # 11 : acción mover(4,0) -> Estado/Reward  -12.0\n","  # 12 : acción mover(3,7) -> Estado/Reward  -15.0\n","  # 13 : acción mover(4,9) -> Estado/Reward  -15.0\n","  # 14 : acción mover(1,3) -> Estado/Reward  -15.0\n","  # 15 : acción mover(3,8) -> Estado/Reward  -14.0\n","  # 16 : acción intercambiar(9,2) -> Estado/Reward  -11.0\n","  # 17 : acción intercambiar(4,3) -> Estado/Reward  -10.0\n","  # 18 : acción mover(3,0) -> Estado/Reward  -7.0\n","  # 19 : acción mover(7,8) -> Estado/Reward  -7.0\n","  # 20 : acción intercambiar(5,7) -> Estado/Reward  -8.0\n","  # 21 : acción intercambiar(9,7) -> Estado/Reward  -8.0\n","  # 22 : acción intercambiar(3,1) -> Estado/Reward  -7.0\n","  # 23 : acción mover(7,2) -> Estado/Reward  -5.0\n","  # 24 : acción intercambiar(5,0) -> Estado/Reward  -12.0\n","  # 25 : acción mover(4,6) -> Estado/Reward  -10.0\n","  # 26 : acción mover(0,0) -> Estado/Reward  -10.0\n","  # 27 : acción mover(5,8) -> Estado/Reward  -11.0\n","  # 28 : acción mover(8,1) -> Estado/Reward  -14.0\n","  # 29 : acción mover(6,0) -> Estado/Reward  -20.0\n","  # 30 : acción mover(7,4) -> Estado/Reward  -18.0\n","  # 31 : acción intercambiar(6,2) -> Estado/Reward  -13.0\n","  # 32 : acción mover(3,3) -> Estado/Reward  -13.0\n","  # 33 : acción intercambiar(9,2) -> Estado/Reward  -18.0\n","  # 34 : acción intercambiar(2,3) -> Estado/Reward  -17.0\n","  # 35 : acción mover(5,6) -> Estado/Reward  -16.0\n","  # 36 : acción mover(0,1) -> Estado/Reward  -15.0\n","  # 37 : acción mover(2,8) -> Estado/Reward  -13.0\n","  # 38 : acción intercambiar(6,7) -> Estado/Reward  -13.0\n","  # 39 : acción intercambiar(6,7) -> Estado/Reward  -13.0\n","  # 40 : acción mover(4,1) -> Estado/Reward  -12.0\n","  # 41 : acción mover(7,2) -> Estado/Reward  -12.0\n","  # 42 : acción intercambiar(3,3) -> Estado/Reward  -12.0\n","  # 43 : acción intercambiar(1,6) -> Estado/Reward  -13.0\n","  # 44 : acción intercambiar(4,5) -> Estado/Reward  -12.0\n","  # 45 : acción mover(4,6) -> Estado/Reward  -14.0\n","  # 46 : acción mover(0,5) -> Estado/Reward  -13.0\n","  # 47 : acción intercambiar(9,3) -> Estado/Reward  -8.0\n","  # 48 : acción intercambiar(0,3) -> Estado/Reward  -7.0\n","  # 49 : acción mover(4,8) -> Estado/Reward  -9.0\n","  # 50 : acción mover(1,4) -> Estado/Reward  -10.0\n","  # 51 : acción intercambiar(4,9) -> Estado/Reward  -9.0\n","  # 52 : acción mover(6,2) -> Estado/Reward  -9.0\n","  # 53 : acción mover(3,0) -> Estado/Reward  -8.0\n","  # 54 : acción intercambiar(6,4) -> Estado/Reward  -9.0\n","  # 55 : acción mover(3,2) -> Estado/Reward  -8.0\n","  # 56 : acción mover(5,6) -> Estado/Reward  -9.0\n","  # 57 : acción intercambiar(3,7) -> Estado/Reward  -4.0\n","  # 58 : acción intercambiar(3,7) -> Estado/Reward  -9.0\n","  # 59 : acción intercambiar(8,1) -> Estado/Reward  -10.0\n","  # 60 : acción mover(7,5) -> Estado/Reward  -9.0\n","  # 61 : acción intercambiar(3,5) -> Estado/Reward  -6.0\n","  # 62 : acción mover(7,2) -> Estado/Reward  -6.0\n","  # 63 : acción intercambiar(2,2) -> Estado/Reward  -6.0\n","  # 64 : acción intercambiar(2,8) -> Estado/Reward  -9.0\n","  # 65 : acción mover(3,1) -> Estado/Reward  -9.0\n","  # 66 : acción mover(6,5) -> Estado/Reward  -8.0\n","  # 67 : acción intercambiar(7,9) -> Estado/Reward  -8.0\n","  # 68 : acción intercambiar(2,1) -> Estado/Reward  -7.0\n","  # 69 : acción mover(6,0) -> Estado/Reward  -11.0\n","  # 70 : acción intercambiar(6,5) -> Estado/Reward  -12.0\n","  # 71 : acción mover(9,8) -> Estado/Reward  -12.0\n","  # 72 : acción intercambiar(6,1) -> Estado/Reward  -9.0\n","  # 73 : acción mover(7,0) -> Estado/Reward  -7.0\n","  # 74 : acción intercambiar(6,5) -> Estado/Reward  -6.0\n","  # 75 : acción mover(0,1) -> Estado/Reward  -7.0\n","  # 76 : acción mover(0,2) -> Estado/Reward  -5.0\n","  # 77 : acción mover(7,6) -> Estado/Reward  -5.0\n","  # 78 : acción intercambiar(5,9) -> Estado/Reward  -6.0\n","  # 79 : acción intercambiar(5,6) -> Estado/Reward  -5.0\n","  # 80 : acción mover(1,9) -> Estado/Reward  -10.0\n","  # 81 : acción mover(7,1) -> Estado/Reward  -5.0\n","  # 82 : acción intercambiar(7,7) -> Estado/Reward  -5.0\n","  # 83 : acción mover(8,6) -> Estado/Reward  -5.0\n","  # 84 : acción intercambiar(8,7) -> Estado/Reward  -5.0\n","  # 85 : acción mover(5,9) -> Estado/Reward  -6.0\n","  # 86 : acción intercambiar(0,3) -> Estado/Reward  -5.0\n","  # 87 : acción intercambiar(3,1) -> Estado/Reward  -6.0\n","  # 88 : acción mover(1,5) -> Estado/Reward  -8.0\n","  # 89 : acción intercambiar(0,7) -> Estado/Reward  -13.0\n","  # 90 : acción intercambiar(6,9) -> Estado/Reward  -13.0\n","  # 91 : acción intercambiar(2,6) -> Estado/Reward  -14.0\n","  # 92 : acción intercambiar(9,4) -> Estado/Reward  -11.0\n","  # 93 : acción mover(0,9) -> Estado/Reward  -9.0\n","  # 94 : acción mover(4,8) -> Estado/Reward  -11.0\n","  # 95 : acción intercambiar(9,4) -> Estado/Reward  -8.0\n","  # 96 : acción mover(8,2) -> Estado/Reward  -12.0\n","  # 97 : acción intercambiar(5,9) -> Estado/Reward  -13.0\n","  # 98 : acción intercambiar(2,6) -> Estado/Reward  -8.0\n","  # 99 : acción mover(3,3) -> Estado/Reward  -8.0\n","  # 100 : acción mover(0,3) -> Estado/Reward  -5.0\n"," Recompensa Final =  -5.0\n"," Lista Final =  [-76 -71   2  34 -81   7  70]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [  2 -71 -76  34 -81  70   7]\n","  # 1 : acción intercambiar(4,0) -> Estado/Reward  -4.0\n","  # 2 : acción mover(1,2) -> Estado/Reward  -3.0\n","  # 3 : acción mover(1,2) -> Estado/Reward  -4.0\n","  # 4 : acción mover(1,2) -> Estado/Reward  -3.0\n","  # 5 : acción mover(1,2) -> Estado/Reward  -4.0\n","  # 6 : acción mover(1,2) -> Estado/Reward  -3.0\n"," Recompensa Final =  -3.0\n"," Lista Final =  [-81 -76 -71  34   2  70   7]\n","\n","--> El Agente Entrenado ( -3.0 ) genera MEJOR resultado que el azar ( -5.0 )\n","\n","> Prueba  5 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [  9  73  87  75 -79 -45  12  93  60  33]\n","  # 1 : acción mover(0,4) -> Estado/Reward  -23.0\n","  # 2 : acción mover(1,5) -> Estado/Reward  -19.0\n","  # 3 : acción mover(1,9) -> Estado/Reward  -15.0\n","  # 4 : acción mover(9,7) -> Estado/Reward  -17.0\n","  # 5 : acción intercambiar(7,1) -> Estado/Reward  -24.0\n","  # 6 : acción mover(6,7) -> Estado/Reward  -23.0\n","  # 7 : acción mover(8,8) -> Estado/Reward  -23.0\n","  # 8 : acción mover(5,7) -> Estado/Reward  -23.0\n","  # 9 : acción intercambiar(4,9) -> Estado/Reward  -20.0\n","  # 10 : acción intercambiar(2,4) -> Estado/Reward  -21.0\n","  # 11 : acción mover(1,4) -> Estado/Reward  -18.0\n","  # 12 : acción mover(7,4) -> Estado/Reward  -17.0\n","  # 13 : acción mover(2,4) -> Estado/Reward  -19.0\n","  # 14 : acción intercambiar(7,4) -> Estado/Reward  -22.0\n","  # 15 : acción intercambiar(0,3) -> Estado/Reward  -19.0\n","  # 16 : acción mover(8,9) -> Estado/Reward  -20.0\n","  # 17 : acción mover(7,6) -> Estado/Reward  -21.0\n","  # 18 : acción mover(1,1) -> Estado/Reward  -21.0\n","  # 19 : acción mover(1,0) -> Estado/Reward  -22.0\n","  # 20 : acción intercambiar(8,5) -> Estado/Reward  -23.0\n","  # 21 : acción mover(0,7) -> Estado/Reward  -22.0\n","  # 22 : acción intercambiar(1,6) -> Estado/Reward  -19.0\n","  # 23 : acción mover(1,9) -> Estado/Reward  -27.0\n","  # 24 : acción mover(9,0) -> Estado/Reward  -18.0\n","  # 25 : acción mover(1,5) -> Estado/Reward  -20.0\n","  # 26 : acción mover(8,6) -> Estado/Reward  -22.0\n","  # 27 : acción mover(4,4) -> Estado/Reward  -22.0\n","  # 28 : acción mover(5,8) -> Estado/Reward  -23.0\n","  # 29 : acción mover(2,4) -> Estado/Reward  -21.0\n","  # 30 : acción mover(5,8) -> Estado/Reward  -18.0\n","  # 31 : acción intercambiar(5,8) -> Estado/Reward  -23.0\n","  # 32 : acción mover(9,2) -> Estado/Reward  -24.0\n","  # 33 : acción mover(7,6) -> Estado/Reward  -23.0\n","  # 34 : acción mover(1,8) -> Estado/Reward  -22.0\n","  # 35 : acción intercambiar(6,2) -> Estado/Reward  -21.0\n","  # 36 : acción intercambiar(3,2) -> Estado/Reward  -20.0\n","  # 37 : acción intercambiar(4,9) -> Estado/Reward  -11.0\n","  # 38 : acción intercambiar(4,5) -> Estado/Reward  -12.0\n","  # 39 : acción mover(8,6) -> Estado/Reward  -12.0\n","  # 40 : acción mover(1,7) -> Estado/Reward  -12.0\n","  # 41 : acción mover(8,7) -> Estado/Reward  -11.0\n","  # 42 : acción mover(4,0) -> Estado/Reward  -11.0\n","  # 43 : acción mover(5,3) -> Estado/Reward  -11.0\n","  # 44 : acción mover(1,5) -> Estado/Reward  -15.0\n","  # 45 : acción intercambiar(9,4) -> Estado/Reward  -20.0\n","  # 46 : acción mover(2,8) -> Estado/Reward  -20.0\n","  # 47 : acción intercambiar(6,3) -> Estado/Reward  -17.0\n","  # 48 : acción mover(9,6) -> Estado/Reward  -14.0\n","  # 49 : acción intercambiar(3,5) -> Estado/Reward  -15.0\n","  # 50 : acción intercambiar(2,6) -> Estado/Reward  -14.0\n","  # 51 : acción mover(0,8) -> Estado/Reward  -18.0\n","  # 52 : acción mover(1,5) -> Estado/Reward  -18.0\n","  # 53 : acción intercambiar(4,4) -> Estado/Reward  -18.0\n","  # 54 : acción mover(9,2) -> Estado/Reward  -21.0\n","  # 55 : acción mover(4,9) -> Estado/Reward  -24.0\n","  # 56 : acción mover(4,1) -> Estado/Reward  -25.0\n","  # 57 : acción mover(8,9) -> Estado/Reward  -26.0\n","  # 58 : acción intercambiar(3,7) -> Estado/Reward  -25.0\n","  # 59 : acción mover(2,8) -> Estado/Reward  -21.0\n","  # 60 : acción intercambiar(6,6) -> Estado/Reward  -21.0\n","  # 61 : acción mover(8,1) -> Estado/Reward  -26.0\n","  # 62 : acción intercambiar(4,6) -> Estado/Reward  -29.0\n","  # 63 : acción mover(9,3) -> Estado/Reward  -25.0\n","  # 64 : acción mover(3,3) -> Estado/Reward  -25.0\n","  # 65 : acción mover(3,6) -> Estado/Reward  -28.0\n","  # 66 : acción mover(7,7) -> Estado/Reward  -28.0\n","  # 67 : acción intercambiar(6,7) -> Estado/Reward  -27.0\n","  # 68 : acción mover(2,1) -> Estado/Reward  -26.0\n","  # 69 : acción intercambiar(3,1) -> Estado/Reward  -25.0\n","  # 70 : acción intercambiar(1,8) -> Estado/Reward  -26.0\n","  # 71 : acción mover(9,2) -> Estado/Reward  -23.0\n","  # 72 : acción mover(9,7) -> Estado/Reward  -25.0\n","  # 73 : acción intercambiar(1,7) -> Estado/Reward  -24.0\n","  # 74 : acción mover(4,3) -> Estado/Reward  -23.0\n","  # 75 : acción intercambiar(2,2) -> Estado/Reward  -23.0\n","  # 76 : acción mover(5,6) -> Estado/Reward  -22.0\n","  # 77 : acción intercambiar(3,9) -> Estado/Reward  -17.0\n","  # 78 : acción intercambiar(8,6) -> Estado/Reward  -14.0\n","  # 79 : acción mover(4,1) -> Estado/Reward  -17.0\n","  # 80 : acción intercambiar(0,2) -> Estado/Reward  -18.0\n","  # 81 : acción mover(6,4) -> Estado/Reward  -16.0\n","  # 82 : acción intercambiar(3,7) -> Estado/Reward  -19.0\n","  # 83 : acción mover(3,7) -> Estado/Reward  -15.0\n","  # 84 : acción intercambiar(2,4) -> Estado/Reward  -16.0\n","  # 85 : acción intercambiar(8,6) -> Estado/Reward  -19.0\n","  # 86 : acción intercambiar(8,8) -> Estado/Reward  -19.0\n","  # 87 : acción mover(1,5) -> Estado/Reward  -15.0\n","  # 88 : acción mover(1,6) -> Estado/Reward  -16.0\n","  # 89 : acción mover(9,0) -> Estado/Reward  -21.0\n","  # 90 : acción mover(1,4) -> Estado/Reward  -18.0\n","  # 91 : acción mover(5,8) -> Estado/Reward  -17.0\n","  # 92 : acción mover(1,3) -> Estado/Reward  -19.0\n","  # 93 : acción mover(1,0) -> Estado/Reward  -18.0\n","  # 94 : acción mover(4,2) -> Estado/Reward  -20.0\n","  # 95 : acción mover(9,3) -> Estado/Reward  -18.0\n","  # 96 : acción mover(9,5) -> Estado/Reward  -20.0\n","  # 97 : acción mover(5,3) -> Estado/Reward  -22.0\n","  # 98 : acción mover(8,2) -> Estado/Reward  -18.0\n","  # 99 : acción mover(3,7) -> Estado/Reward  -16.0\n","  # 100 : acción mover(3,3) -> Estado/Reward  -16.0\n"," Recompensa Final =  -16.0\n"," Lista Final =  [-45  75   9  87  12  33 -79  60  93  73]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [  9  73  87  75 -79 -45  12  93  60  33]\n","  # 1 : acción intercambiar(2,9) -> Estado/Reward  -16.0\n","  # 2 : acción mover(3,8) -> Estado/Reward  -13.0\n","  # 3 : acción intercambiar(9,1) -> Estado/Reward  -16.0\n","  # 4 : acción intercambiar(9,1) -> Estado/Reward  -13.0\n","  # 5 : acción intercambiar(9,1) -> Estado/Reward  -16.0\n","  # 6 : acción intercambiar(9,1) -> Estado/Reward  -13.0\n","  # 7 : acción intercambiar(9,1) -> Estado/Reward  -16.0\n"," Recompensa Final =  -16.0\n"," Lista Final =  [  9  87  33 -79 -45  12  93  60  75  73]\n","\n","--> El Agente Entrenado ( -16.0 ) genera PEOR resultado que el azar ( -16.0 )\n","\n","> Prueba  6 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 22 -50 -83  87  19   9  24 -25]\n","  # 1 : acción mover(9,9) -> Estado/Reward  -14.0\n","  # 2 : acción intercambiar(3,2) -> Estado/Reward  -15.0\n","  # 3 : acción mover(1,5) -> Estado/Reward  -17.0\n","  # 4 : acción mover(8,6) -> Estado/Reward  -16.0\n","  # 5 : acción intercambiar(4,6) -> Estado/Reward  -15.0\n","  # 6 : acción intercambiar(9,4) -> Estado/Reward  -18.0\n","  # 7 : acción mover(4,9) -> Estado/Reward  -15.0\n","  # 8 : acción mover(8,8) -> Estado/Reward  -15.0\n","  # 9 : acción mover(5,2) -> Estado/Reward  -16.0\n","  # 10 : acción mover(8,2) -> Estado/Reward  -21.0\n","  # 11 : acción intercambiar(1,7) -> Estado/Reward  -14.0\n","  # 12 : acción intercambiar(2,4) -> Estado/Reward  -11.0\n","  # 13 : acción mover(0,7) -> Estado/Reward  -8.0\n","  # 14 : acción mover(7,7) -> Estado/Reward  -8.0\n","  # 15 : acción mover(3,7) -> Estado/Reward  -6.0\n","  # 16 : acción intercambiar(0,2) -> Estado/Reward  -7.0\n","  # 17 : acción intercambiar(4,3) -> Estado/Reward  -6.0\n","  # 18 : acción mover(0,3) -> Estado/Reward  -3.0\n","  # 19 : acción intercambiar(2,2) -> Estado/Reward  -3.0\n","  # 20 : acción intercambiar(5,4) -> Estado/Reward  -4.0\n","  # 21 : acción mover(6,5) -> Estado/Reward  -5.0\n","  # 22 : acción intercambiar(0,7) -> Estado/Reward  -16.0\n","  # 23 : acción mover(6,7) -> Estado/Reward  -15.0\n","  # 24 : acción mover(0,7) -> Estado/Reward  -10.0\n","  # 25 : acción intercambiar(3,0) -> Estado/Reward  -13.0\n","  # 26 : acción mover(6,6) -> Estado/Reward  -13.0\n","  # 27 : acción intercambiar(9,2) -> Estado/Reward  -18.0\n","  # 28 : acción mover(9,0) -> Estado/Reward  -17.0\n","  # 29 : acción intercambiar(7,8) -> Estado/Reward  -17.0\n","  # 30 : acción intercambiar(4,0) -> Estado/Reward  -16.0\n","  # 31 : acción mover(4,9) -> Estado/Reward  -17.0\n","  # 32 : acción mover(7,6) -> Estado/Reward  -16.0\n","  # 33 : acción mover(1,4) -> Estado/Reward  -13.0\n","  # 34 : acción intercambiar(3,2) -> Estado/Reward  -12.0\n","  # 35 : acción intercambiar(0,0) -> Estado/Reward  -12.0\n","  # 36 : acción mover(0,5) -> Estado/Reward  -13.0\n","  # 37 : acción mover(3,2) -> Estado/Reward  -14.0\n","  # 38 : acción intercambiar(5,1) -> Estado/Reward  -13.0\n","  # 39 : acción intercambiar(0,9) -> Estado/Reward  -18.0\n","  # 40 : acción mover(6,0) -> Estado/Reward  -16.0\n","  # 41 : acción intercambiar(6,3) -> Estado/Reward  -13.0\n","  # 42 : acción mover(7,4) -> Estado/Reward  -12.0\n","  # 43 : acción mover(6,6) -> Estado/Reward  -12.0\n","  # 44 : acción intercambiar(5,8) -> Estado/Reward  -13.0\n","  # 45 : acción intercambiar(3,1) -> Estado/Reward  -14.0\n","  # 46 : acción mover(7,5) -> Estado/Reward  -14.0\n","  # 47 : acción intercambiar(7,0) -> Estado/Reward  -9.0\n","  # 48 : acción intercambiar(7,9) -> Estado/Reward  -9.0\n","  # 49 : acción intercambiar(9,4) -> Estado/Reward  -10.0\n","  # 50 : acción intercambiar(8,0) -> Estado/Reward  -11.0\n","  # 51 : acción mover(9,3) -> Estado/Reward  -7.0\n","  # 52 : acción mover(0,9) -> Estado/Reward  -12.0\n","  # 53 : acción intercambiar(0,4) -> Estado/Reward  -9.0\n","  # 54 : acción mover(0,8) -> Estado/Reward  -10.0\n","  # 55 : acción mover(9,0) -> Estado/Reward  -9.0\n","  # 56 : acción intercambiar(9,2) -> Estado/Reward  -10.0\n","  # 57 : acción mover(2,3) -> Estado/Reward  -11.0\n","  # 58 : acción intercambiar(6,3) -> Estado/Reward  -16.0\n","  # 59 : acción mover(6,6) -> Estado/Reward  -16.0\n","  # 60 : acción mover(3,0) -> Estado/Reward  -19.0\n","  # 61 : acción intercambiar(5,9) -> Estado/Reward  -16.0\n","  # 62 : acción mover(0,9) -> Estado/Reward  -9.0\n","  # 63 : acción mover(2,4) -> Estado/Reward  -9.0\n","  # 64 : acción intercambiar(1,3) -> Estado/Reward  -8.0\n","  # 65 : acción mover(5,5) -> Estado/Reward  -8.0\n","  # 66 : acción intercambiar(0,0) -> Estado/Reward  -8.0\n","  # 67 : acción mover(0,0) -> Estado/Reward  -8.0\n","  # 68 : acción mover(5,8) -> Estado/Reward  -10.0\n","  # 69 : acción intercambiar(3,7) -> Estado/Reward  -9.0\n","  # 70 : acción mover(5,1) -> Estado/Reward  -13.0\n","  # 71 : acción intercambiar(7,7) -> Estado/Reward  -13.0\n","  # 72 : acción intercambiar(4,2) -> Estado/Reward  -14.0\n","  # 73 : acción intercambiar(5,9) -> Estado/Reward  -13.0\n","  # 74 : acción intercambiar(1,0) -> Estado/Reward  -14.0\n","  # 75 : acción mover(1,6) -> Estado/Reward  -13.0\n","  # 76 : acción mover(0,6) -> Estado/Reward  -9.0\n","  # 77 : acción intercambiar(0,2) -> Estado/Reward  -8.0\n","  # 78 : acción mover(4,5) -> Estado/Reward  -7.0\n","  # 79 : acción intercambiar(4,9) -> Estado/Reward  -8.0\n","  # 80 : acción intercambiar(1,4) -> Estado/Reward  -7.0\n","  # 81 : acción intercambiar(7,6) -> Estado/Reward  -6.0\n","  # 82 : acción mover(6,6) -> Estado/Reward  -6.0\n","  # 83 : acción intercambiar(4,9) -> Estado/Reward  -7.0\n","  # 84 : acción mover(0,1) -> Estado/Reward  -8.0\n","  # 85 : acción intercambiar(5,2) -> Estado/Reward  -13.0\n","  # 86 : acción intercambiar(3,2) -> Estado/Reward  -12.0\n","  # 87 : acción intercambiar(6,8) -> Estado/Reward  -13.0\n","  # 88 : acción intercambiar(8,1) -> Estado/Reward  -18.0\n","  # 89 : acción intercambiar(5,1) -> Estado/Reward  -15.0\n","  # 90 : acción intercambiar(1,1) -> Estado/Reward  -15.0\n","  # 91 : acción intercambiar(9,2) -> Estado/Reward  -14.0\n","  # 92 : acción mover(4,5) -> Estado/Reward  -13.0\n","  # 93 : acción intercambiar(9,1) -> Estado/Reward  -14.0\n","  # 94 : acción mover(6,6) -> Estado/Reward  -14.0\n","  # 95 : acción intercambiar(9,5) -> Estado/Reward  -11.0\n","  # 96 : acción intercambiar(3,1) -> Estado/Reward  -12.0\n","  # 97 : acción intercambiar(0,7) -> Estado/Reward  -15.0\n","  # 98 : acción intercambiar(7,7) -> Estado/Reward  -15.0\n","  # 99 : acción mover(1,0) -> Estado/Reward  -16.0\n","  # 100 : acción mover(3,4) -> Estado/Reward  -17.0\n"," Recompensa Final =  -17.0\n"," Lista Final =  [ 87  24 -83   9 -25 -50  22  19]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 22 -50 -83  87  19   9  24 -25]\n","  # 1 : acción mover(7,0) -> Estado/Reward  -11.0\n","  # 2 : acción mover(4,7) -> Estado/Reward  -8.0\n","  # 3 : acción intercambiar(3,1) -> Estado/Reward  -5.0\n","  # 4 : acción mover(0,3) -> Estado/Reward  -4.0\n","  # 5 : acción intercambiar(2,3) -> Estado/Reward  -3.0\n","  # 6 : acción intercambiar(3,4) -> Estado/Reward  -2.0\n","  # 7 : acción intercambiar(3,4) -> Estado/Reward  -3.0\n","  # 8 : acción intercambiar(3,4) -> Estado/Reward  -2.0\n","  # 9 : acción intercambiar(3,4) -> Estado/Reward  -3.0\n","  # 10 : acción intercambiar(3,4) -> Estado/Reward  -2.0\n"," Recompensa Final =  -2.0\n"," Lista Final =  [-83 -50 -25  19  22   9  24  87]\n","\n","--> El Agente Entrenado ( -2.0 ) genera MEJOR resultado que el azar ( -17.0 )\n","\n","> Prueba  7 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [-68  53 -28 -26  33 -53 -45]\n","  # 1 : acción intercambiar(5,3) -> Estado/Reward  -10.0\n","  # 2 : acción mover(4,2) -> Estado/Reward  -12.0\n","  # 3 : acción intercambiar(0,2) -> Estado/Reward  -13.0\n","  # 4 : acción mover(8,5) -> Estado/Reward  -12.0\n","  # 5 : acción mover(9,4) -> Estado/Reward  -14.0\n","  # 6 : acción intercambiar(8,3) -> Estado/Reward  -13.0\n","  # 7 : acción mover(3,1) -> Estado/Reward  -13.0\n","  # 8 : acción intercambiar(4,7) -> Estado/Reward  -12.0\n","  # 9 : acción mover(1,3) -> Estado/Reward  -12.0\n","  # 10 : acción intercambiar(0,4) -> Estado/Reward  -11.0\n","  # 11 : acción intercambiar(6,2) -> Estado/Reward  -16.0\n","  # 12 : acción mover(4,0) -> Estado/Reward  -18.0\n","  # 13 : acción intercambiar(9,5) -> Estado/Reward  -17.0\n","  # 14 : acción intercambiar(0,5) -> Estado/Reward  -10.0\n","  # 15 : acción intercambiar(2,4) -> Estado/Reward  -7.0\n","  # 16 : acción mover(4,5) -> Estado/Reward  -6.0\n","  # 17 : acción mover(5,6) -> Estado/Reward  -5.0\n","  # 18 : acción mover(1,5) -> Estado/Reward  -5.0\n","  # 19 : acción mover(7,7) -> Estado/Reward  -5.0\n","  # 20 : acción mover(1,0) -> Estado/Reward  -6.0\n","  # 21 : acción mover(7,9) -> Estado/Reward  -6.0\n","  # 22 : acción mover(7,7) -> Estado/Reward  -6.0\n","  # 23 : acción mover(4,8) -> Estado/Reward  -8.0\n","  # 24 : acción mover(9,8) -> Estado/Reward  -8.0\n","  # 25 : acción intercambiar(4,2) -> Estado/Reward  -7.0\n","  # 26 : acción intercambiar(9,3) -> Estado/Reward  -4.0\n","  # 27 : acción mover(3,4) -> Estado/Reward  -5.0\n","  # 28 : acción intercambiar(9,5) -> Estado/Reward  -4.0\n","  # 29 : acción mover(0,3) -> Estado/Reward  -5.0\n","  # 30 : acción intercambiar(2,3) -> Estado/Reward  -4.0\n","  # 31 : acción mover(9,8) -> Estado/Reward  -4.0\n","  # 32 : acción mover(6,6) -> Estado/Reward  -4.0\n","  # 33 : acción intercambiar(5,3) -> Estado/Reward  -5.0\n","  # 34 : acción intercambiar(3,9) -> Estado/Reward  -6.0\n","  # 35 : acción mover(7,2) -> Estado/Reward  -8.0\n","  # 36 : acción intercambiar(2,6) -> Estado/Reward  -7.0\n","  # 37 : acción intercambiar(7,1) -> Estado/Reward  -10.0\n","  # 38 : acción mover(0,9) -> Estado/Reward  -16.0\n","  # 39 : acción intercambiar(5,8) -> Estado/Reward  -15.0\n","  # 40 : acción mover(3,4) -> Estado/Reward  -14.0\n","  # 41 : acción intercambiar(0,4) -> Estado/Reward  -15.0\n","  # 42 : acción intercambiar(4,2) -> Estado/Reward  -16.0\n","  # 43 : acción mover(0,0) -> Estado/Reward  -16.0\n","  # 44 : acción mover(3,3) -> Estado/Reward  -16.0\n","  # 45 : acción intercambiar(3,9) -> Estado/Reward  -19.0\n","  # 46 : acción intercambiar(9,9) -> Estado/Reward  -19.0\n","  # 47 : acción intercambiar(9,1) -> Estado/Reward  -14.0\n","  # 48 : acción mover(9,8) -> Estado/Reward  -14.0\n","  # 49 : acción mover(5,3) -> Estado/Reward  -12.0\n","  # 50 : acción mover(7,9) -> Estado/Reward  -12.0\n","  # 51 : acción mover(8,7) -> Estado/Reward  -12.0\n","  # 52 : acción intercambiar(9,8) -> Estado/Reward  -12.0\n","  # 53 : acción intercambiar(8,3) -> Estado/Reward  -17.0\n","  # 54 : acción intercambiar(7,2) -> Estado/Reward  -10.0\n","  # 55 : acción intercambiar(3,8) -> Estado/Reward  -11.0\n","  # 56 : acción intercambiar(2,8) -> Estado/Reward  -16.0\n","  # 57 : acción intercambiar(3,2) -> Estado/Reward  -17.0\n","  # 58 : acción intercambiar(8,7) -> Estado/Reward  -17.0\n","  # 59 : acción mover(9,6) -> Estado/Reward  -17.0\n","  # 60 : acción mover(1,2) -> Estado/Reward  -18.0\n","  # 61 : acción intercambiar(2,9) -> Estado/Reward  -17.0\n","  # 62 : acción mover(3,3) -> Estado/Reward  -17.0\n","  # 63 : acción mover(6,6) -> Estado/Reward  -17.0\n","  # 64 : acción mover(3,6) -> Estado/Reward  -14.0\n","  # 65 : acción intercambiar(5,9) -> Estado/Reward  -15.0\n","  # 66 : acción mover(2,3) -> Estado/Reward  -16.0\n","  # 67 : acción intercambiar(0,2) -> Estado/Reward  -13.0\n","  # 68 : acción intercambiar(6,5) -> Estado/Reward  -12.0\n","  # 69 : acción mover(2,0) -> Estado/Reward  -14.0\n","  # 70 : acción mover(0,1) -> Estado/Reward  -13.0\n","  # 71 : acción mover(0,7) -> Estado/Reward  -13.0\n","  # 72 : acción intercambiar(1,9) -> Estado/Reward  -10.0\n","  # 73 : acción mover(1,5) -> Estado/Reward  -8.0\n","  # 74 : acción mover(8,0) -> Estado/Reward  -12.0\n","  # 75 : acción intercambiar(8,1) -> Estado/Reward  -9.0\n","  # 76 : acción mover(1,1) -> Estado/Reward  -9.0\n","  # 77 : acción mover(5,7) -> Estado/Reward  -10.0\n","  # 78 : acción intercambiar(1,3) -> Estado/Reward  -9.0\n","  # 79 : acción mover(9,5) -> Estado/Reward  -8.0\n","  # 80 : acción mover(4,3) -> Estado/Reward  -7.0\n","  # 81 : acción mover(9,7) -> Estado/Reward  -7.0\n","  # 82 : acción mover(5,6) -> Estado/Reward  -8.0\n","  # 83 : acción intercambiar(6,1) -> Estado/Reward  -11.0\n","  # 84 : acción intercambiar(2,6) -> Estado/Reward  -14.0\n","  # 85 : acción mover(4,8) -> Estado/Reward  -14.0\n","  # 86 : acción mover(0,9) -> Estado/Reward  -10.0\n","  # 87 : acción mover(1,4) -> Estado/Reward  -9.0\n","  # 88 : acción mover(9,0) -> Estado/Reward  -13.0\n","  # 89 : acción intercambiar(9,7) -> Estado/Reward  -13.0\n","  # 90 : acción mover(1,2) -> Estado/Reward  -12.0\n","  # 91 : acción intercambiar(3,0) -> Estado/Reward  -13.0\n","  # 92 : acción mover(9,5) -> Estado/Reward  -14.0\n","  # 93 : acción mover(0,8) -> Estado/Reward  -8.0\n","  # 94 : acción mover(5,5) -> Estado/Reward  -8.0\n","  # 95 : acción mover(3,3) -> Estado/Reward  -8.0\n","  # 96 : acción intercambiar(2,2) -> Estado/Reward  -8.0\n","  # 97 : acción intercambiar(8,4) -> Estado/Reward  -9.0\n","  # 98 : acción mover(0,0) -> Estado/Reward  -9.0\n","  # 99 : acción mover(2,8) -> Estado/Reward  -7.0\n","  # 100 : acción mover(8,5) -> Estado/Reward  -8.0\n"," Recompensa Final =  -8.0\n"," Lista Final =  [-53 -26 -68  53 -45  33 -28]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [-68  53 -28 -26  33 -53 -45]\n","  # 1 : acción intercambiar(6,1) -> Estado/Reward  -4.0\n","  # 2 : acción intercambiar(5,4) -> Estado/Reward  -3.0\n","  # 3 : acción intercambiar(4,2) -> Estado/Reward  -2.0\n","  # 4 : acción intercambiar(1,2) -> Estado/Reward  -1.0\n","  # 5 : acción intercambiar(3,4) -> Estado/Reward  95.0\n"," Recompensa Final =  95.0\n"," Lista Final =  [-68 -53 -45 -28 -26  33  53]\n","\n","--> El Agente Entrenado ( 95.0 ) genera MEJOR resultado que el azar ( -8.0 )\n","\n","> Prueba  8 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 99  51 -66  19 -43  13  32]\n","  # 1 : acción intercambiar(1,4) -> Estado/Reward  -10.0\n","  # 2 : acción mover(7,5) -> Estado/Reward  -11.0\n","  # 3 : acción mover(7,6) -> Estado/Reward  -11.0\n","  # 4 : acción mover(1,5) -> Estado/Reward  -13.0\n","  # 5 : acción intercambiar(8,7) -> Estado/Reward  -13.0\n","  # 6 : acción mover(9,8) -> Estado/Reward  -13.0\n","  # 7 : acción intercambiar(1,8) -> Estado/Reward  -16.0\n","  # 8 : acción mover(4,3) -> Estado/Reward  -15.0\n","  # 9 : acción mover(1,6) -> Estado/Reward  -16.0\n","  # 10 : acción mover(0,5) -> Estado/Reward  -11.0\n","  # 11 : acción mover(9,5) -> Estado/Reward  -10.0\n","  # 12 : acción mover(0,3) -> Estado/Reward  -11.0\n","  # 13 : acción mover(7,8) -> Estado/Reward  -11.0\n","  # 14 : acción mover(5,9) -> Estado/Reward  -12.0\n","  # 15 : acción mover(6,7) -> Estado/Reward  -12.0\n","  # 16 : acción intercambiar(1,5) -> Estado/Reward  -13.0\n","  # 17 : acción intercambiar(0,8) -> Estado/Reward  -10.0\n","  # 18 : acción intercambiar(9,4) -> Estado/Reward  -11.0\n","  # 19 : acción intercambiar(3,0) -> Estado/Reward  -12.0\n","  # 20 : acción intercambiar(1,3) -> Estado/Reward  -11.0\n","  # 21 : acción mover(5,2) -> Estado/Reward  -12.0\n","  # 22 : acción intercambiar(0,3) -> Estado/Reward  -9.0\n","  # 23 : acción mover(7,7) -> Estado/Reward  -9.0\n","  # 24 : acción intercambiar(8,7) -> Estado/Reward  -9.0\n","  # 25 : acción mover(9,6) -> Estado/Reward  -9.0\n","  # 26 : acción mover(9,0) -> Estado/Reward  -3.0\n","  # 27 : acción mover(8,6) -> Estado/Reward  -3.0\n","  # 28 : acción mover(1,9) -> Estado/Reward  -8.0\n","  # 29 : acción mover(6,3) -> Estado/Reward  -5.0\n","  # 30 : acción intercambiar(5,4) -> Estado/Reward  -6.0\n","  # 31 : acción mover(4,2) -> Estado/Reward  -8.0\n","  # 32 : acción intercambiar(6,6) -> Estado/Reward  -8.0\n","  # 33 : acción intercambiar(7,1) -> Estado/Reward  -11.0\n","  # 34 : acción intercambiar(5,9) -> Estado/Reward  -10.0\n","  # 35 : acción intercambiar(7,0) -> Estado/Reward  -15.0\n","  # 36 : acción intercambiar(6,6) -> Estado/Reward  -15.0\n","  # 37 : acción mover(6,6) -> Estado/Reward  -15.0\n","  # 38 : acción intercambiar(4,3) -> Estado/Reward  -14.0\n","  # 39 : acción intercambiar(1,2) -> Estado/Reward  -15.0\n","  # 40 : acción mover(0,2) -> Estado/Reward  -17.0\n","  # 41 : acción mover(6,4) -> Estado/Reward  -15.0\n","  # 42 : acción intercambiar(2,6) -> Estado/Reward  -14.0\n","  # 43 : acción mover(8,1) -> Estado/Reward  -15.0\n","  # 44 : acción intercambiar(3,7) -> Estado/Reward  -16.0\n","  # 45 : acción intercambiar(8,4) -> Estado/Reward  -17.0\n","  # 46 : acción intercambiar(0,5) -> Estado/Reward  -8.0\n","  # 47 : acción intercambiar(7,7) -> Estado/Reward  -8.0\n","  # 48 : acción intercambiar(0,2) -> Estado/Reward  -11.0\n","  # 49 : acción mover(9,6) -> Estado/Reward  -11.0\n","  # 50 : acción mover(4,6) -> Estado/Reward  -11.0\n","  # 51 : acción mover(2,2) -> Estado/Reward  -11.0\n","  # 52 : acción mover(6,3) -> Estado/Reward  -10.0\n","  # 53 : acción intercambiar(9,8) -> Estado/Reward  -10.0\n","  # 54 : acción intercambiar(8,6) -> Estado/Reward  -10.0\n","  # 55 : acción intercambiar(3,1) -> Estado/Reward  -9.0\n","  # 56 : acción mover(7,1) -> Estado/Reward  -6.0\n","  # 57 : acción intercambiar(8,7) -> Estado/Reward  -6.0\n","  # 58 : acción mover(4,2) -> Estado/Reward  -8.0\n","  # 59 : acción intercambiar(5,5) -> Estado/Reward  -8.0\n","  # 60 : acción mover(1,7) -> Estado/Reward  -11.0\n","  # 61 : acción mover(4,0) -> Estado/Reward  -15.0\n","  # 62 : acción intercambiar(2,4) -> Estado/Reward  -12.0\n","  # 63 : acción intercambiar(6,7) -> Estado/Reward  -12.0\n","  # 64 : acción mover(5,2) -> Estado/Reward  -15.0\n","  # 65 : acción intercambiar(3,7) -> Estado/Reward  -16.0\n","  # 66 : acción mover(8,1) -> Estado/Reward  -11.0\n","  # 67 : acción intercambiar(6,6) -> Estado/Reward  -11.0\n","  # 68 : acción mover(6,0) -> Estado/Reward  -11.0\n","  # 69 : acción intercambiar(7,4) -> Estado/Reward  -10.0\n","  # 70 : acción mover(0,6) -> Estado/Reward  -10.0\n","  # 71 : acción mover(2,0) -> Estado/Reward  -10.0\n","  # 72 : acción intercambiar(0,3) -> Estado/Reward  -9.0\n","  # 73 : acción intercambiar(2,0) -> Estado/Reward  -8.0\n","  # 74 : acción intercambiar(2,0) -> Estado/Reward  -9.0\n","  # 75 : acción mover(4,7) -> Estado/Reward  -11.0\n","  # 76 : acción intercambiar(1,2) -> Estado/Reward  -10.0\n","  # 77 : acción mover(2,8) -> Estado/Reward  -8.0\n","  # 78 : acción intercambiar(6,3) -> Estado/Reward  -7.0\n","  # 79 : acción mover(9,0) -> Estado/Reward  -13.0\n","  # 80 : acción mover(6,7) -> Estado/Reward  -13.0\n","  # 81 : acción mover(3,6) -> Estado/Reward  -12.0\n","  # 82 : acción mover(0,8) -> Estado/Reward  -6.0\n","  # 83 : acción mover(4,1) -> Estado/Reward  -5.0\n","  # 84 : acción intercambiar(0,4) -> Estado/Reward  -6.0\n","  # 85 : acción intercambiar(3,3) -> Estado/Reward  -6.0\n","  # 86 : acción mover(4,1) -> Estado/Reward  -7.0\n","  # 87 : acción intercambiar(1,2) -> Estado/Reward  -6.0\n","  # 88 : acción intercambiar(5,2) -> Estado/Reward  -7.0\n","  # 89 : acción intercambiar(3,2) -> Estado/Reward  -6.0\n","  # 90 : acción intercambiar(4,8) -> Estado/Reward  -7.0\n","  # 91 : acción intercambiar(6,9) -> Estado/Reward  -7.0\n","  # 92 : acción mover(7,8) -> Estado/Reward  -7.0\n","  # 93 : acción mover(1,8) -> Estado/Reward  -10.0\n","  # 94 : acción intercambiar(5,9) -> Estado/Reward  -9.0\n","  # 95 : acción mover(1,0) -> Estado/Reward  -8.0\n","  # 96 : acción mover(9,5) -> Estado/Reward  -9.0\n","  # 97 : acción intercambiar(8,3) -> Estado/Reward  -4.0\n","  # 98 : acción intercambiar(7,5) -> Estado/Reward  -5.0\n","  # 99 : acción intercambiar(5,3) -> Estado/Reward  -8.0\n","  # 100 : acción intercambiar(0,6) -> Estado/Reward  -17.0\n"," Recompensa Final =  -17.0\n"," Lista Final =  [ 51  19  32  99  13 -43 -66]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 99  51 -66  19 -43  13  32]\n","  # 1 : acción mover(0,6) -> Estado/Reward  -7.0\n","  # 2 : acción mover(0,3) -> Estado/Reward  -4.0\n","  # 3 : acción intercambiar(3,4) -> Estado/Reward  -3.0\n","  # 4 : acción intercambiar(1,2) -> Estado/Reward  -2.0\n","  # 5 : acción mover(4,5) -> Estado/Reward  -1.0\n","  # 6 : acción intercambiar(2,3) -> Estado/Reward  94.0\n"," Recompensa Final =  94.0\n"," Lista Final =  [-66 -43  13  19  32  51  99]\n","\n","--> El Agente Entrenado ( 94.0 ) genera MEJOR resultado que el azar ( -17.0 )\n","\n","> Prueba  9 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [  3  31 -95  55 -37 -34  -7]\n","  # 1 : acción intercambiar(0,2) -> Estado/Reward  -10.0\n","  # 2 : acción mover(3,7) -> Estado/Reward  -7.0\n","  # 3 : acción intercambiar(0,4) -> Estado/Reward  -10.0\n","  # 4 : acción mover(0,8) -> Estado/Reward  -12.0\n","  # 5 : acción intercambiar(8,2) -> Estado/Reward  -13.0\n","  # 6 : acción intercambiar(0,8) -> Estado/Reward  -6.0\n","  # 7 : acción intercambiar(7,3) -> Estado/Reward  -9.0\n","  # 8 : acción intercambiar(2,7) -> Estado/Reward  -8.0\n","  # 9 : acción mover(4,9) -> Estado/Reward  -8.0\n","  # 10 : acción mover(1,8) -> Estado/Reward  -7.0\n","  # 11 : acción mover(0,9) -> Estado/Reward  -11.0\n","  # 12 : acción intercambiar(7,9) -> Estado/Reward  -11.0\n","  # 13 : acción mover(9,3) -> Estado/Reward  -8.0\n","  # 14 : acción intercambiar(4,2) -> Estado/Reward  -7.0\n","  # 15 : acción mover(5,3) -> Estado/Reward  -7.0\n","  # 16 : acción intercambiar(8,4) -> Estado/Reward  -8.0\n","  # 17 : acción mover(5,8) -> Estado/Reward  -7.0\n","  # 18 : acción mover(4,4) -> Estado/Reward  -7.0\n","  # 19 : acción mover(5,2) -> Estado/Reward  -4.0\n","  # 20 : acción intercambiar(6,6) -> Estado/Reward  -4.0\n","  # 21 : acción intercambiar(2,2) -> Estado/Reward  -4.0\n","  # 22 : acción intercambiar(5,7) -> Estado/Reward  -5.0\n","  # 23 : acción mover(9,7) -> Estado/Reward  -5.0\n","  # 24 : acción mover(9,7) -> Estado/Reward  -5.0\n","  # 25 : acción intercambiar(3,2) -> Estado/Reward  -6.0\n","  # 26 : acción intercambiar(4,4) -> Estado/Reward  -6.0\n","  # 27 : acción mover(6,8) -> Estado/Reward  -6.0\n","  # 28 : acción mover(4,3) -> Estado/Reward  -7.0\n","  # 29 : acción mover(7,3) -> Estado/Reward  -8.0\n","  # 30 : acción intercambiar(5,1) -> Estado/Reward  -1.0\n","  # 31 : acción intercambiar(7,5) -> Estado/Reward  -2.0\n","  # 32 : acción mover(6,1) -> Estado/Reward  -5.0\n","  # 33 : acción intercambiar(3,6) -> Estado/Reward  -10.0\n","  # 34 : acción mover(8,5) -> Estado/Reward  -9.0\n","  # 35 : acción mover(6,1) -> Estado/Reward  -8.0\n","  # 36 : acción mover(5,6) -> Estado/Reward  -7.0\n","  # 37 : acción intercambiar(9,2) -> Estado/Reward  -6.0\n","  # 38 : acción intercambiar(2,2) -> Estado/Reward  -6.0\n","  # 39 : acción intercambiar(4,8) -> Estado/Reward  -5.0\n","  # 40 : acción mover(3,2) -> Estado/Reward  -4.0\n","  # 41 : acción mover(4,5) -> Estado/Reward  -3.0\n","  # 42 : acción intercambiar(7,0) -> Estado/Reward  -14.0\n","  # 43 : acción intercambiar(0,5) -> Estado/Reward  -13.0\n","  # 44 : acción intercambiar(3,3) -> Estado/Reward  -13.0\n","  # 45 : acción mover(4,8) -> Estado/Reward  -13.0\n","  # 46 : acción mover(5,3) -> Estado/Reward  -11.0\n","  # 47 : acción mover(2,0) -> Estado/Reward  -9.0\n","  # 48 : acción mover(3,6) -> Estado/Reward  -12.0\n","  # 49 : acción mover(6,4) -> Estado/Reward  -10.0\n","  # 50 : acción mover(9,6) -> Estado/Reward  -10.0\n","  # 51 : acción mover(1,8) -> Estado/Reward  -7.0\n","  # 52 : acción mover(1,8) -> Estado/Reward  -8.0\n","  # 53 : acción mover(4,9) -> Estado/Reward  -10.0\n","  # 54 : acción intercambiar(2,6) -> Estado/Reward  -11.0\n","  # 55 : acción mover(8,0) -> Estado/Reward  -5.0\n","  # 56 : acción intercambiar(0,4) -> Estado/Reward  -12.0\n","  # 57 : acción intercambiar(3,5) -> Estado/Reward  -13.0\n","  # 58 : acción mover(4,2) -> Estado/Reward  -11.0\n","  # 59 : acción intercambiar(4,4) -> Estado/Reward  -11.0\n","  # 60 : acción intercambiar(7,8) -> Estado/Reward  -11.0\n","  # 61 : acción mover(4,4) -> Estado/Reward  -11.0\n","  # 62 : acción mover(8,5) -> Estado/Reward  -12.0\n","  # 63 : acción mover(8,8) -> Estado/Reward  -12.0\n","  # 64 : acción mover(0,1) -> Estado/Reward  -11.0\n","  # 65 : acción intercambiar(0,8) -> Estado/Reward  -12.0\n","  # 66 : acción mover(8,2) -> Estado/Reward  -10.0\n","  # 67 : acción intercambiar(8,4) -> Estado/Reward  -9.0\n","  # 68 : acción intercambiar(7,2) -> Estado/Reward  -12.0\n","  # 69 : acción mover(3,8) -> Estado/Reward  -15.0\n","  # 70 : acción mover(7,5) -> Estado/Reward  -14.0\n","  # 71 : acción intercambiar(7,9) -> Estado/Reward  -14.0\n","  # 72 : acción intercambiar(5,9) -> Estado/Reward  -15.0\n","  # 73 : acción mover(6,3) -> Estado/Reward  -12.0\n","  # 74 : acción intercambiar(0,8) -> Estado/Reward  -11.0\n","  # 75 : acción mover(5,9) -> Estado/Reward  -10.0\n","  # 76 : acción mover(3,2) -> Estado/Reward  -9.0\n","  # 77 : acción intercambiar(1,4) -> Estado/Reward  -6.0\n","  # 78 : acción intercambiar(9,1) -> Estado/Reward  -9.0\n","  # 79 : acción mover(9,1) -> Estado/Reward  -8.0\n","  # 80 : acción intercambiar(4,3) -> Estado/Reward  -9.0\n","  # 81 : acción mover(6,8) -> Estado/Reward  -9.0\n","  # 82 : acción intercambiar(7,5) -> Estado/Reward  -8.0\n","  # 83 : acción intercambiar(9,2) -> Estado/Reward  -9.0\n","  # 84 : acción intercambiar(5,9) -> Estado/Reward  -10.0\n","  # 85 : acción mover(6,3) -> Estado/Reward  -9.0\n","  # 86 : acción intercambiar(3,1) -> Estado/Reward  -8.0\n","  # 87 : acción intercambiar(9,4) -> Estado/Reward  -9.0\n","  # 88 : acción mover(8,0) -> Estado/Reward  -11.0\n","  # 89 : acción intercambiar(9,6) -> Estado/Reward  -11.0\n","  # 90 : acción mover(0,2) -> Estado/Reward  -9.0\n","  # 91 : acción mover(0,3) -> Estado/Reward  -12.0\n","  # 92 : acción mover(6,2) -> Estado/Reward  -8.0\n","  # 93 : acción mover(6,8) -> Estado/Reward  -8.0\n","  # 94 : acción intercambiar(1,4) -> Estado/Reward  -7.0\n","  # 95 : acción intercambiar(4,6) -> Estado/Reward  -8.0\n","  # 96 : acción mover(3,9) -> Estado/Reward  -5.0\n","  # 97 : acción intercambiar(2,3) -> Estado/Reward  -6.0\n","  # 98 : acción intercambiar(6,3) -> Estado/Reward  -11.0\n","  # 99 : acción intercambiar(4,1) -> Estado/Reward  -12.0\n","  # 100 : acción intercambiar(1,5) -> Estado/Reward  -13.0\n"," Recompensa Final =  -13.0\n"," Lista Final =  [-34   3  31  55 -37  -7 -95]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [  3  31 -95  55 -37 -34  -7]\n","  # 1 : acción mover(1,8) -> Estado/Reward  -8.0\n","  # 2 : acción intercambiar(2,4) -> Estado/Reward  -7.0\n","  # 3 : acción mover(0,3) -> Estado/Reward  -4.0\n","  # 4 : acción mover(4,9) -> Estado/Reward  -2.0\n","  # 5 : acción intercambiar(3,4) -> Estado/Reward  -1.0\n","  # 6 : acción intercambiar(1,2) -> Estado/Reward  94.0\n"," Recompensa Final =  94.0\n"," Lista Final =  [-95 -37 -34  -7   3  31  55]\n","\n","--> El Agente Entrenado ( 94.0 ) genera MEJOR resultado que el azar ( -13.0 )\n","\n","> Prueba  10 :\n","\n","**  Resultados Aleatorio **\n"," Lista Inicial =  [ 70 -32 -48  -2  46]\n","  # 1 : acción intercambiar(2,1) -> Estado/Reward  -4.0\n","  # 2 : acción intercambiar(9,9) -> Estado/Reward  -4.0\n","  # 3 : acción intercambiar(2,1) -> Estado/Reward  -5.0\n","  # 4 : acción mover(4,6) -> Estado/Reward  -5.0\n","  # 5 : acción mover(4,9) -> Estado/Reward  -5.0\n","  # 6 : acción intercambiar(0,4) -> Estado/Reward  -4.0\n","  # 7 : acción mover(5,6) -> Estado/Reward  -4.0\n","  # 8 : acción mover(4,5) -> Estado/Reward  -4.0\n","  # 9 : acción intercambiar(7,9) -> Estado/Reward  -4.0\n","  # 10 : acción mover(3,2) -> Estado/Reward  -5.0\n","  # 11 : acción mover(0,0) -> Estado/Reward  -5.0\n","  # 12 : acción intercambiar(4,7) -> Estado/Reward  -5.0\n","  # 13 : acción intercambiar(0,2) -> Estado/Reward  -4.0\n","  # 14 : acción intercambiar(5,4) -> Estado/Reward  -4.0\n","  # 15 : acción mover(7,6) -> Estado/Reward  -4.0\n","  # 16 : acción intercambiar(4,9) -> Estado/Reward  -4.0\n","  # 17 : acción mover(7,1) -> Estado/Reward  -7.0\n","  # 18 : acción mover(7,9) -> Estado/Reward  -7.0\n","  # 19 : acción intercambiar(9,6) -> Estado/Reward  -7.0\n","  # 20 : acción intercambiar(5,4) -> Estado/Reward  -7.0\n","  # 21 : acción intercambiar(9,9) -> Estado/Reward  -7.0\n","  # 22 : acción intercambiar(0,1) -> Estado/Reward  -8.0\n","  # 23 : acción mover(0,5) -> Estado/Reward  -4.0\n","  # 24 : acción intercambiar(0,3) -> Estado/Reward  -1.0\n","  # 25 : acción mover(4,2) -> Estado/Reward  -3.0\n","  # 26 : acción intercambiar(3,9) -> Estado/Reward  -2.0\n","  # 27 : acción mover(7,0) -> Estado/Reward  -4.0\n","  # 28 : acción intercambiar(7,5) -> Estado/Reward  -4.0\n","  # 29 : acción intercambiar(3,8) -> Estado/Reward  -3.0\n","  # 30 : acción mover(2,6) -> Estado/Reward  -5.0\n","  # 31 : acción mover(0,1) -> Estado/Reward  -4.0\n","  # 32 : acción mover(6,8) -> Estado/Reward  -4.0\n","  # 33 : acción intercambiar(1,0) -> Estado/Reward  -5.0\n","  # 34 : acción intercambiar(9,6) -> Estado/Reward  -5.0\n","  # 35 : acción intercambiar(3,6) -> Estado/Reward  -4.0\n","  # 36 : acción intercambiar(3,6) -> Estado/Reward  -5.0\n","  # 37 : acción mover(8,4) -> Estado/Reward  -5.0\n","  # 38 : acción intercambiar(1,5) -> Estado/Reward  -6.0\n","  # 39 : acción intercambiar(2,6) -> Estado/Reward  -5.0\n","  # 40 : acción intercambiar(9,7) -> Estado/Reward  -5.0\n","  # 41 : acción mover(2,8) -> Estado/Reward  -7.0\n","  # 42 : acción mover(5,5) -> Estado/Reward  -7.0\n","  # 43 : acción intercambiar(3,2) -> Estado/Reward  -6.0\n","  # 44 : acción intercambiar(3,3) -> Estado/Reward  -6.0\n","  # 45 : acción intercambiar(2,5) -> Estado/Reward  -5.0\n","  # 46 : acción mover(0,6) -> Estado/Reward  -3.0\n","  # 47 : acción mover(7,9) -> Estado/Reward  -3.0\n","  # 48 : acción intercambiar(6,9) -> Estado/Reward  -3.0\n","  # 49 : acción intercambiar(0,7) -> Estado/Reward  -6.0\n","  # 50 : acción mover(4,0) -> Estado/Reward  -4.0\n","  # 51 : acción intercambiar(4,8) -> Estado/Reward  -4.0\n","  # 52 : acción intercambiar(1,3) -> Estado/Reward  -5.0\n","  # 53 : acción intercambiar(8,5) -> Estado/Reward  -5.0\n","  # 54 : acción intercambiar(8,7) -> Estado/Reward  -5.0\n","  # 55 : acción intercambiar(4,4) -> Estado/Reward  -5.0\n","  # 56 : acción intercambiar(9,1) -> Estado/Reward  -2.0\n","  # 57 : acción mover(3,5) -> Estado/Reward  -3.0\n","  # 58 : acción mover(7,6) -> Estado/Reward  -3.0\n","  # 59 : acción intercambiar(8,5) -> Estado/Reward  -3.0\n","  # 60 : acción intercambiar(4,4) -> Estado/Reward  -3.0\n","  # 61 : acción intercambiar(0,3) -> Estado/Reward  -6.0\n","  # 62 : acción mover(7,5) -> Estado/Reward  -6.0\n","  # 63 : acción intercambiar(2,1) -> Estado/Reward  -5.0\n","  # 64 : acción mover(2,3) -> Estado/Reward  -4.0\n","  # 65 : acción mover(6,0) -> Estado/Reward  -6.0\n","  # 66 : acción intercambiar(3,4) -> Estado/Reward  -7.0\n","  # 67 : acción mover(2,7) -> Estado/Reward  -9.0\n","  # 68 : acción intercambiar(9,4) -> Estado/Reward  -9.0\n","  # 69 : acción intercambiar(6,5) -> Estado/Reward  -9.0\n","  # 70 : acción intercambiar(2,5) -> Estado/Reward  -6.0\n","  # 71 : acción intercambiar(8,9) -> Estado/Reward  -6.0\n","  # 72 : acción intercambiar(6,7) -> Estado/Reward  -6.0\n","  # 73 : acción intercambiar(2,1) -> Estado/Reward  -5.0\n","  # 74 : acción intercambiar(7,2) -> Estado/Reward  -4.0\n","  # 75 : acción intercambiar(8,0) -> Estado/Reward  -5.0\n","  # 76 : acción intercambiar(3,7) -> Estado/Reward  -6.0\n","  # 77 : acción intercambiar(9,1) -> Estado/Reward  -7.0\n","  # 78 : acción intercambiar(4,5) -> Estado/Reward  -7.0\n","  # 79 : acción intercambiar(0,8) -> Estado/Reward  21.0\n"," Recompensa Final =  21.0\n"," Lista Final =  [-48 -32  -2  46  70]\n","\n","**  Resultados de Agente Entrenado **\n"," Lista Inicial =  [ 70 -32 -48  -2  46]\n","  # 1 : acción intercambiar(0,2) -> Estado/Reward  -2.0\n","  # 2 : acción mover(2,3) -> Estado/Reward  -1.0\n","  # 3 : acción intercambiar(3,4) -> Estado/Reward  97.0\n"," Recompensa Final =  97.0\n"," Lista Final =  [-48 -32  -2  46  70]\n","\n","--> El Agente Entrenado ( 97.0 ) genera MEJOR resultado que el azar ( 21.0 )\n","\n","================================================================================================\n","\n","= En promedio, el Agente Entrenado ( 63.8 ) tiene MEJORES resultado que  el azar ( -10.4 )\n","\n","================================================================================================\n","\n"],"name":"stdout"}]}]}